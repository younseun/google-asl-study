{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE,ADASYN\n",
    "from imblearn.combine import SMOTEENN,SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('creditcard.csv',dtype='float32',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train : (227845, 31)\n",
      "df_test : (56962, 31)\n"
     ]
    }
   ],
   "source": [
    "## data setting\n",
    "# setting up testing and training sets\n",
    "df_train, df_test = train_test_split(data, test_size=0.2, random_state=27)\n",
    "print('df_train :', df_train.shape)\n",
    "print('df_test :', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train : (205060, 31)\n",
      "df_valid : (22785, 31)\n",
      "df_test : (56962, 31)\n",
      "df_train Class: \n",
      " 0.0    204712\n",
      "1.0       348\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_train, df_valid = train_test_split(df_train, test_size=0.1, random_state=27)\n",
    "print('df_train :', df_train.shape)\n",
    "print('df_valid :', df_valid.shape)\n",
    "print('df_test :', df_test.shape)\n",
    "print('df_train Class: \\n', df_train.Class.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 1. OUTLIER Delete in train data => 시각화 분석 결과 정상유저의 threshold 범위 안에 사기유저 있음\n",
    "###### 즉, threshold 를 얼마정도 잡고 데이터를 없애도 크게 상관은 없어보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_treatment(df,col,beta):\n",
    "    q1,q3 = df.describe().loc['25%',col], df.describe().loc['75%',col]\n",
    "    IQR = q3 - q1\n",
    "    lower_range = q1 - (beta * IQR)\n",
    "    upper_range = q3 + (beta * IQR)\n",
    "#     print(col,'lower_range :',lower_range)\n",
    "#     print(col,'upper_range :',upper_range)\n",
    "    df = df[(df[col] > lower_range) & (df[col] < upper_range)]\n",
    "    return df\n",
    "\n",
    "def cleaning_df(df,cols,beta=10):\n",
    "    for i in cols:\n",
    "        df = outlier_treatment(df,i,beta=beta)\n",
    "    print('outlier delete')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 2. normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(col):\n",
    "    mean = df_train['Amount'].mean()\n",
    "    std = df_train['Amount'].std()\n",
    "    return (col - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 3. feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = df_train.copy()\n",
    "df_tr['V14pV12'] = df_tr['V14'] + df_tr['V12']\n",
    "df_tr['V2pV11'] = df_tr['V2'] + df_tr['V11']\n",
    "df_tr['V10pV3'] = df_tr['V10'] + df_tr['V3']\n",
    "df_tr['V17pV14'] = df_tr['V17'] + df_tr['V14']\n",
    "df_tr['V4pV2'] = df_tr['V4'] + df_tr['V2']\n",
    "\n",
    "df_tr['sqV3-V2'] = (df_tr['V3'] - df_tr['V2']) **2\n",
    "df_tr['sqV8'] = (df_tr['V8']) **2\n",
    "df_tr['sqV2'] = (df_tr['V2']) **2\n",
    "df_tr['sqV17'] = (df_tr['V17']) **2\n",
    "\n",
    "\n",
    "df_tr['V17-V11'] = df_tr['V17'] - df_tr['V11'] \n",
    "df_tr = df_tr[AA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_engineered_features(features):\n",
    "    features = features.astype('float32')\n",
    "    features['V14pV12'] = features['V14'] + features['V12']\n",
    "    features['V2pV11'] = features['V2'] + features['V11']\n",
    "    features['V10pV3'] = features['V10'] + features['V3']\n",
    "    features['V17pV14'] = features['V17'] + features['V14']\n",
    "    features['V4pV2'] = features['V4'] + features['V2']\n",
    "\n",
    "    features['sqV3-V2'] = (features['V3'] - features['V2']) **2\n",
    "    features['sqV8'] = (features['V8']) **2\n",
    "    features['sqV2'] = (features['V2']) **2\n",
    "    features['sqV17'] = (features['V17']) **2\n",
    "\n",
    "    features['V17-V11'] = features['V17'] - features['V11'] \n",
    "    print('generate feature engineered')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def add_engineered_features(features):\n",
    "    features = features.astype('float32')\n",
    "    features[\"V17-V11\"] = features[\"V17\"] - features[\"V11\"]\n",
    "    features[\"V17-V2\"] = features[\"V17\"] - features[\"V2\"]\n",
    "    features[\"V17-V15\"] = features[\"V17\"] - features[\"V15\"]\n",
    "    features[\"V14-V11\"] = features[\"V14\"] - features[\"V11\"]\n",
    "    features[\"V14-V12\"] = features[\"V14\"] - features[\"V12\"]\n",
    "    features[\"V14-V4\"] = features[\"V14\"] - features[\"V4\"]\n",
    "    features[\"V12-V11\"] = features[\"V12\"] - features[\"V11\"]\n",
    "    features[\"V12-V4\"] = features[\"V12\"] - features[\"V4\"]\n",
    "    features[\"V10-V8\"] = features[\"V10\"] - features[\"V8\"]\n",
    "    features[\"V5-V4\"] = features[\"V5\"] - features[\"V4\"]\n",
    "    features[\"V3-V2\"] = features[\"V3\"] - features[\"V2\"]\n",
    "    features[\"V3-V4\"] = features[\"V3\"] - features[\"V4\"]\n",
    "    features[\"V7-V6\"] = features[\"V7\"] - features[\"V6\"]\n",
    "    \n",
    "    features[\"V11V4\"] = features[\"V11\"] * features[\"V4\"]\n",
    "    features[\"V19V8\"] = features[\"V19\"] * features[\"V8\"]\n",
    "    features[\"V16V10\"] = features[\"V16\"] * features[\"V10\"]\n",
    "    features[\"V3V8\"] = features[\"V3\"] * features[\"V8\"]\n",
    "    features[\"V7V8\"] = features[\"V7\"] * features[\"V8\"]\n",
    "    \n",
    "    features[\"V17V17\"] = features[\"V17\"] ** 2\n",
    "    features[\"V16V16\"] = features[\"V16\"] ** 2\n",
    "    features[\"V12V12\"] = features[\"V12\"] ** 2\n",
    "    features[\"V4V4\"] = features[\"V4\"] ** 2\n",
    "    features[\"sqV17-V11\"] = features[\"V17-V11\"] **2\n",
    "    features[\"sqV17-V15\"] = features[\"V17-V15\"] **2\n",
    "    features[\"sqV17-V2\"] = features[\"V17-V2\"] **2\n",
    "    features[\"sqV14-V11\"] = features[\"V14-V11\"] **2\n",
    "    features[\"sqV14-V4\"] = features[\"V14-V4\"] **2\n",
    "    features[\"sqV12-V11\"] = features[\"V12-V11\"] **2\n",
    "    features[\"sqV10-V8\"] = features[\"V10-V8\"] **2\n",
    "    features[\"sqV3-V4\"] =features[\"V3-V4\"] **2\n",
    "    \n",
    "    features[\"V12pV14\"] = ((features[\"V12\"]+features[\"V14\"]) ** 2)**(1/2)\n",
    "#     features = features.astype('float32')\n",
    "    print('generate feature engineered')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 4.DATA Augmentation --> oversampling SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_smote_features(features):\n",
    "    sm = SMOTE(random_state=27, k_neighbors=5)\n",
    "    features, _ = sm.fit_sample(features, features.Class)\n",
    "    features = pd.DataFrame(features, columns=list(features))\n",
    "    print(features.Class.value_counts())\n",
    "    features = features.astype('float32')\n",
    "    print('augmentation using SMOTE')\n",
    "    return features\n",
    "\n",
    "def add_smoteenn_features(features):\n",
    "    sm = SMOTEENN(random_state=27)\n",
    "    features, _ = sm.fit_resample(features, features.Class)\n",
    "    features = pd.DataFrame(features, columns=list(features))\n",
    "    print(features.Class.value_counts())\n",
    "    features = features.astype('float32')\n",
    "    print('augmentation using SMOTEENN')\n",
    "    return features\n",
    "\n",
    "def add_smotetomek_features(features):\n",
    "    sm = SMOTETomek(random_state=27)\n",
    "    features, _ = sm.fit_resample(features, features.Class)\n",
    "    features = pd.DataFrame(features, columns=list(features))\n",
    "    print(features.Class.value_counts())\n",
    "    features = features.astype('float32')\n",
    "    print('augmentation using SMOTETomek')\n",
    "    return features\n",
    "\n",
    "def add_adasyn_features(features):\n",
    "    ada = ADASYN(random_state=27)\n",
    "    features, _ = ada.fit_resample(features, features.Class)\n",
    "    features = pd.DataFrame(features, columns=list(features))\n",
    "    print(features.Class.value_counts())\n",
    "    features = features.astype('float32')\n",
    "    print('augmentation using ADASYN')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_cols\n",
    "outlier_cols = ['V2','V6','V7','V13','V16','V23','V24','V25','V26','V28','Amount']\n",
    "# FEATURE_NAMES = ['V'+str(i) for i in range(1,29)] + \\\n",
    "#                 [\"V14-V4\",\"V17-V15\",\"V12-V11\",\"V10-V8\",\"V5-V4\",\"V3-V2\",\"V3-V4\",\"V17-V11\",\"V12-V4\",\"V14-V11\",\"V17-V2\",\"V7-V6\",\"V14-V12\"] + \\\n",
    "#                 [\"V11V4\",\"V16V10\",\"V19V8\",\"V17V17\",\"V16V16\",\"V12V12\",\"V4V4\",\"sqV17-V11\",\"sqV17-V15\",\"sqV17-V2\",\"V3V8\",\"V7V8\",\"sqV14-V11\",\"sqV14-V4\",\"sqV12-V11\",\"sqV10-V8\",\"sqV3-V4\"] +\\\n",
    "#                 [\"V12pV14\"]\n",
    "\n",
    "FEATURE_NAMES = ['V'+str(i) for i in range(1,29)]\n",
    "FEATURE_NAMES.append('Amount')\n",
    "FEATURE_NAMES.append('Time')\n",
    "\n",
    "#### 직접 눈으로보고 만든 피처  +  앙상블모델에 의한 피처 임포턴스로 추가한 피처(V26,V17,V14,V10,V7)\n",
    "# FEATURE_NAMES = ['V14pV12','V2pV11','V17-V11','sqV3-V2','V10pV3','V17pV14','V4pV2','sqV8','sqV2','sqV17',\n",
    "#                 'V17','V14','V10','V26','V7']\n",
    "feature_cols = [tf.feature_column.numeric_column(k) for k in FEATURE_NAMES]\n",
    "# feature_cols.append(tf.feature_column.numeric_column('Amount',normalizer_fn=zscore))\n",
    "\n",
    "# FEATURE_NAMES.append('Amount')\n",
    "LABEL_NAME = 'Class'\n",
    "# feature_cols = [tf.feature_column.numeric_column(k) for k in ['V'+str(i) for i in range(1,29)]]\n",
    "# feature_cols.append(tf.feature_column.numeric_column(\"V17-V15\"))\n",
    "# feature_cols.append(tf.feature_column.numeric_column(\"V12-V11\"))\n",
    "# feature_cols.append(tf.feature_column.numeric_column(\"V10-V8\"))\n",
    "# feature_cols.append(tf.feature_column.numeric_column(\"V5-V4\"))\n",
    "# feature_cols.append(tf.feature_column.numeric_column(\"V3-V2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas_input_fn\n",
    "def train_input_fn(df,batch_size=128,num_epochs=5,shuffle=True):\n",
    "    # 1.delete outlier\n",
    "    df = cleaning_df(df,outlier_cols,beta=10)\n",
    "    # 2.feature engineering\n",
    "    df = add_engineered_features(df)\n",
    "    # 3.add_smote\n",
    "#     df = add_smote_features(df)\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "      x=df[FEATURE_NAMES],\n",
    "      y=df[LABEL_NAME],\n",
    "      batch_size = batch_size,\n",
    "      num_epochs = num_epochs,\n",
    "      shuffle = shuffle,\n",
    "      queue_capacity = 1000,\n",
    "      num_threads = 1\n",
    "  )\n",
    "\n",
    "def eval_input_fn(df,batch_size=128):\n",
    "    # 2.feature engineering\n",
    "    df = add_engineered_features(df)\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "      x=df[FEATURE_NAMES],\n",
    "      y=df[LABEL_NAME],\n",
    "      batch_size = batch_size,\n",
    "      num_epochs = 1,\n",
    "      shuffle = False,\n",
    "      queue_capacity = 1000,\n",
    "      num_threads = 1\n",
    "  )\n",
    "\n",
    "def predict_input_fn(df,batch_size=128):\n",
    "    # 2.feature engineering\n",
    "    df = add_engineered_features(df)\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "      x=df[FEATURE_NAMES],\n",
    "      y=df[LABEL_NAME],\n",
    "      batch_size = batch_size,\n",
    "      num_epochs = 1,\n",
    "      shuffle = False,\n",
    "      queue_capacity = 1000,\n",
    "      num_threads = 1\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_tf_random_seed': 1, '_is_chief': True, '_eval_distribute': None, '_num_ps_replicas': 0, '_experimental_distribute': None, '_model_dir': 'credit-prac', '_task_id': 0, '_global_id_in_cluster': 0, '_keep_checkpoint_every_n_hours': 10000, '_experimental_max_worker_delay_secs': None, '_num_worker_replicas': 1, '_service': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_protocol': None, '_save_checkpoints_steps': 1000, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6517250b38>, '_session_creation_timeout_secs': 7200, '_evaluation_master': '', '_keep_checkpoint_max': 5, '_device_fn': None, '_save_checkpoints_secs': None, '_save_summary_steps': 100, '_task_type': 'worker', '_master': '', '_log_step_count_steps': 100, '_train_distribute': None}\n"
     ]
    }
   ],
   "source": [
    "OUTDIR = \"credit-prac\"\n",
    "\n",
    "config = tf.estimator.RunConfig(\n",
    "    model_dir = OUTDIR,\n",
    "    tf_random_seed = 1,\n",
    "    save_checkpoints_steps = 1000\n",
    ")\n",
    "\n",
    "model = tf.estimator.DNNClassifier(\n",
    "        hidden_units = [30,30,30], \n",
    "        feature_columns = feature_cols,\n",
    "        config = config,\n",
    "        optimizer='Adam',\n",
    "        batch_norm=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outlier delete\n",
      "generate feature engineered\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into credit-prac/model.ckpt.\n",
      "INFO:tensorflow:loss = 121.555, step = 1\n",
      "INFO:tensorflow:global_step/sec: 24.3124\n",
      "INFO:tensorflow:loss = 0.1597428, step = 101 (4.115 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.6565\n",
      "INFO:tensorflow:loss = 0.22930491, step = 201 (2.808 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.3569\n",
      "INFO:tensorflow:loss = 0.1922042, step = 301 (2.607 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.3037\n",
      "INFO:tensorflow:loss = 0.1052516, step = 401 (2.607 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.8484\n",
      "INFO:tensorflow:loss = 0.059819005, step = 501 (2.574 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.0332\n",
      "INFO:tensorflow:loss = 0.18544982, step = 601 (2.502 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.3169\n",
      "INFO:tensorflow:loss = 7.4384665, step = 701 (2.420 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.6994\n",
      "INFO:tensorflow:loss = 0.06508988, step = 801 (2.585 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.8507\n",
      "INFO:tensorflow:loss = 0.086890906, step = 901 (2.281 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into credit-prac/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 38.6366\n",
      "INFO:tensorflow:loss = 0.10462497, step = 1001 (2.587 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.5835\n",
      "INFO:tensorflow:loss = 0.1167095, step = 1101 (2.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.4111\n",
      "INFO:tensorflow:loss = 0.21577609, step = 1201 (2.305 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.0279\n",
      "INFO:tensorflow:loss = 0.10632465, step = 1301 (2.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.9502\n",
      "INFO:tensorflow:loss = 0.02786024, step = 1401 (2.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.2479\n",
      "INFO:tensorflow:loss = 0.09445392, step = 1501 (2.615 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.1219\n",
      "INFO:tensorflow:loss = 0.16275138, step = 1601 (2.428 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.4914\n",
      "INFO:tensorflow:loss = 0.0376599, step = 1701 (2.357 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.249\n",
      "INFO:tensorflow:loss = 0.17676923, step = 1801 (2.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.912\n",
      "INFO:tensorflow:loss = 0.101551555, step = 1901 (2.445 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into credit-prac/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 37.6377\n",
      "INFO:tensorflow:loss = 0.2995329, step = 2001 (2.653 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.8142\n",
      "INFO:tensorflow:loss = 0.11253237, step = 2101 (2.644 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.271\n",
      "INFO:tensorflow:loss = 0.23721713, step = 2201 (2.617 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.3837\n",
      "INFO:tensorflow:loss = 0.07717571, step = 2301 (2.826 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.9874\n",
      "INFO:tensorflow:loss = 0.14546737, step = 2401 (2.561 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.8228\n",
      "INFO:tensorflow:loss = 7.74726, step = 2501 (2.644 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.4507\n",
      "INFO:tensorflow:loss = 0.047870055, step = 2601 (2.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.4669\n",
      "INFO:tensorflow:loss = 0.14903948, step = 2701 (2.300 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.6819\n",
      "INFO:tensorflow:loss = 0.04483954, step = 2801 (2.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.8509\n",
      "INFO:tensorflow:loss = 0.17102407, step = 2901 (2.135 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into credit-prac/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 37.8165\n",
      "INFO:tensorflow:loss = 0.059930086, step = 3001 (2.641 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.5615\n",
      "INFO:tensorflow:loss = 0.06140336, step = 3101 (2.532 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.2896\n",
      "INFO:tensorflow:loss = 0.12192385, step = 3201 (2.257 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.565\n",
      "INFO:tensorflow:loss = 0.12840539, step = 3301 (2.196 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.1867\n",
      "INFO:tensorflow:loss = 0.17957963, step = 3401 (2.370 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.2824\n",
      "INFO:tensorflow:loss = 0.43508655, step = 3501 (2.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.847\n",
      "INFO:tensorflow:loss = 0.119788505, step = 3601 (2.281 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.9276\n",
      "INFO:tensorflow:loss = 0.30654398, step = 3701 (2.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.8056\n",
      "INFO:tensorflow:loss = 0.08744457, step = 3801 (2.332 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.4505\n",
      "INFO:tensorflow:loss = 0.37137616, step = 3901 (2.250 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4000 into credit-prac/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 39.5917\n",
      "INFO:tensorflow:loss = 0.6185362, step = 4001 (2.528 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.7998\n",
      "INFO:tensorflow:loss = 0.21957088, step = 4101 (2.050 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.503\n",
      "INFO:tensorflow:loss = 0.22416896, step = 4201 (2.353 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.7649\n",
      "INFO:tensorflow:loss = 0.021669205, step = 4301 (2.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.2641\n",
      "INFO:tensorflow:loss = 0.12219425, step = 4401 (2.255 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.3766\n",
      "INFO:tensorflow:loss = 0.29941195, step = 4501 (2.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.727\n",
      "INFO:tensorflow:loss = 0.11739722, step = 4601 (2.579 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.3719\n",
      "INFO:tensorflow:loss = 0.070081025, step = 4701 (2.421 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.1591\n",
      "INFO:tensorflow:loss = 6.933755, step = 4801 (2.429 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.3837\n",
      "INFO:tensorflow:loss = 0.21668279, step = 4901 (2.606 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into credit-prac/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 36.7711\n",
      "INFO:tensorflow:loss = 0.19004168, step = 5001 (2.716 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.2914\n",
      "INFO:tensorflow:loss = 0.1068595, step = 5101 (2.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.938\n",
      "INFO:tensorflow:loss = 0.07349909, step = 5201 (2.329 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.3976\n",
      "INFO:tensorflow:loss = 0.24731116, step = 5301 (2.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.8087\n",
      "INFO:tensorflow:loss = 7.1647573, step = 5401 (2.336 sec)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf.logging.set_verbosity(tf.logging.INFO) \n",
    "shutil.rmtree(path = OUTDIR, ignore_errors = True)\n",
    "\n",
    "model.train(input_fn = train_input_fn(df=df_train, batch_size=128, num_epochs=None), steps = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.evaluate(input_fn=eval_input_fn(df=df_valid, batch_size=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, input_fn):\n",
    "    return [x[\"class_ids\"][0] for x in model.predict(input_fn=input_fn)]\n",
    "\n",
    "# Create a confusion matrix\n",
    "with tf.Graph().as_default():\n",
    "    y_pred = get_predictions(model, predict_input_fn(df=df_test, batch_size=128))\n",
    "    cm = tf.confusion_matrix(df_test[\"Class\"],y_pred)\n",
    "    with tf.Session() as session:\n",
    "        cm_out = session.run(cm)\n",
    "        print(cm_out)\n",
    "print()\n",
    "print('Class1')\n",
    "print('precision :',cm_out[1][1]/(cm_out[0][1]+cm_out[1][1]))\n",
    "print('recall :',cm_out[1][1]/(cm_out[1][0]+cm_out[1][1]))\n",
    "print('fbeta_score :', fbeta_score(df_test[\"Class\"], y_pred, beta=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56826</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0   1\n",
       "0  56826  32\n",
       "1     21  83"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversamling 안한것 결과\n",
    "[[56796    62]\n",
    " [   20    84]]\n",
    "\n",
    "Class1\n",
    "precision : 0.5753424657534246\n",
    "recall : 0.8076923076923077\n",
    "fbeta_score : 0.7953386744355425"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOGIT.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logit_train = df_train.copy()\n",
    "# 0.Amount normalization\n",
    "df_logit_train['Amount'] = zscore(df_logit_train['Amount'])\n",
    "# 1.delete outlier\n",
    "df_logit_train = cleaning_df(df_logit_train,outlier_cols,beta=10)\n",
    "# 2.feature engineering\n",
    "df_logit_train = add_engineered_features(df_logit_train)\n",
    "# 3.add_smote\n",
    "df_logit_train = add_smoteenn_features(df_logit_train)\n",
    "\n",
    "df_logit_train = df_logit_train.drop('Time',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mid_df_logit_train = pd.concat([df_logit_train[df_logit_train.Class == 0].sample(frac=1).iloc[:len(df_logit_train)//2, :],df_logit_train[df_logit_train.Class ==1]], axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logit_test = df_test.copy()\n",
    "# 0.Amount normalization\n",
    "df_logit_test['Amount'] = zscore(df_logit_test['Amount'])\n",
    "# 2.feature engineering\n",
    "df_logit_test = add_engineered_features(df_logit_test)\n",
    "df_logit_test = df_logit_test.drop('Time',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_logit_train_y = mid_df_logit_train.pop('Class')\n",
    "df_logit_train_y = df_logit_train.pop('Class')\n",
    "df_logit_test_y = df_logit_test.pop('Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = LogisticRegression(solver='saga',class_weight='balanced').fit(mid_df_logit_train, df_logit_train_y)\n",
    "clf = LogisticRegression(solver='saga',class_weight='balanced').fit(df_logit_train, df_logit_train_y)\n",
    "pred_y = clf.predict(df_logit_test)\n",
    "print(confusion_matrix(df_logit_test_y,pred_y))\n",
    "print(classification_report(df_logit_test_y,pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(df_logit_train)\n",
    "df_logit_train_scaler = scaler.transform(df_logit_train)\n",
    "df_logit_test_scaler = scaler.transform(df_logit_test)\n",
    "clf = LogisticRegression(solver='saga').fit(df_logit_train_scaler, df_logit_train_y)\n",
    "pred_y = clf.predict(df_logit_test_scaler)\n",
    "print(confusion_matrix(df_logit_test_y,pred_y))\n",
    "print(classification_report(df_logit_test_y,pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 직접 피처만들어서 샘플링 안돌리고 그냥 한 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "AA = ['V14pV12', 'V2pV11', 'V17-V11','sqV3-V2','V10pV3','V17pV14','V4pV2','sqV8','sqV2','sqV17','Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = df_train.copy()\n",
    "df_tr['V14pV12'] = df_tr['V14'] + df_tr['V12']\n",
    "df_tr['V2pV11'] = df_tr['V2'] + df_tr['V11']\n",
    "df_tr['V10pV3'] = df_tr['V10'] + df_tr['V3']\n",
    "df_tr['V17pV14'] = df_tr['V17'] + df_tr['V14']\n",
    "df_tr['V4pV2'] = df_tr['V4'] + df_tr['V2']\n",
    "\n",
    "df_tr['sqV3-V2'] = (df_tr['V3'] - df_tr['V2']) **2\n",
    "df_tr['sqV8'] = (df_tr['V8']) **2\n",
    "df_tr['sqV2'] = (df_tr['V2']) **2\n",
    "df_tr['sqV17'] = (df_tr['V17']) **2\n",
    "\n",
    "\n",
    "df_tr['V17-V11'] = df_tr['V17'] - df_tr['V11'] \n",
    "df_tr = df_tr[AA]\n",
    "\n",
    "#-----------------------\n",
    "df_te = df_test.copy()\n",
    "df_te['V14pV12'] = df_te['V14'] + df_te['V12']\n",
    "df_te['V2pV11'] = df_te['V2'] + df_te['V11']\n",
    "df_te['V10pV3'] = df_te['V10'] + df_te['V3']\n",
    "df_te['V17pV14'] = df_te['V17'] + df_te['V14']\n",
    "df_te['V4pV2'] = df_te['V4'] + df_te['V2']\n",
    "\n",
    "df_te['sqV3-V2'] = (df_te['V3'] - df_te['V2']) **2\n",
    "df_te['sqV8'] = (df_te['V8']) **2\n",
    "df_te['sqV2'] = (df_te['V2']) **2\n",
    "df_te['sqV17'] = (df_te['V17']) **2\n",
    "\n",
    "df_te['V17-V11'] = df_te['V17'] - df_te['V11']\n",
    "df_te = df_te[AA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_logit_train_y = mid_df_logit_train.pop('Class')\n",
    "df_tr_y = df_tr.pop('Class')\n",
    "df_te_y = df_te.pop('Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48348  8510]\n",
      " [    9    95]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.85      0.92     56858\n",
      "         1.0       0.01      0.91      0.02       104\n",
      "\n",
      "    accuracy                           0.85     56962\n",
      "   macro avg       0.51      0.88      0.47     56962\n",
      "weighted avg       1.00      0.85      0.92     56962\n",
      "\n",
      "fbeta_score : 0.22043730477465417\n"
     ]
    }
   ],
   "source": [
    "# clf = LogisticRegression(solver='saga',class_weight='balanced').fit(mid_df_logit_train, df_logit_train_y)\n",
    "clf = LogisticRegression(solver='saga',class_weight='balanced').fit(df_tr, df_tr_y)\n",
    "pred_y = clf.predict(df_te)\n",
    "print(confusion_matrix(df_te_y,pred_y))\n",
    "print(classification_report(df_te_y,pred_y))\n",
    "\n",
    "print('fbeta_score :', fbeta_score(df_te_y, pred_y, beta=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Gradient Boosting Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier(loss='exponential',\n",
    "                                      learning_rate=0.1,\n",
    "                                      n_estimators=100,\n",
    "                                      min_samples_split=3,\n",
    "                                      min_samples_leaf=2,\n",
    "                                      max_depth=5,).fit(df_tr, df_tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_pred = gb_model.predict(df_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0   1\n",
      "0  56851   7\n",
      "1     22  82\n"
     ]
    }
   ],
   "source": [
    "# 결과\n",
    "print(pd.DataFrame(confusion_matrix(df_te_y, gb_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     56858\n",
      "         1.0       0.92      0.79      0.85       104\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.96      0.89      0.92     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "fbeta_score : 0.792859799181852\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_te_y, gb_pred))\n",
    "print('fbeta_score :',fbeta_score(df_te_y, gb_pred,beta=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Ramdom Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0   1\n",
      "0  56853   5\n",
      "1     21  83\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     56858\n",
      "         1.0       0.94      0.80      0.86       104\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.97      0.90      0.93     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "fbeta_score : 0.802827380952381\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100,\n",
    "    criterion='gini',\n",
    "    max_depth=15).fit(df_tr, df_tr_y)\n",
    "rf_pred = rf_model.predict(df_te)\n",
    "# 결과\n",
    "print(pd.DataFrame(confusion_matrix(df_te_y, rf_pred)))\n",
    "print(classification_report(df_te_y, rf_pred))\n",
    "print('fbeta_score :',fbeta_score(df_te_y, rf_pred,beta=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
