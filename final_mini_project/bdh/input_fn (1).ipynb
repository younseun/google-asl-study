{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE,ADASYN\n",
    "from imblearn.combine import SMOTEENN,SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('creditcard.csv',dtype='float32',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train : (227845, 31)\n",
      "df_test : (56962, 31)\n"
     ]
    }
   ],
   "source": [
    "## data setting\n",
    "# setting up testing and training sets\n",
    "df_train, df_test = train_test_split(data, test_size=0.2, random_state=27)\n",
    "print('df_train :', df_train.shape)\n",
    "print('df_test :', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train : (205060, 31)\n",
      "df_valid : (22785, 31)\n",
      "df_test : (56962, 31)\n",
      "df_train Class: \n",
      " 0.0    204712\n",
      "1.0       348\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_train, df_valid = train_test_split(df_train, test_size=0.1, random_state=27)\n",
    "print('df_train :', df_train.shape)\n",
    "print('df_valid :', df_valid.shape)\n",
    "print('df_test :', df_test.shape)\n",
    "print('df_train Class: \\n', df_train.Class.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 1. OUTLIER Delete in train data => 시각화 분석 결과 정상유저의 threshold 범위 안에 사기유저 있음\n",
    "###### 즉, threshold 를 얼마정도 잡고 데이터를 없애도 크게 상관은 없어보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_treatment(df,col,beta):\n",
    "    q1,q3 = df.describe().loc['25%',col], df.describe().loc['75%',col]\n",
    "    IQR = q3 - q1\n",
    "    lower_range = q1 - (beta * IQR)\n",
    "    upper_range = q3 + (beta * IQR)\n",
    "#     print(col,'lower_range :',lower_range)\n",
    "#     print(col,'upper_range :',upper_range)\n",
    "    df = df[(df[col] > lower_range) & (df[col] < upper_range)]\n",
    "    return df\n",
    "\n",
    "def cleaning_df(df,cols,beta=10):\n",
    "    for i in cols:\n",
    "        df = outlier_treatment(df,i,beta=beta)\n",
    "    print('outlier delete')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 2. normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(col):\n",
    "    mean = df_train['Amount'].mean()\n",
    "    std = df_train['Amount'].std()\n",
    "    return (col - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 3. feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = df_train.copy()\n",
    "df_tr['V14pV12'] = df_tr['V14'] + df_tr['V12']\n",
    "df_tr['V2pV11'] = df_tr['V2'] + df_tr['V11']\n",
    "df_tr['V10pV3'] = df_tr['V10'] + df_tr['V3']\n",
    "df_tr['V17pV14'] = df_tr['V17'] + df_tr['V14']\n",
    "df_tr['V4pV2'] = df_tr['V4'] + df_tr['V2']\n",
    "\n",
    "df_tr['sqV3-V2'] = (df_tr['V3'] - df_tr['V2']) **2\n",
    "df_tr['sqV8'] = (df_tr['V8']) **2\n",
    "df_tr['sqV2'] = (df_tr['V2']) **2\n",
    "df_tr['sqV17'] = (df_tr['V17']) **2\n",
    "\n",
    "\n",
    "df_tr['V17-V11'] = df_tr['V17'] - df_tr['V11'] \n",
    "df_tr = df_tr[AA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_engineered_features(features):\n",
    "    features = features.astype('float32')\n",
    "    features['V14pV12'] = features['V14'] + features['V12']\n",
    "    features['V2pV11'] = features['V2'] + features['V11']\n",
    "    features['V10pV3'] = features['V10'] + features['V3']\n",
    "    features['V17pV14'] = features['V17'] + features['V14']\n",
    "    features['V4pV2'] = features['V4'] + features['V2']\n",
    "\n",
    "    features['sqV3-V2'] = (features['V3'] - features['V2']) **2\n",
    "    features['sqV8'] = (features['V8']) **2\n",
    "    features['sqV2'] = (features['V2']) **2\n",
    "    features['sqV17'] = (features['V17']) **2\n",
    "\n",
    "    features['V17-V11'] = features['V17'] - features['V11'] \n",
    "    print('generate feature engineered')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def add_engineered_features(features):\n",
    "    features = features.astype('float32')\n",
    "    features[\"V17-V11\"] = features[\"V17\"] - features[\"V11\"]\n",
    "    features[\"V17-V2\"] = features[\"V17\"] - features[\"V2\"]\n",
    "    features[\"V17-V15\"] = features[\"V17\"] - features[\"V15\"]\n",
    "    features[\"V14-V11\"] = features[\"V14\"] - features[\"V11\"]\n",
    "    features[\"V14-V12\"] = features[\"V14\"] - features[\"V12\"]\n",
    "    features[\"V14-V4\"] = features[\"V14\"] - features[\"V4\"]\n",
    "    features[\"V12-V11\"] = features[\"V12\"] - features[\"V11\"]\n",
    "    features[\"V12-V4\"] = features[\"V12\"] - features[\"V4\"]\n",
    "    features[\"V10-V8\"] = features[\"V10\"] - features[\"V8\"]\n",
    "    features[\"V5-V4\"] = features[\"V5\"] - features[\"V4\"]\n",
    "    features[\"V3-V2\"] = features[\"V3\"] - features[\"V2\"]\n",
    "    features[\"V3-V4\"] = features[\"V3\"] - features[\"V4\"]\n",
    "    features[\"V7-V6\"] = features[\"V7\"] - features[\"V6\"]\n",
    "    \n",
    "    features[\"V11V4\"] = features[\"V11\"] * features[\"V4\"]\n",
    "    features[\"V19V8\"] = features[\"V19\"] * features[\"V8\"]\n",
    "    features[\"V16V10\"] = features[\"V16\"] * features[\"V10\"]\n",
    "    features[\"V3V8\"] = features[\"V3\"] * features[\"V8\"]\n",
    "    features[\"V7V8\"] = features[\"V7\"] * features[\"V8\"]\n",
    "    \n",
    "    features[\"V17V17\"] = features[\"V17\"] ** 2\n",
    "    features[\"V16V16\"] = features[\"V16\"] ** 2\n",
    "    features[\"V12V12\"] = features[\"V12\"] ** 2\n",
    "    features[\"V4V4\"] = features[\"V4\"] ** 2\n",
    "    features[\"sqV17-V11\"] = features[\"V17-V11\"] **2\n",
    "    features[\"sqV17-V15\"] = features[\"V17-V15\"] **2\n",
    "    features[\"sqV17-V2\"] = features[\"V17-V2\"] **2\n",
    "    features[\"sqV14-V11\"] = features[\"V14-V11\"] **2\n",
    "    features[\"sqV14-V4\"] = features[\"V14-V4\"] **2\n",
    "    features[\"sqV12-V11\"] = features[\"V12-V11\"] **2\n",
    "    features[\"sqV10-V8\"] = features[\"V10-V8\"] **2\n",
    "    features[\"sqV3-V4\"] =features[\"V3-V4\"] **2\n",
    "    \n",
    "    features[\"V12pV14\"] = ((features[\"V12\"]+features[\"V14\"]) ** 2)**(1/2)\n",
    "#     features = features.astype('float32')\n",
    "    print('generate feature engineered')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 4.DATA Augmentation --> oversampling SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_smote_features(features):\n",
    "    sm = SMOTE(random_state=27, k_neighbors=5)\n",
    "    features, _ = sm.fit_sample(features, features.Class)\n",
    "    features = pd.DataFrame(features, columns=list(features))\n",
    "    print(features.Class.value_counts())\n",
    "    features = features.astype('float32')\n",
    "    print('augmentation using SMOTE')\n",
    "    return features\n",
    "\n",
    "def add_smoteenn_features(features):\n",
    "    sm = SMOTEENN(random_state=27)\n",
    "    features, _ = sm.fit_resample(features, features.Class)\n",
    "    features = pd.DataFrame(features, columns=list(features))\n",
    "    print(features.Class.value_counts())\n",
    "    features = features.astype('float32')\n",
    "    print('augmentation using SMOTEENN')\n",
    "    return features\n",
    "\n",
    "def add_smotetomek_features(features):\n",
    "    sm = SMOTETomek(random_state=27)\n",
    "    features, _ = sm.fit_resample(features, features.Class)\n",
    "    features = pd.DataFrame(features, columns=list(features))\n",
    "    print(features.Class.value_counts())\n",
    "    features = features.astype('float32')\n",
    "    print('augmentation using SMOTETomek')\n",
    "    return features\n",
    "\n",
    "def add_adasyn_features(features):\n",
    "    ada = ADASYN(random_state=27)\n",
    "    features, _ = ada.fit_resample(features, features.Class)\n",
    "    features = pd.DataFrame(features, columns=list(features))\n",
    "    print(features.Class.value_counts())\n",
    "    features = features.astype('float32')\n",
    "    print('augmentation using ADASYN')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_cols\n",
    "outlier_cols = ['V2','V6','V7','V13','V16','V23','V24','V25','V26','V28','Amount']\n",
    "# FEATURE_NAMES = ['V'+str(i) for i in range(1,29)] + \\\n",
    "#                 [\"V14-V4\",\"V17-V15\",\"V12-V11\",\"V10-V8\",\"V5-V4\",\"V3-V2\",\"V3-V4\",\"V17-V11\",\"V12-V4\",\"V14-V11\",\"V17-V2\",\"V7-V6\",\"V14-V12\"] + \\\n",
    "#                 [\"V11V4\",\"V16V10\",\"V19V8\",\"V17V17\",\"V16V16\",\"V12V12\",\"V4V4\",\"sqV17-V11\",\"sqV17-V15\",\"sqV17-V2\",\"V3V8\",\"V7V8\",\"sqV14-V11\",\"sqV14-V4\",\"sqV12-V11\",\"sqV10-V8\",\"sqV3-V4\"] +\\\n",
    "#                 [\"V12pV14\"]\n",
    "\n",
    "FEATURE_NAMES = ['V'+str(i) for i in range(1,29)]\n",
    "# FEATURE_NAMES.append('Amount')\n",
    "# FEATURE_NAMES.append('Time')\n",
    "\n",
    "#### 직접 눈으로보고 만든 피처  +  앙상블모델에 의한 피처 임포턴스로 추가한 피처(V26,V17,V14,V10,V7)\n",
    "# FEATURE_NAMES = ['V14pV12','V2pV11','V17-V11','sqV3-V2','V10pV3','V17pV14','V4pV2','sqV8','sqV2','sqV17',\n",
    "#                 'V17','V14','V10','V26','V7']\n",
    "feature_cols = [tf.feature_column.numeric_column(k) for k in FEATURE_NAMES]\n",
    "feature_cols.append(tf.feature_column.numeric_column('Amount',normalizer_fn=zscore))\n",
    "FEATURE_NAMES.append('Amount')\n",
    "LABEL_NAME = 'Class'\n",
    "# feature_cols = [tf.feature_column.numeric_column(k) for k in ['V'+str(i) for i in range(1,29)]]\n",
    "# feature_cols.append(tf.feature_column.numeric_column(\"V17-V15\"))\n",
    "# feature_cols.append(tf.feature_column.numeric_column(\"V12-V11\"))\n",
    "# feature_cols.append(tf.feature_column.numeric_column(\"V10-V8\"))\n",
    "# feature_cols.append(tf.feature_column.numeric_column(\"V5-V4\"))\n",
    "# feature_cols.append(tf.feature_column.numeric_column(\"V3-V2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas_input_fn\n",
    "def train_input_fn(df,batch_size=128,num_epochs=5,shuffle=True):\n",
    "    # 1.delete outlier\n",
    "    df = cleaning_df(df,outlier_cols,beta=10)\n",
    "    # 2.feature engineering\n",
    "    df = add_engineered_features(df)\n",
    "    # 3.add_smote\n",
    "    df = add_smotetomek_features(df)\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "      x=df[FEATURE_NAMES],\n",
    "      y=df[LABEL_NAME],\n",
    "      batch_size = batch_size,\n",
    "      num_epochs = num_epochs,\n",
    "      shuffle = shuffle,\n",
    "      queue_capacity = 1000,\n",
    "      num_threads = 1\n",
    "  )\n",
    "\n",
    "def eval_input_fn(df,batch_size=128):\n",
    "    # 2.feature engineering\n",
    "    df = add_engineered_features(df)\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "      x=df[FEATURE_NAMES],\n",
    "      y=df[LABEL_NAME],\n",
    "      batch_size = batch_size,\n",
    "      num_epochs = 1,\n",
    "      shuffle = False,\n",
    "      queue_capacity = 1000,\n",
    "      num_threads = 1\n",
    "  )\n",
    "\n",
    "def predict_input_fn(df,batch_size=128):\n",
    "    # 2.feature engineering\n",
    "    df = add_engineered_features(df)\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "      x=df[FEATURE_NAMES],\n",
    "      y=df[LABEL_NAME],\n",
    "      batch_size = batch_size,\n",
    "      num_epochs = 1,\n",
    "      shuffle = False,\n",
    "      queue_capacity = 1000,\n",
    "      num_threads = 1\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_tf_random_seed': 1, '_is_chief': True, '_eval_distribute': None, '_num_ps_replicas': 0, '_experimental_distribute': None, '_model_dir': 'credit-prac', '_task_id': 0, '_global_id_in_cluster': 0, '_keep_checkpoint_every_n_hours': 10000, '_experimental_max_worker_delay_secs': None, '_num_worker_replicas': 1, '_service': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_protocol': None, '_save_checkpoints_steps': 1000, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f65177a5d30>, '_session_creation_timeout_secs': 7200, '_evaluation_master': '', '_keep_checkpoint_max': 5, '_device_fn': None, '_save_checkpoints_secs': None, '_save_summary_steps': 100, '_task_type': 'worker', '_master': '', '_log_step_count_steps': 100, '_train_distribute': None}\n"
     ]
    }
   ],
   "source": [
    "OUTDIR = \"credit-prac\"\n",
    "\n",
    "config = tf.estimator.RunConfig(\n",
    "    model_dir = OUTDIR,\n",
    "    tf_random_seed = 1,\n",
    "    save_checkpoints_steps = 1000\n",
    ")\n",
    "\n",
    "model = tf.estimator.DNNClassifier(\n",
    "        hidden_units = [30,30,30], \n",
    "        feature_columns = feature_cols,\n",
    "        config = config,\n",
    "        optimizer='Adam',\n",
    "        batch_norm=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outlier delete\n",
      "generate feature engineered\n",
      "1.0    197792\n",
      "0.0    193212\n",
      "Name: Class, dtype: int64\n",
      "augmentation using SMOTEENN\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into credit-prac/model.ckpt.\n",
      "INFO:tensorflow:loss = 114.62146, step = 1\n",
      "INFO:tensorflow:global_step/sec: 16.9371\n",
      "INFO:tensorflow:loss = 0.00054929225, step = 101 (5.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.1682\n",
      "INFO:tensorflow:loss = 0.0005036519, step = 201 (3.014 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.6029\n",
      "INFO:tensorflow:loss = 0.00043755161, step = 301 (2.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.6635\n",
      "INFO:tensorflow:loss = 0.0004216545, step = 401 (2.800 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.9986\n",
      "INFO:tensorflow:loss = 0.00036741694, step = 501 (2.703 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.2021\n",
      "INFO:tensorflow:loss = 0.00034557105, step = 601 (2.691 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.9504\n",
      "INFO:tensorflow:loss = 0.00031435592, step = 701 (2.946 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.1605\n",
      "INFO:tensorflow:loss = 15.561738, step = 801 (2.691 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.2218\n",
      "INFO:tensorflow:loss = 0.00015946504, step = 901 (2.612 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into credit-prac/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 35.7426\n",
      "INFO:tensorflow:loss = 0.000110816145, step = 1001 (2.799 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.1383\n",
      "INFO:tensorflow:loss = 1.6676368e-05, step = 1101 (3.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.2551\n",
      "INFO:tensorflow:loss = 2.4549343e-05, step = 1201 (3.197 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.8466\n",
      "INFO:tensorflow:loss = 3.2992957e-05, step = 1301 (2.873 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.8901\n",
      "INFO:tensorflow:loss = 1.556023e-05, step = 1401 (2.786 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.561\n",
      "INFO:tensorflow:loss = 2.8793958e-05, step = 1501 (2.890 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.0959\n",
      "INFO:tensorflow:loss = 4.2004223e-05, step = 1601 (2.770 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.442\n",
      "INFO:tensorflow:loss = 2.6599359e-05, step = 1701 (2.993 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.8327\n",
      "INFO:tensorflow:loss = 5.3609438e-05, step = 1801 (2.872 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.0003\n",
      "INFO:tensorflow:loss = 1.5550793e-05, step = 1901 (2.699 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into credit-prac/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 31.4797\n",
      "INFO:tensorflow:loss = 2.0092533e-05, step = 2001 (3.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.6075\n",
      "INFO:tensorflow:loss = 2.4851957e-05, step = 2101 (2.732 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.129\n",
      "INFO:tensorflow:loss = 1.2306704e-05, step = 2201 (2.496 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.7641\n",
      "INFO:tensorflow:loss = 1.5456593e-05, step = 2301 (2.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.3144\n",
      "INFO:tensorflow:loss = 8.387965e-16, step = 2401 (2.680 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.2929\n",
      "INFO:tensorflow:loss = 9.585798e-25, step = 2501 (2.834 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.491\n",
      "INFO:tensorflow:loss = 1.09981e-24, step = 2601 (2.737 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.4765\n",
      "INFO:tensorflow:loss = 1.3603305e-24, step = 2701 (2.742 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.9395\n",
      "INFO:tensorflow:loss = 1.0545403e-24, step = 2801 (2.707 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.0436\n",
      "INFO:tensorflow:loss = 1.5570665e-24, step = 2901 (2.857 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into credit-prac/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 34.4124\n",
      "INFO:tensorflow:loss = 1.0600813e-24, step = 3001 (2.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.5384\n",
      "INFO:tensorflow:loss = 9.98778e-25, step = 3101 (2.811 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.8869\n",
      "INFO:tensorflow:loss = 1.0490221e-24, step = 3201 (2.954 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.157\n",
      "INFO:tensorflow:loss = 1.6828862e-24, step = 3301 (3.012 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.6217\n",
      "INFO:tensorflow:loss = 1.0436646e-24, step = 3401 (2.893 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.5861\n",
      "INFO:tensorflow:loss = 1.058436e-24, step = 3501 (3.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.2871\n",
      "INFO:tensorflow:loss = 1.1604757e-24, step = 3601 (2.917 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.5393\n",
      "INFO:tensorflow:loss = 1.0097903e-24, step = 3701 (2.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.7317\n",
      "INFO:tensorflow:loss = 1.1630686e-24, step = 3801 (2.879 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.738\n",
      "INFO:tensorflow:loss = 7.989618e-27, step = 3901 (2.725 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4000 into credit-prac/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 32.8878\n",
      "INFO:tensorflow:loss = 2.6040568e-20, step = 4001 (3.040 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.8047\n",
      "INFO:tensorflow:loss = 7.206626e-25, step = 4101 (2.793 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.3658\n",
      "INFO:tensorflow:loss = 2.399358e-17, step = 4201 (2.750 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.8513\n",
      "INFO:tensorflow:loss = 7.524564e-20, step = 4301 (2.573 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.6763\n",
      "INFO:tensorflow:loss = 1.1486936e-20, step = 4401 (2.726 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.1321\n",
      "INFO:tensorflow:loss = 3.0137898e-25, step = 4501 (2.556 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.3782\n",
      "INFO:tensorflow:loss = 2.0753485e-23, step = 4601 (2.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.8875\n",
      "INFO:tensorflow:loss = 2.1595332e-18, step = 4701 (2.507 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.4715\n",
      "INFO:tensorflow:loss = 2.365087e-22, step = 4801 (2.669 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.4706\n",
      "INFO:tensorflow:loss = 1.3243162e-22, step = 4901 (2.991 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into credit-prac/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 32.3435\n",
      "INFO:tensorflow:loss = 2.187585e-19, step = 5001 (3.091 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.7052\n",
      "INFO:tensorflow:loss = 2.5596657e-20, step = 5101 (2.878 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.9516\n",
      "INFO:tensorflow:loss = 6.0069746e-22, step = 5201 (2.710 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.2751\n",
      "INFO:tensorflow:loss = 4.8580214e-20, step = 5301 (2.831 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.4289\n",
      "INFO:tensorflow:loss = 49.79302, step = 5401 (2.823 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.2132\n",
      "INFO:tensorflow:loss = 0.0, step = 5501 (2.839 sec)\n",
      "INFO:tensorflow:global_step/sec: 27.8072\n",
      "INFO:tensorflow:loss = 0.0, step = 5601 (3.597 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.2373\n",
      "INFO:tensorflow:loss = 0.0, step = 5701 (3.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.3931\n",
      "INFO:tensorflow:loss = 0.0, step = 5801 (2.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.8328\n",
      "INFO:tensorflow:loss = 0.0, step = 5901 (2.791 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6000 into credit-prac/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 34.469\n",
      "INFO:tensorflow:loss = 0.0, step = 6001 (2.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.4453\n",
      "INFO:tensorflow:loss = 0.0, step = 6101 (2.671 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.2424\n",
      "INFO:tensorflow:loss = 0.0, step = 6201 (2.618 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.7517\n",
      "INFO:tensorflow:loss = 0.0, step = 6301 (2.798 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.7509\n",
      "INFO:tensorflow:loss = 0.0, step = 6401 (2.793 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.4987\n",
      "INFO:tensorflow:loss = 0.0, step = 6501 (2.598 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.736\n",
      "INFO:tensorflow:loss = 0.0, step = 6601 (2.649 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.8512\n",
      "INFO:tensorflow:loss = 0.0, step = 6701 (2.578 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.3522\n",
      "INFO:tensorflow:loss = 0.0, step = 6801 (2.607 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.027\n",
      "INFO:tensorflow:loss = 1.7384434e-07, step = 6901 (2.628 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7000 into credit-prac/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 32.6106\n",
      "INFO:tensorflow:loss = 0.0, step = 7001 (3.065 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.2617\n",
      "INFO:tensorflow:loss = 0.0, step = 7101 (2.840 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.8698\n",
      "INFO:tensorflow:loss = 0.0, step = 7201 (2.447 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.728\n",
      "INFO:tensorflow:loss = 0.0, step = 7301 (2.719 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.578\n",
      "INFO:tensorflow:loss = 0.0, step = 7401 (2.737 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.8146\n",
      "INFO:tensorflow:loss = 0.0, step = 7501 (3.048 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.4745\n",
      "INFO:tensorflow:loss = 0.0, step = 7601 (2.738 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.0798\n",
      "INFO:tensorflow:loss = 0.0, step = 7701 (3.023 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.5228\n",
      "INFO:tensorflow:loss = 0.0, step = 7801 (2.987 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.3286\n",
      "INFO:tensorflow:loss = 0.0, step = 7901 (2.913 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8000 into credit-prac/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 33.7852\n",
      "INFO:tensorflow:loss = 0.0, step = 8001 (2.956 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.2233\n",
      "INFO:tensorflow:loss = 0.0, step = 8101 (3.104 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.8139\n",
      "INFO:tensorflow:loss = 0.0, step = 8201 (2.644 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.257\n",
      "INFO:tensorflow:loss = 0.0, step = 8301 (2.919 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.4257\n",
      "INFO:tensorflow:loss = 0.0, step = 8401 (2.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.3421\n",
      "INFO:tensorflow:loss = 0.0, step = 8501 (2.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.1967\n",
      "INFO:tensorflow:loss = 0.0, step = 8601 (2.759 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.1793\n",
      "INFO:tensorflow:loss = 0.0, step = 8701 (2.690 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.0421\n",
      "INFO:tensorflow:loss = 0.0, step = 8801 (2.777 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.9139\n",
      "INFO:tensorflow:loss = 0.0, step = 8901 (2.781 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 9000 into credit-prac/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 30.9256\n",
      "INFO:tensorflow:loss = 0.0, step = 9001 (3.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.7085\n",
      "INFO:tensorflow:loss = 0.0, step = 9101 (2.652 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.0965\n",
      "INFO:tensorflow:loss = 0.0, step = 9201 (3.025 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.842\n",
      "INFO:tensorflow:loss = 0.0, step = 9301 (2.951 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.5719\n",
      "INFO:tensorflow:loss = 0.0, step = 9401 (2.735 sec)\n",
      "INFO:tensorflow:global_step/sec: 33.9804\n",
      "INFO:tensorflow:loss = 0.0, step = 9501 (2.943 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.2783\n",
      "INFO:tensorflow:loss = 0.0, step = 9601 (2.834 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.3752\n",
      "INFO:tensorflow:loss = 0.0, step = 9701 (2.830 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.3296\n",
      "INFO:tensorflow:loss = 0.0, step = 9801 (3.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.4214\n",
      "INFO:tensorflow:loss = 0.0, step = 9901 (2.823 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into credit-prac/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0.\n",
      "CPU times: user 29min 22s, sys: 46min 47s, total: 1h 16min 9s\n",
      "Wall time: 5min 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.dnn.DNNClassifier at 0x7f6518246908>"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tf.logging.set_verbosity(tf.logging.INFO) \n",
    "shutil.rmtree(path = OUTDIR, ignore_errors = True)\n",
    "\n",
    "model.train(input_fn = train_input_fn(df=df_train, batch_size=128, num_epochs=None), steps = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate feature engineered\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-12T04:43:22Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from credit-prac/model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-12-04:43:28\n",
      "INFO:tensorflow:Saving dict for global step 10000: accuracy = 0.3735791, accuracy_baseline = 0.99824446, auc = 0.6294845, auc_precision_recall = 0.43885592, average_loss = 99.48548, global_step = 10000, label/mean = 0.001755541, loss = 12663.558, precision = 0.0024470391, prediction/mean = 0.6274654, recall = 0.875\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10000: credit-prac/model.ckpt-10000\n"
     ]
    }
   ],
   "source": [
    "metrics = model.evaluate(input_fn=eval_input_fn(df=df_valid, batch_size=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate feature engineered\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from credit-prac/model.ckpt-10000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "[[21415 35443]\n",
      " [   22    82]]\n",
      "\n",
      "Class1\n",
      "precision : 0.0023082336382828994\n",
      "recall : 0.7884615384615384\n",
      "fbeta_score : 0.055921311475409845\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(model, input_fn):\n",
    "    return [x[\"class_ids\"][0] for x in model.predict(input_fn=input_fn)]\n",
    "\n",
    "# Create a confusion matrix\n",
    "with tf.Graph().as_default():\n",
    "    y_pred = get_predictions(model, predict_input_fn(df=df_test, batch_size=128))\n",
    "    cm = tf.confusion_matrix(df_test[\"Class\"],y_pred)\n",
    "    with tf.Session() as session:\n",
    "        cm_out = session.run(cm)\n",
    "        print(cm_out)\n",
    "print()\n",
    "print('Class1')\n",
    "print('precision :',cm_out[1][1]/(cm_out[0][1]+cm_out[1][1]))\n",
    "print('recall :',cm_out[1][1]/(cm_out[1][0]+cm_out[1][1]))\n",
    "print('fbeta_score :', fbeta_score(df_test[\"Class\"], y_pred, beta=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56826</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0   1\n",
       "0  56826  32\n",
       "1     21  83"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversamling 안한것 결과\n",
    "[[56796    62]\n",
    " [   20    84]]\n",
    "\n",
    "Class1\n",
    "precision : 0.5753424657534246\n",
    "recall : 0.8076923076923077\n",
    "fbeta_score : 0.7953386744355425"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOGIT.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logit_train = df_train.copy()\n",
    "# 0.Amount normalization\n",
    "df_logit_train['Amount'] = zscore(df_logit_train['Amount'])\n",
    "# 1.delete outlier\n",
    "df_logit_train = cleaning_df(df_logit_train,outlier_cols,beta=10)\n",
    "# 2.feature engineering\n",
    "df_logit_train = add_engineered_features(df_logit_train)\n",
    "# 3.add_smote\n",
    "df_logit_train = add_smoteenn_features(df_logit_train)\n",
    "\n",
    "df_logit_train = df_logit_train.drop('Time',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mid_df_logit_train = pd.concat([df_logit_train[df_logit_train.Class == 0].sample(frac=1).iloc[:len(df_logit_train)//2, :],df_logit_train[df_logit_train.Class ==1]], axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logit_test = df_test.copy()\n",
    "# 0.Amount normalization\n",
    "df_logit_test['Amount'] = zscore(df_logit_test['Amount'])\n",
    "# 2.feature engineering\n",
    "df_logit_test = add_engineered_features(df_logit_test)\n",
    "df_logit_test = df_logit_test.drop('Time',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_logit_train_y = mid_df_logit_train.pop('Class')\n",
    "df_logit_train_y = df_logit_train.pop('Class')\n",
    "df_logit_test_y = df_logit_test.pop('Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = LogisticRegression(solver='saga',class_weight='balanced').fit(mid_df_logit_train, df_logit_train_y)\n",
    "clf = LogisticRegression(solver='saga',class_weight='balanced').fit(df_logit_train, df_logit_train_y)\n",
    "pred_y = clf.predict(df_logit_test)\n",
    "print(confusion_matrix(df_logit_test_y,pred_y))\n",
    "print(classification_report(df_logit_test_y,pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(df_logit_train)\n",
    "df_logit_train_scaler = scaler.transform(df_logit_train)\n",
    "df_logit_test_scaler = scaler.transform(df_logit_test)\n",
    "clf = LogisticRegression(solver='saga').fit(df_logit_train_scaler, df_logit_train_y)\n",
    "pred_y = clf.predict(df_logit_test_scaler)\n",
    "print(confusion_matrix(df_logit_test_y,pred_y))\n",
    "print(classification_report(df_logit_test_y,pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 직접 피처만들어서 샘플링 안돌리고 그냥 한 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "AA = ['V14pV12', 'V2pV11', 'V17-V11','sqV3-V2','V10pV3','V17pV14','V4pV2','sqV8','sqV2','sqV17','Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = df_train.copy()\n",
    "df_tr['V14pV12'] = df_tr['V14'] + df_tr['V12']\n",
    "df_tr['V2pV11'] = df_tr['V2'] + df_tr['V11']\n",
    "df_tr['V10pV3'] = df_tr['V10'] + df_tr['V3']\n",
    "df_tr['V17pV14'] = df_tr['V17'] + df_tr['V14']\n",
    "df_tr['V4pV2'] = df_tr['V4'] + df_tr['V2']\n",
    "\n",
    "df_tr['sqV3-V2'] = (df_tr['V3'] - df_tr['V2']) **2\n",
    "df_tr['sqV8'] = (df_tr['V8']) **2\n",
    "df_tr['sqV2'] = (df_tr['V2']) **2\n",
    "df_tr['sqV17'] = (df_tr['V17']) **2\n",
    "\n",
    "\n",
    "df_tr['V17-V11'] = df_tr['V17'] - df_tr['V11'] \n",
    "df_tr = df_tr[AA]\n",
    "\n",
    "#-----------------------\n",
    "df_te = df_test.copy()\n",
    "df_te['V14pV12'] = df_te['V14'] + df_te['V12']\n",
    "df_te['V2pV11'] = df_te['V2'] + df_te['V11']\n",
    "df_te['V10pV3'] = df_te['V10'] + df_te['V3']\n",
    "df_te['V17pV14'] = df_te['V17'] + df_te['V14']\n",
    "df_te['V4pV2'] = df_te['V4'] + df_te['V2']\n",
    "\n",
    "df_te['sqV3-V2'] = (df_te['V3'] - df_te['V2']) **2\n",
    "df_te['sqV8'] = (df_te['V8']) **2\n",
    "df_te['sqV2'] = (df_te['V2']) **2\n",
    "df_te['sqV17'] = (df_te['V17']) **2\n",
    "\n",
    "df_te['V17-V11'] = df_te['V17'] - df_te['V11']\n",
    "df_te = df_te[AA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_logit_train_y = mid_df_logit_train.pop('Class')\n",
    "df_tr_y = df_tr.pop('Class')\n",
    "df_te_y = df_te.pop('Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48348  8510]\n",
      " [    9    95]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.85      0.92     56858\n",
      "         1.0       0.01      0.91      0.02       104\n",
      "\n",
      "    accuracy                           0.85     56962\n",
      "   macro avg       0.51      0.88      0.47     56962\n",
      "weighted avg       1.00      0.85      0.92     56962\n",
      "\n",
      "fbeta_score : 0.22043730477465417\n"
     ]
    }
   ],
   "source": [
    "# clf = LogisticRegression(solver='saga',class_weight='balanced').fit(mid_df_logit_train, df_logit_train_y)\n",
    "clf = LogisticRegression(solver='saga',class_weight='balanced').fit(df_tr, df_tr_y)\n",
    "pred_y = clf.predict(df_te)\n",
    "print(confusion_matrix(df_te_y,pred_y))\n",
    "print(classification_report(df_te_y,pred_y))\n",
    "\n",
    "print('fbeta_score :', fbeta_score(df_te_y, pred_y, beta=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Gradient Boosting Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier(loss='exponential',\n",
    "                                      learning_rate=0.1,\n",
    "                                      n_estimators=100,\n",
    "                                      min_samples_split=3,\n",
    "                                      min_samples_leaf=2,\n",
    "                                      max_depth=5,).fit(df_tr, df_tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_pred = gb_model.predict(df_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0   1\n",
      "0  56851   7\n",
      "1     22  82\n"
     ]
    }
   ],
   "source": [
    "# 결과\n",
    "print(pd.DataFrame(confusion_matrix(df_te_y, gb_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     56858\n",
      "         1.0       0.92      0.79      0.85       104\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.96      0.89      0.92     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "fbeta_score : 0.792859799181852\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_te_y, gb_pred))\n",
    "print('fbeta_score :',fbeta_score(df_te_y, gb_pred,beta=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Ramdom Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0   1\n",
      "0  56853   5\n",
      "1     21  83\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     56858\n",
      "         1.0       0.94      0.80      0.86       104\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.97      0.90      0.93     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "fbeta_score : 0.802827380952381\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100,\n",
    "    criterion='gini',\n",
    "    max_depth=15).fit(df_tr, df_tr_y)\n",
    "rf_pred = rf_model.predict(df_te)\n",
    "# 결과\n",
    "print(pd.DataFrame(confusion_matrix(df_te_y, rf_pred)))\n",
    "print(classification_report(df_te_y, rf_pred))\n",
    "print('fbeta_score :',fbeta_score(df_te_y, rf_pred,beta=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
