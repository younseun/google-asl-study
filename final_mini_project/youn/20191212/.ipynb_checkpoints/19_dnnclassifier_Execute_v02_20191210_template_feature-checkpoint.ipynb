{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"qwiklabs-gcp-ml-49b827b781ab\"  # Replace with your PROJECT\n",
    "BUCKET = PROJECT  # Replace with your BUCKET\n",
    "REGION = \"us-central1\"            # Choose an available region for Cloud CAIP\n",
    "TFVERSION = \"1.14\"                # TF version for CMLE to use\n",
    "PROJ_NAME='miniproj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = TFVERSION\n",
    "os.environ[\"PROJ_NAME\"] = PROJ_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_LEN = 31\n",
    "os.environ['FEATURE_LEN'] = str(FEATURE_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "모델 이름을 이력하세요: dnnclass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnnclass\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    MODEL_NAME=input('모델 이름을 이력하세요:')\n",
    "    if len(MODEL_NAME) < 1:\n",
    "        MODEL_NAME='linear'\n",
    "except ValueError:\n",
    "    MODEL_NAME='linear'\n",
    "os.environ[\"MODEL_NAME\"] = MODEL_NAME\n",
    "print(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is interrupted.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# MODEL_NAME=linear\n",
    "DATADIR=$(pwd)\n",
    "OUTDIR=$(pwd)/trained_${MODEL_NAME}/${PROJ_NAME}_v1\n",
    "rm -rf $OUTDIR\n",
    "echo $OUTDIR\n",
    "echo ${PROJ_NAME}_v1.trainer.task\n",
    "echo ${PWD}/${PROJ_NAME}_v1.trainer\n",
    "gcloud ai-platform local train \\\n",
    "   --module-name=${PROJ_NAME}_v1.trainer.task \\\n",
    "   --package-path=${PWD}/${PROJ_NAME}_v1.trainer \\\n",
    "   -- \\\n",
    "   --train_data_path=\"${DATADIR}/input/train_smote.csv\" \\\n",
    "   --eval_data_path=\"${DATADIR}/input/valid.csv\"  \\\n",
    "   --output_dir=${OUTDIR} \\\n",
    "   --model=${MODEL_NAME} \\\n",
    "   --train_steps=2000 \\\n",
    "   --feature_length=$FEATURE_LEN \\\n",
    "   --learning_rate=0.0001 \\\n",
    "   --eval_delay_secs=1 \\\n",
    "   --min_eval_frequency=10 \\\n",
    "   --stop_loss=1 \\\n",
    "   --nnsize='100,10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: miniproj_dnnclass_191210_064953\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [miniproj_dnnclass_191210_064953] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe miniproj_dnnclass_191210_064953\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs miniproj_dnnclass_191210_064953\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# MODEL_NAME=linear\n",
    "DATADIR=$(pwd)\n",
    "OUTDIR=gs://${BUCKET}/${PROJ_NAME}/${MODEL_NAME}\n",
    "JOBNAME=${PROJ_NAME}_${MODEL_NAME}_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "rm -rf $OUTDIR\n",
    "gcloud ai-platform jobs submit training $JOBNAME \\\n",
    "    --region=$REGION \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=${PWD}/${PROJ_NAME}/trainer \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --scale-tier=BASIC \\\n",
    "    --runtime-version=$TFVERSION \\\n",
    "    -- \\\n",
    "    --train_data_path=\"gs://${BUCKET}/${PROJ_NAME}/input/train_smote.csv\" \\\n",
    "    --eval_data_path=\"gs://${BUCKET}/${PROJ_NAME}/input/valid.csv\"  \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --model=${MODEL_NAME} \\\n",
    "    --train_steps=2000 \\\n",
    "    --feature_length=$FEATURE_LEN \\\n",
    "    --learning_rate=0.0001 \\\n",
    "    --eval_delay_secs=1 \\\n",
    "    --min_eval_frequency=10 \\\n",
    "    --stop_loss=1 \\\n",
    "    --nnsize='30,10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt_object = 2019-12-12 01:56:30\n",
      "type(dt_object) = <class 'datetime.datetime'>\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "timestamp = 1576115790\n",
    "dt_object = datetime.fromtimestamp(timestamp)\n",
    "print(\"dt_object =\", dt_object)\n",
    "print(\"type(dt_object) =\", type(dt_object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/contrib/predictor/saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v1/export/exporter/1576115790/variables/variables\n"
     ]
    }
   ],
   "source": [
    "loaded_model = tf.contrib.predictor.from_saved_model('/home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v1/export/exporter/1576115790')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SavedModelPredictor with feed tensors {'V13': <tf.Tensor 'Placeholder_12:0' shape=(?,) dtype=float32>, 'V8': <tf.Tensor 'Placeholder_7:0' shape=(?,) dtype=float32>, 'V5': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=float32>, 'V23': <tf.Tensor 'Placeholder_22:0' shape=(?,) dtype=float32>, 'V6': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=float32>, 'V10': <tf.Tensor 'Placeholder_9:0' shape=(?,) dtype=float32>, 'V3': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'V27': <tf.Tensor 'Placeholder_26:0' shape=(?,) dtype=float32>, 'V16': <tf.Tensor 'Placeholder_15:0' shape=(?,) dtype=float32>, 'V1': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=float32>, 'V20': <tf.Tensor 'Placeholder_19:0' shape=(?,) dtype=float32>, 'V21': <tf.Tensor 'Placeholder_20:0' shape=(?,) dtype=float32>, 'V14': <tf.Tensor 'Placeholder_13:0' shape=(?,) dtype=float32>, 'V17': <tf.Tensor 'Placeholder_16:0' shape=(?,) dtype=float32>, 'V15': <tf.Tensor 'Placeholder_14:0' shape=(?,) dtype=float32>, 'V9': <tf.Tensor 'Placeholder_8:0' shape=(?,) dtype=float32>, 'V19': <tf.Tensor 'Placeholder_18:0' shape=(?,) dtype=float32>, 'Amount': <tf.Tensor 'Placeholder_28:0' shape=(?,) dtype=float32>, 'V12': <tf.Tensor 'Placeholder_11:0' shape=(?,) dtype=float32>, 'V11': <tf.Tensor 'Placeholder_10:0' shape=(?,) dtype=float32>, 'V18': <tf.Tensor 'Placeholder_17:0' shape=(?,) dtype=float32>, 'V24': <tf.Tensor 'Placeholder_23:0' shape=(?,) dtype=float32>, 'V25': <tf.Tensor 'Placeholder_24:0' shape=(?,) dtype=float32>, 'V26': <tf.Tensor 'Placeholder_25:0' shape=(?,) dtype=float32>, 'V4': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'V2': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>, 'V22': <tf.Tensor 'Placeholder_21:0' shape=(?,) dtype=float32>, 'V28': <tf.Tensor 'Placeholder_27:0' shape=(?,) dtype=float32>, 'V7': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=float32>} and fetch_tensors {'predicted': <tf.Tensor 'dense_2/Sigmoid:0' shape=(?, 1) dtype=float32>}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_predictions(estimator, input_fn):\n",
    "    result =loaded_model({'Class':input_fn})\n",
    "    return result['probabilities']\n",
    "\n",
    "def eval_input_fn(csv_path):\n",
    "    from numpy import genfromtxt\n",
    "    data = genfromtxt(csv_path, delimiter=',')\n",
    "    data = data[1:,:-1]\n",
    "    return data\n",
    "\n",
    "LABELS = [\"0\", \"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.read_csv('./input/valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>163647.0</td>\n",
       "      <td>0.129017</td>\n",
       "      <td>1.896071</td>\n",
       "      <td>-1.909277</td>\n",
       "      <td>-0.085467</td>\n",
       "      <td>1.516371</td>\n",
       "      <td>-2.947193</td>\n",
       "      <td>1.922066</td>\n",
       "      <td>-0.649116</td>\n",
       "      <td>-0.797983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046164</td>\n",
       "      <td>0.272604</td>\n",
       "      <td>-0.286842</td>\n",
       "      <td>0.621032</td>\n",
       "      <td>0.080475</td>\n",
       "      <td>0.108701</td>\n",
       "      <td>0.046224</td>\n",
       "      <td>0.123110</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83671.0</td>\n",
       "      <td>-2.810614</td>\n",
       "      <td>-1.255934</td>\n",
       "      <td>0.534252</td>\n",
       "      <td>1.833196</td>\n",
       "      <td>1.266633</td>\n",
       "      <td>-0.336898</td>\n",
       "      <td>-0.524892</td>\n",
       "      <td>0.566502</td>\n",
       "      <td>-0.583604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029318</td>\n",
       "      <td>0.150226</td>\n",
       "      <td>-0.861510</td>\n",
       "      <td>-0.163730</td>\n",
       "      <td>-0.210010</td>\n",
       "      <td>-0.245811</td>\n",
       "      <td>0.605932</td>\n",
       "      <td>-0.486416</td>\n",
       "      <td>108.86</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>148046.0</td>\n",
       "      <td>1.632977</td>\n",
       "      <td>-0.650973</td>\n",
       "      <td>-0.700894</td>\n",
       "      <td>1.498131</td>\n",
       "      <td>-0.548833</td>\n",
       "      <td>-0.505637</td>\n",
       "      <td>-0.043126</td>\n",
       "      <td>-0.114222</td>\n",
       "      <td>0.870073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142117</td>\n",
       "      <td>0.172234</td>\n",
       "      <td>0.036681</td>\n",
       "      <td>-0.121723</td>\n",
       "      <td>-0.148125</td>\n",
       "      <td>-0.681985</td>\n",
       "      <td>0.008653</td>\n",
       "      <td>-0.007497</td>\n",
       "      <td>179.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53524.0</td>\n",
       "      <td>-1.244625</td>\n",
       "      <td>-0.862845</td>\n",
       "      <td>0.902662</td>\n",
       "      <td>-0.220987</td>\n",
       "      <td>-1.363494</td>\n",
       "      <td>0.435640</td>\n",
       "      <td>0.255022</td>\n",
       "      <td>0.640365</td>\n",
       "      <td>-1.588047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227404</td>\n",
       "      <td>0.313352</td>\n",
       "      <td>0.719519</td>\n",
       "      <td>-0.067816</td>\n",
       "      <td>-0.091712</td>\n",
       "      <td>-0.451338</td>\n",
       "      <td>-0.153241</td>\n",
       "      <td>-0.125369</td>\n",
       "      <td>310.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58411.0</td>\n",
       "      <td>1.216248</td>\n",
       "      <td>0.217858</td>\n",
       "      <td>-0.185325</td>\n",
       "      <td>0.771201</td>\n",
       "      <td>0.106515</td>\n",
       "      <td>0.003741</td>\n",
       "      <td>-0.162653</td>\n",
       "      <td>0.071036</td>\n",
       "      <td>0.308806</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177230</td>\n",
       "      <td>-0.274791</td>\n",
       "      <td>-0.216183</td>\n",
       "      <td>-0.543854</td>\n",
       "      <td>0.644877</td>\n",
       "      <td>0.443618</td>\n",
       "      <td>-0.000127</td>\n",
       "      <td>0.025547</td>\n",
       "      <td>12.31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45564</th>\n",
       "      <td>41044.0</td>\n",
       "      <td>-0.577343</td>\n",
       "      <td>1.200293</td>\n",
       "      <td>0.969770</td>\n",
       "      <td>-0.135319</td>\n",
       "      <td>1.489512</td>\n",
       "      <td>-0.251285</td>\n",
       "      <td>1.319183</td>\n",
       "      <td>-0.360093</td>\n",
       "      <td>-0.545662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004469</td>\n",
       "      <td>0.324550</td>\n",
       "      <td>-0.423130</td>\n",
       "      <td>-0.443144</td>\n",
       "      <td>0.049154</td>\n",
       "      <td>-0.568848</td>\n",
       "      <td>-0.317382</td>\n",
       "      <td>-0.307767</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45565</th>\n",
       "      <td>153360.0</td>\n",
       "      <td>2.025756</td>\n",
       "      <td>-0.050818</td>\n",
       "      <td>-1.423694</td>\n",
       "      <td>0.095830</td>\n",
       "      <td>0.549865</td>\n",
       "      <td>-0.109934</td>\n",
       "      <td>0.125709</td>\n",
       "      <td>-0.094079</td>\n",
       "      <td>0.364987</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.219888</td>\n",
       "      <td>-0.507831</td>\n",
       "      <td>0.250445</td>\n",
       "      <td>0.211346</td>\n",
       "      <td>-0.051542</td>\n",
       "      <td>-0.607790</td>\n",
       "      <td>-0.011727</td>\n",
       "      <td>-0.054534</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45566</th>\n",
       "      <td>118740.0</td>\n",
       "      <td>-0.223272</td>\n",
       "      <td>1.965183</td>\n",
       "      <td>-0.142978</td>\n",
       "      <td>4.371084</td>\n",
       "      <td>0.862892</td>\n",
       "      <td>0.534504</td>\n",
       "      <td>0.703937</td>\n",
       "      <td>0.424352</td>\n",
       "      <td>-2.238565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088413</td>\n",
       "      <td>0.341840</td>\n",
       "      <td>-0.033551</td>\n",
       "      <td>0.593325</td>\n",
       "      <td>-0.483217</td>\n",
       "      <td>0.372115</td>\n",
       "      <td>0.319060</td>\n",
       "      <td>0.212642</td>\n",
       "      <td>15.13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45567</th>\n",
       "      <td>145003.0</td>\n",
       "      <td>-0.795843</td>\n",
       "      <td>-0.045877</td>\n",
       "      <td>-0.789747</td>\n",
       "      <td>0.432049</td>\n",
       "      <td>0.778602</td>\n",
       "      <td>4.688147</td>\n",
       "      <td>-1.813848</td>\n",
       "      <td>2.047509</td>\n",
       "      <td>0.125861</td>\n",
       "      <td>...</td>\n",
       "      <td>0.519562</td>\n",
       "      <td>1.279315</td>\n",
       "      <td>0.271045</td>\n",
       "      <td>0.784797</td>\n",
       "      <td>-1.871832</td>\n",
       "      <td>0.575462</td>\n",
       "      <td>0.082396</td>\n",
       "      <td>0.065257</td>\n",
       "      <td>99.90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45568</th>\n",
       "      <td>37442.0</td>\n",
       "      <td>-1.521083</td>\n",
       "      <td>-0.851251</td>\n",
       "      <td>1.728070</td>\n",
       "      <td>-2.220317</td>\n",
       "      <td>-1.406698</td>\n",
       "      <td>-1.030617</td>\n",
       "      <td>-0.846418</td>\n",
       "      <td>0.471451</td>\n",
       "      <td>-2.240889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080307</td>\n",
       "      <td>0.144757</td>\n",
       "      <td>-0.058467</td>\n",
       "      <td>0.344586</td>\n",
       "      <td>0.447243</td>\n",
       "      <td>-0.174573</td>\n",
       "      <td>0.168484</td>\n",
       "      <td>-0.033630</td>\n",
       "      <td>87.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0      163647.0  0.129017  1.896071 -1.909277 -0.085467  1.516371 -2.947193   \n",
       "1       83671.0 -2.810614 -1.255934  0.534252  1.833196  1.266633 -0.336898   \n",
       "2      148046.0  1.632977 -0.650973 -0.700894  1.498131 -0.548833 -0.505637   \n",
       "3       53524.0 -1.244625 -0.862845  0.902662 -0.220987 -1.363494  0.435640   \n",
       "4       58411.0  1.216248  0.217858 -0.185325  0.771201  0.106515  0.003741   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "45564   41044.0 -0.577343  1.200293  0.969770 -0.135319  1.489512 -0.251285   \n",
       "45565  153360.0  2.025756 -0.050818 -1.423694  0.095830  0.549865 -0.109934   \n",
       "45566  118740.0 -0.223272  1.965183 -0.142978  4.371084  0.862892  0.534504   \n",
       "45567  145003.0 -0.795843 -0.045877 -0.789747  0.432049  0.778602  4.688147   \n",
       "45568   37442.0 -1.521083 -0.851251  1.728070 -2.220317 -1.406698 -1.030617   \n",
       "\n",
       "             V7        V8        V9  ...       V21       V22       V23  \\\n",
       "0      1.922066 -0.649116 -0.797983  ...  0.046164  0.272604 -0.286842   \n",
       "1     -0.524892  0.566502 -0.583604  ...  0.029318  0.150226 -0.861510   \n",
       "2     -0.043126 -0.114222  0.870073  ...  0.142117  0.172234  0.036681   \n",
       "3      0.255022  0.640365 -1.588047  ...  0.227404  0.313352  0.719519   \n",
       "4     -0.162653  0.071036  0.308806  ... -0.177230 -0.274791 -0.216183   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "45564  1.319183 -0.360093 -0.545662  ...  0.004469  0.324550 -0.423130   \n",
       "45565  0.125709 -0.094079  0.364987  ... -0.219888 -0.507831  0.250445   \n",
       "45566  0.703937  0.424352 -2.238565  ...  0.088413  0.341840 -0.033551   \n",
       "45567 -1.813848  2.047509  0.125861  ...  0.519562  1.279315  0.271045   \n",
       "45568 -0.846418  0.471451 -2.240889  ...  0.080307  0.144757 -0.058467   \n",
       "\n",
       "            V24       V25       V26       V27       V28  Amount  Class  \n",
       "0      0.621032  0.080475  0.108701  0.046224  0.123110    0.30      0  \n",
       "1     -0.163730 -0.210010 -0.245811  0.605932 -0.486416  108.86      0  \n",
       "2     -0.121723 -0.148125 -0.681985  0.008653 -0.007497  179.00      0  \n",
       "3     -0.067816 -0.091712 -0.451338 -0.153241 -0.125369  310.96      0  \n",
       "4     -0.543854  0.644877  0.443618 -0.000127  0.025547   12.31      0  \n",
       "...         ...       ...       ...       ...       ...     ...    ...  \n",
       "45564 -0.443144  0.049154 -0.568848 -0.317382 -0.307767    1.00      0  \n",
       "45565  0.211346 -0.051542 -0.607790 -0.011727 -0.054534    9.99      0  \n",
       "45566  0.593325 -0.483217  0.372115  0.319060  0.212642   15.13      0  \n",
       "45567  0.784797 -1.871832  0.575462  0.082396  0.065257   99.90      0  \n",
       "45568  0.344586  0.447243 -0.174573  0.168484 -0.033630   87.00      0  \n",
       "\n",
       "[45569 rows x 31 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Got unexpected keys in input_dict: {'Class'}\nexpected: {'V8', 'V6', 'V16', 'V21', 'V9', 'Amount', 'V12', 'V5', 'V4', 'V2', 'V22', 'V28', 'V7', 'V13', 'V23', 'V26', 'V10', 'V3', 'V27', 'V1', 'V20', 'V14', 'V17', 'V15', 'V19', 'V11', 'V18', 'V24', 'V25'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-920da583c559>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mloaded_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0meval_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./input/valid.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/contrib/predictor/predictor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m     68\u001b[0m       raise ValueError(\n\u001b[1;32m     69\u001b[0m           'Got unexpected keys in input_dict: {}\\nexpected: {}'.format(\n\u001b[0;32m---> 70\u001b[0;31m               unexpected_keys, expected_keys))\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Got unexpected keys in input_dict: {'Class'}\nexpected: {'V8', 'V6', 'V16', 'V21', 'V9', 'Amount', 'V12', 'V5', 'V4', 'V2', 'V22', 'V28', 'V7', 'V13', 'V23', 'V26', 'V10', 'V3', 'V27', 'V1', 'V20', 'V14', 'V17', 'V15', 'V19', 'V11', 'V18', 'V24', 'V25'}"
     ]
    }
   ],
   "source": [
    "result =loaded_model({'Class':eval_input_fn('./input/valid.csv')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = eval_input_fn('./input/valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.63647000e+05,  1.29017143e-01,  1.89607101e+00, -1.90927731e+00,\n",
       "       -8.54673894e-02,  1.51637077e+00, -2.94719255e+00,  1.92206567e+00,\n",
       "       -6.49115892e-01, -7.97982927e-01, -2.45847883e+00,  1.35283327e+00,\n",
       "       -3.83150328e-01, -1.67107749e-01, -3.74494021e+00,  2.59790888e-01,\n",
       "        5.05714448e-01,  3.32977889e+00,  1.16454320e+00, -1.08663729e+00,\n",
       "       -8.29925244e-02,  4.61643100e-02,  2.72603765e-01, -2.86842273e-01,\n",
       "        6.21031777e-01,  8.04751408e-02,  1.08701061e-01,  4.62241959e-02,\n",
       "        1.23110432e-01,  3.00000000e-01])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Got unexpected keys in input_dict: {'Class'}\nexpected: {'V8', 'V6', 'V16', 'V21', 'V9', 'Amount', 'V12', 'V5', 'V4', 'V2', 'V22', 'V28', 'V7', 'V13', 'V23', 'V26', 'V10', 'V3', 'V27', 'V1', 'V20', 'V14', 'V17', 'V15', 'V19', 'V11', 'V18', 'V24', 'V25'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-f3202db2e9bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Create a confusion matrix on training data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_eval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Class\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mget_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./input/valid.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mcm_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-f3202db2e9bd>\u001b[0m in \u001b[0;36mget_predictions\u001b[0;34m(estimator, input_fn)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mloaded_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'probabilities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/contrib/predictor/predictor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m     68\u001b[0m       raise ValueError(\n\u001b[1;32m     69\u001b[0m           'Got unexpected keys in input_dict: {}\\nexpected: {}'.format(\n\u001b[0;32m---> 70\u001b[0;31m               unexpected_keys, expected_keys))\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Got unexpected keys in input_dict: {'Class'}\nexpected: {'V8', 'V6', 'V16', 'V21', 'V9', 'Amount', 'V12', 'V5', 'V4', 'V2', 'V22', 'V28', 'V7', 'V13', 'V23', 'V26', 'V10', 'V3', 'V27', 'V1', 'V20', 'V14', 'V17', 'V15', 'V19', 'V11', 'V18', 'V24', 'V25'}"
     ]
    }
   ],
   "source": [
    "# Create a confusion matrix on training data.\n",
    "with tf.Graph().as_default():\n",
    "    cm = tf.confusion_matrix(df_eval[\"Class\"],get_predictions(loaded_model, eval_input_fn('./input/valid.csv')))\n",
    "    with tf.Session() as session:\n",
    "        cm_out = session.run(cm)\n",
    "        print(cm_out)\n",
    "\n",
    "# Normalize the confusion matrix so that each row sums to 1.\n",
    "cm_out = cm_out.astype(float) \n",
    "# / cm_out.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "sns.heatmap(cm_out, annot=True, xticklabels=LABELS, yticklabels=LABELS);\n",
    "plt.xlabel(\"Predicted\");\n",
    "plt.ylabel(\"True\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# yamul파일 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hyperparam.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "    scaleTier: PREMIUM_1\n",
    "    hyperparameters:\n",
    "        hyperparameterMetricTag: RMSE\n",
    "        goal: MINIMIZE\n",
    "        maxTrials: 4\n",
    "        maxParallelTrials: 2\n",
    "        enableTrialEarlyStopping: True\n",
    "        params:\n",
    "        - parameterName: nnsize\n",
    "          type: CATEGORICAL\n",
    "          categoricalValues:\n",
    "          - 30,10\n",
    "          - 100,10\n",
    "          - 30,100\n",
    "          - 30,10,30        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: miniproj_dnnclass_191210_071526\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [miniproj_dnnclass_191210_071526] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe miniproj_dnnclass_191210_071526\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs miniproj_dnnclass_191210_071526\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# MODEL_NAME=linear\n",
    "DATADIR=$(pwd)\n",
    "OUTDIR=gs://${BUCKET}/${PROJ_NAME}/${MODEL_NAME}_HParam\n",
    "JOBNAME=${PROJ_NAME}_${MODEL_NAME}_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "rm -rf $OUTDIR\n",
    "gcloud ai-platform jobs submit training $JOBNAME \\\n",
    "    --region=$REGION \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=${PWD}/${PROJ_NAME}/trainer \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --scale-tier=PREMIUM_1 \\\n",
    "    --runtime-version=$TFVERSION \\\n",
    "    --config=hyperparam.yaml \\\n",
    "    -- \\\n",
    "    --train_data_path=\"gs://${BUCKET}/${PROJ_NAME}/input/train_smote.csv\" \\\n",
    "    --eval_data_path=\"gs://${BUCKET}/${PROJ_NAME}/input/valid.csv\"  \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --model=${MODEL_NAME} \\\n",
    "    --train_steps=2000 \\\n",
    "    --feature_length=$FEATURE_LEN \\\n",
    "    --learning_rate=0.0001 \\\n",
    "    --eval_delay_secs=1 \\\n",
    "    --min_eval_frequency=10 \\\n",
    "    --stop_loss=1 \\\n",
    "    --nnsize='30,10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 코드 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ${PROJ_NAME}_v1/trainer\n",
    "touch ${PROJ_NAME}_v1/__init__.py\n",
    "touch ${PROJ_NAME}_v1/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting miniproj_v1/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PROJ_NAME}_v1/trainer/task.py\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Example implementation of code to run on the Cloud ML service.\n",
    "\"\"\"\n",
    "\n",
    "import traceback\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from . import model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  # Input Arguments\n",
    "  parser.add_argument(\n",
    "      '--train_data_path',\n",
    "      help='GCS or local path to training data',\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--eval_data_path',\n",
    "      help='GCS or local path to evaluation data',\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--train_batch_size',\n",
    "      help='Batch size for training steps',\n",
    "      type=int,\n",
    "      default=100\n",
    "  )\n",
    "  parser.add_argument(\n",
    "        \"--nnsize\",\n",
    "        help=\"Hidden layer sizes to use for DNN feature columns -- provide \\\n",
    "        space-separated layers\",\n",
    "        type=str,\n",
    "        default='128,32,4'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--learning_rate',\n",
    "      help='Initial learning rate for training',\n",
    "      type=float,\n",
    "      default=0.01\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--train_steps',\n",
    "      help=\"\"\"\\\n",
    "      Steps to run the training job for. A step is one batch-size,\\\n",
    "      \"\"\",\n",
    "      type=int,\n",
    "      default=0\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--feature_length',\n",
    "      help=\"\"\"\\\n",
    "      This model works with fixed length sequences. 1-(N-1) are inputs, last is output\n",
    "      \"\"\",\n",
    "      type=int,\n",
    "      default=10\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--output_dir',\n",
    "      help='GCS location to write checkpoints and export models',\n",
    "      required=True\n",
    "  )\n",
    "  model_names = [name.replace('_model','') \\\n",
    "                   for name in dir(model) \\\n",
    "                     if name.endswith('_model')]\n",
    "  parser.add_argument(\n",
    "      '--model',\n",
    "      help='Type of model. Supported types are {}'.format(model_names),\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--job-dir',\n",
    "      help='this model ignores this field, but it is required by gcloud',\n",
    "      default='junk'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--eval_delay_secs',\n",
    "      help='How long to wait before running first evaluation',\n",
    "      default=10,\n",
    "      type=int\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--min_eval_frequency',\n",
    "      help='Minimum number of training steps between evaluations',\n",
    "      default=60,\n",
    "      type=int\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--stop_loss',\n",
    "      help='Value of Early Stopping Loss',\n",
    "      default=1,\n",
    "      type=int\n",
    "  )\n",
    "\n",
    "  args = parser.parse_args()\n",
    "  hparams = args.__dict__\n",
    "  \n",
    "  # unused args provided by service\n",
    "  hparams.pop('job_dir', None)\n",
    "  hparams.pop('job-dir', None)\n",
    "\n",
    "  output_dir = hparams.pop('output_dir')\n",
    "  model.BATCH_SIZE = hparams['train_batch_size']\n",
    "  model.NNSIZE = list(map(int, hparams.pop(\"nnsize\").split(\",\")))\n",
    "  print (\"Will use DNN size of {}\".format(model.NNSIZE))\n",
    "\n",
    "  # Append trial_id to path if we are doing hptuning\n",
    "  # This code can be removed if you are not using hyperparameter tuning\n",
    "  output_dir = os.path.join(\n",
    "      output_dir,\n",
    "      json.loads(\n",
    "          os.environ.get('TF_CONFIG', '{}')\n",
    "      ).get('task', {}).get('trial', '')\n",
    "  )\n",
    "\n",
    "  # calculate train_steps if not provided\n",
    "  if hparams['train_steps'] < 1:\n",
    "     # 1,000 steps at batch_size of 100\n",
    "     hparams['train_steps'] = (1000 * 100) // hparams['train_batch_size']\n",
    "     print (\"Training for {} steps\".format(hparams['train_steps']))\n",
    "\n",
    "  model.init(hparams)\n",
    "\n",
    "  # Run the training job\n",
    "  model.train_and_evaluate(output_dir, hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting miniproj_v1/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PROJ_NAME}_v1/trainer/model.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "LABLE_COL = 'Class'\n",
    "N_OUTPUTS = 1  # in each sequence, 1-49 are features, and 50 is label\n",
    "FEATURE_LEN = None\n",
    "DEFAULTS = None\n",
    "N_INPUTS = None\n",
    "BATCH_SIZE = 512\n",
    "NNSIZE = [64, 16, 4]\n",
    "\n",
    "\n",
    "def init(hparams):\n",
    "    global FEATURE_LEN, DEFAULTS, N_INPUTS\n",
    "    FEATURE_LEN = hparams['feature_length']\n",
    "    DEFAULTS = [[0.0] for x in range(0, FEATURE_LEN)]\n",
    "    N_INPUTS = FEATURE_LEN - N_OUTPUTS\n",
    "\n",
    "\n",
    "def linear_model(features, mode, params):\n",
    "    X = features[LABLE_COL]\n",
    "    predictions = tf.layers.dense(X, 1, activation=None)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def dnn_model(features, mode, params):\n",
    "    X = features[LABLE_COL]\n",
    "    print('NNSIZE:{}'.format(NNSIZE))\n",
    "    for unit_num in NNSIZE:\n",
    "        X = tf.layers.dense(X, units=unit_num, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(X, 1, activation=None)  # linear output: regression\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def dnnclass_model(features, mode, params):\n",
    "    X = features[LABLE_COL]\n",
    "    print(x.shape)\n",
    "    print('*'*100)\n",
    "    X = tf.Print(X, [tf.shape(X), X], summarize=-1)\n",
    "    print('NNSIZE:{}'.format(NNSIZE))\n",
    "    for unit_num in NNSIZE:\n",
    "        X = tf.layers.dense(X, units=unit_num, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(X, 1, activation=tf.nn.softmax)  # logistic output\n",
    "\n",
    "\n",
    "def cnn_model(features, mode, params):\n",
    "    X = tf.reshape(features[LABLE_COL],\n",
    "                   [-1, N_INPUTS, 1])  # as a 1D \"sequence\" with only one time-series observation (height)\n",
    "    c1 = tf.layers.conv1d(X, filters=N_INPUTS // 2,\n",
    "                          kernel_size=3, strides=1,\n",
    "                          padding='same', activation=tf.nn.relu)\n",
    "    p1 = tf.layers.max_pooling1d(c1, pool_size=2, strides=2)\n",
    "\n",
    "    c2 = tf.layers.conv1d(p1, filters=N_INPUTS // 2,\n",
    "                          kernel_size=3, strides=1,\n",
    "                          padding='same', activation=tf.nn.relu)\n",
    "    p2 = tf.layers.max_pooling1d(c2, pool_size=2, strides=2)\n",
    "\n",
    "    outlen = p2.shape[1] * p2.shape[2]\n",
    "    c2flat = tf.reshape(p2, [-1, outlen])\n",
    "    h1 = tf.layers.dense(c2flat, 3, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h1, 1, activation=None)  # linear output: regression\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def rnn_model(features, mode, params):\n",
    "    CELL_SIZE = N_INPUTS // 3  # size of the internal state in each of the cells\n",
    "\n",
    "    # 1. dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
    "    x = tf.reshape(features[LABLE_COL], [-1, N_INPUTS, 1])\n",
    "\n",
    "    # 2. configure the RNN\n",
    "    cell = tf.nn.rnn_cell.GRUCell(CELL_SIZE)\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)\n",
    "\n",
    "    # 3. pass rnn output through a dense layer\n",
    "    h1 = tf.layers.dense(state, N_INPUTS // 2, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h1, 1, activation=None)  # (?, 1)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# 2-layer RNN\n",
    "def rnn2_model(features, mode, params):\n",
    "    # dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
    "    x = tf.reshape(features[LABLE_COL], [-1, N_INPUTS, 1])\n",
    "\n",
    "    # 2. configure the RNN\n",
    "    cell1 = tf.nn.rnn_cell.GRUCell(N_INPUTS * 2)\n",
    "    cell2 = tf.nn.rnn_cell.GRUCell(N_INPUTS // 2)\n",
    "    cells = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
    "    outputs, state = tf.nn.dynamic_rnn(cells, x, dtype=tf.float32)\n",
    "    # 'state' is now a tuple containing the final state of each cell layer\n",
    "    # we use state[1] below to extract the final state of the final layer\n",
    "    \n",
    "    # 3. pass rnn output through a dense layer\n",
    "    h1 = tf.layers.dense(state[1], cells.output_size // 2, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h1, 1, activation=None)  # (?, 1)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# create N-1 predictions\n",
    "def rnnN_model(features, mode, params):\n",
    "    # dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
    "    x = tf.reshape(features[LABLE_COL], [-1, N_INPUTS, 1])\n",
    "\n",
    "    # 2. configure the RNN\n",
    "    cell1 = tf.nn.rnn_cell.GRUCell(N_INPUTS * 2)\n",
    "    cell2 = tf.nn.rnn_cell.GRUCell(N_INPUTS // 2)\n",
    "    cells = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
    "    outputs, state = tf.nn.dynamic_rnn(cells, x, dtype=tf.float32)\n",
    "    # 'outputs' contains the state of the final layer for every time step\n",
    "    # not just the last time step (?,N_INPUTS, final cell size)\n",
    "    \n",
    "    # 3. pass state for each time step through a DNN, to get a prediction\n",
    "    # for each time step \n",
    "    h1 = tf.layers.dense(outputs, cells.output_size, activation=tf.nn.relu)\n",
    "    h2 = tf.layers.dense(h1, cells.output_size // 2, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h2, 1, activation=None)  # (?, N_INPUTS, 1)\n",
    "    predictions = tf.reshape(predictions, [-1, N_INPUTS])\n",
    "    return predictions # return prediction for each time step\n",
    "\n",
    "\n",
    "# read data and convert to needed format\n",
    "def read_dataset(filename, mode, batch_size=512):\n",
    "    def _input_fn():\n",
    "        def decode_csv(row):\n",
    "            # row is a string tensor containing the contents of one row\n",
    "            features = tf.decode_csv(row, record_defaults=DEFAULTS)  # string tensor -> list of 50 rank 0 float tensors\n",
    "            label = features.pop()  # remove last feature and use as label\n",
    "            features = tf.stack(features)  # list of rank 0 tensors -> single rank 1 tensor\n",
    "            return {LABLE_COL: features}, label\n",
    "\n",
    "        # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "        dataset = tf.data.Dataset.list_files(filename)\n",
    "        # Read in data from files\n",
    "        dataset = dataset.flat_map(tf.data.TextLineDataset)\n",
    "        # Parse text lines as comma-separated values (CSV)\n",
    "        \n",
    "        dataset = dataset.skip(1).map(decode_csv)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None  # loop indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size=10 * batch_size)\n",
    "        else:\n",
    "            num_epochs = 1  # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "    return _input_fn\n",
    "\n",
    "\n",
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "        LABLE_COL: tf.placeholder(tf.float32, [None, N_INPUTS])\n",
    "    }\n",
    "\n",
    "    features = {\n",
    "        key: tf.expand_dims(tensor, -1)\n",
    "        for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    features[LABLE_COL] = tf.squeeze(features[LABLE_COL], axis=[2])\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n",
    "\n",
    "\n",
    "def compute_errors(features, labels, predictions):\n",
    "    labels = tf.expand_dims(labels, -1)  # rank 1 -> rank 2 to match rank of predictions\n",
    "\n",
    "    if predictions.shape[1] == 1:\n",
    "#        loss = tf.losses.mean_squared_error(labels, predictions)\n",
    "        loss = tf.losses.sigmoid_cross_entropy(labels, predictions)\n",
    "        rmse = tf.metrics.root_mean_squared_error(labels, predictions)\n",
    "        return loss, rmse\n",
    "    else:\n",
    "        # one prediction for every input in sequence\n",
    "        # get 1-N of (x + label)\n",
    "        labelsN = tf.concat([features[LABLE_COL], labels], axis=1)\n",
    "        labelsN = labelsN[:, 1:]\n",
    "        # loss is computed from the last 1/3 of the series\n",
    "        N = (2 * N_INPUTS) // 3\n",
    "#        loss = tf.losses.mean_squared_error(labelsN[:, N:], predictions[:, N:])\n",
    "        loss = tf.losses.sigmoid_cross_entropy(labelsN[:, N:], predictions[:, N:])\n",
    "        # rmse is computed from last prediction and last label\n",
    "        lastPred = predictions[:, -1]\n",
    "        rmse = tf.metrics.root_mean_squared_error(labels, lastPred)\n",
    "        return loss, rmse\n",
    "\n",
    "# RMSE when predicting same as last value\n",
    "def same_as_last_benchmark(features, labels):\n",
    "    predictions = features[LABLE_COL][:,-1] # last value in input sequence\n",
    "    return tf.metrics.root_mean_squared_error(labels, predictions)\n",
    "\n",
    "\n",
    "# create the inference model\n",
    "def data_regressor(features, labels, mode, params):\n",
    "    # 1. run the appropriate model\n",
    "    model_functions = {\n",
    "        'linear': linear_model,\n",
    "        'dnn': dnn_model,\n",
    "        'dnnclass': dnn_model,\n",
    "        'cnn': cnn_model,\n",
    "        'rnn': rnn_model,\n",
    "        'rnn2': rnn2_model,\n",
    "        'rnnN': rnnN_model}\n",
    "    model_function = model_functions[params['model']]\n",
    "    predictions = model_function(features, mode, params)\n",
    "\n",
    "    # 2. loss function, training/eval ops\n",
    "    loss = None\n",
    "    train_op = None\n",
    "    eval_metric_ops = None\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
    "        loss, rmse = compute_errors(features, labels, predictions)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            # this is needed for batch normalization, but has no effect otherwise\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                # 2b. set up training operation\n",
    "                train_op = tf.contrib.layers.optimize_loss(\n",
    "                    loss,\n",
    "                    tf.train.get_global_step(),\n",
    "                    learning_rate=params['learning_rate'],\n",
    "                    optimizer=\"Adam\")\n",
    "\n",
    "        # 2c. eval metric\n",
    "        eval_metric_ops = {\n",
    "            \"RMSE\": rmse,\n",
    "            \"RMSE_same_as_last\": same_as_last_benchmark(features, labels),\n",
    "        }\n",
    "\n",
    "    # 3. Create predictions\n",
    "    if predictions.shape[1] != 1:\n",
    "        predictions = predictions[:, -1]  # last predicted value\n",
    "    predictions_dict = {\"predicted\": predictions}\n",
    "\n",
    "    # 4. return EstimatorSpec\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions_dict,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops=eval_metric_ops,\n",
    "        export_outputs={\n",
    "            'predictions': tf.estimator.export.PredictOutput(predictions_dict)}\n",
    "    )\n",
    "\n",
    "\n",
    "def train_and_evaluate(output_dir, hparams):\n",
    "    tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "    get_train = read_dataset(hparams['train_data_path'],\n",
    "                             tf.estimator.ModeKeys.TRAIN,\n",
    "                             hparams['train_batch_size'])\n",
    "    get_valid = read_dataset(hparams['eval_data_path'],\n",
    "                             tf.estimator.ModeKeys.EVAL,\n",
    "                             1000)\n",
    "    \n",
    "    estimator = tf.estimator.Estimator(model_fn=data_regressor,\n",
    "                                       params=hparams,\n",
    "                                       config=tf.estimator.RunConfig(\n",
    "                                           save_checkpoints_secs=\n",
    "                                           hparams['min_eval_frequency']),\n",
    "                                       model_dir=output_dir)\n",
    "   \n",
    "    train_spec = tf.estimator.TrainSpec(input_fn=get_train,\n",
    "                                        max_steps=hparams['train_steps'],\n",
    "                                        hooks=[tf.estimator.experimental.stop_if_no_decrease_hook(estimator, \"loss\", hparams['stop_loss'])])\n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=get_valid,\n",
    "                                      steps=None,\n",
    "                                      exporters=exporter,\n",
    "                                      start_delay_secs=hparams['eval_delay_secs'],\n",
    "                                      throttle_secs=hparams['min_eval_frequency'])\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
