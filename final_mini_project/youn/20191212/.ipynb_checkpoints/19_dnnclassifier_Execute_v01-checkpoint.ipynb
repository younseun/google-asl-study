{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"qwiklabs-gcp-ml-49b827b781ab\"  # Replace with your PROJECT\n",
    "BUCKET = PROJECT  # Replace with your BUCKET\n",
    "REGION = \"us-central1\"            # Choose an available region for Cloud CAIP\n",
    "TFVERSION = \"1.14\"                # TF version for CMLE to use\n",
    "PROJ_NAME='miniproj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = TFVERSION\n",
    "os.environ[\"PROJ_NAME\"] = PROJ_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_LEN = 31\n",
    "os.environ['FEATURE_LEN'] = str(FEATURE_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From miniproj/trainer/model.py:24: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:24: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:267: The name tf.summary.FileWriterCache is deprecated. Please use tf.compat.v1.summary.FileWriterCache instead.\n",
      "\n",
      "INFO:tensorflow:TF_CONFIG environment variable: {u'environment': u'cloud', u'cluster': {}, u'job': {u'args': [u'--train_data_path=/home/jupyter/google-asl-study/final_mini_project/youn/input/train_smote.csv', u'--eval_data_path=/home/jupyter/google-asl-study/final_mini_project/youn/input/valid.csv', u'--output_dir=/home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj', u'--model=dnnclass', u'--train_steps=2000', u'--feature_length=31', u'--learning_rate=0.0001', u'--eval_delay_secs=1', u'--min_eval_frequency=10', u'--stop_loss=1'], u'job_name': u'miniproj.trainer.task'}, u'task': {}}\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 10, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4579c67e10>, '_model_dir': '/home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_session_creation_timeout_secs': 7200, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_experimental_max_worker_delay_secs': None, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 10.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/training_util.py:236: initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:12: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.decode_csv is deprecated. Please use tf.io.decode_csv instead.\n",
      "\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:164: make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:48: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/layers/core.py:187: apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:188: The name tf.losses.sigmoid_cross_entropy is deprecated. Please use tf.compat.v1.losses.sigmoid_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:189: The name tf.metrics.root_mean_squared_error is deprecated. Please use tf.compat.v1.metrics.root_mean_squared_error instead.\n",
      "\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:234: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:234: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:239: The name tf.train.get_global_step is deprecated. Please use tf.compat.v1.train.get_global_step instead.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2019-12-10 05:40:56.725185: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2019-12-10 05:40:56.738781: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2019-12-10 05:40:56.740664: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fd4a1dfa20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2019-12-10 05:40:56.740704: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2019-12-10 05:40:56.741967: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt.\n",
      "INFO:tensorflow:loss = 3114.09, step = 1\n",
      "INFO:tensorflow:global_step/sec: 47.1225\n",
      "INFO:tensorflow:loss = 883.79095, step = 101 (2.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.7666\n",
      "INFO:tensorflow:loss = 5.6979113, step = 201 (1.433 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.3293\n",
      "INFO:tensorflow:loss = 5.70459, step = 301 (1.631 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.3957\n",
      "INFO:tensorflow:loss = 2.773248, step = 401 (1.529 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.3694\n",
      "INFO:tensorflow:loss = 2.845654, step = 501 (1.530 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 586 into /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-10T05:41:07Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt-586\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-10-05:41:15\n",
      "INFO:tensorflow:Saving dict for global step 586: RMSE = 6.597429, RMSE_same_as_last = 245.8532, global_step = 586, loss = 0.79996634\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 586: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt-586\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:171: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default', 'predictions']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt-586\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/export/exporter/temp-1575956475/saved_model.pb\n",
      "INFO:tensorflow:global_step/sec: 10.5744\n",
      "INFO:tensorflow:loss = 0.626594, step = 601 (9.457 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.0276\n",
      "INFO:tensorflow:loss = 0.6194699, step = 701 (1.586 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 714 into /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-10T05:41:17Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt-714\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-10-05:41:25\n",
      "INFO:tensorflow:Saving dict for global step 714: RMSE = 11.532954, RMSE_same_as_last = 245.8532, global_step = 714, loss = 0.10438882\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 714: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt-714\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default', 'predictions']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt-714\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/export/exporter/temp-1575956485/saved_model.pb\n",
      "INFO:tensorflow:global_step/sec: 10.136\n",
      "INFO:tensorflow:loss = 0.30334687, step = 801 (9.866 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 825 into /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-10T05:41:27Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt-825\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-10-05:41:35\n",
      "INFO:tensorflow:Saving dict for global step 825: RMSE = 9.900751, RMSE_same_as_last = 245.8532, global_step = 825, loss = 0.114644945\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 825: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt-825\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default', 'predictions']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt-825\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/export/exporter/temp-1575956495/saved_model.pb\n",
      "INFO:tensorflow:global_step/sec: 9.82234\n",
      "INFO:tensorflow:loss = 1.2524878, step = 901 (10.181 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 911 into /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-10T05:41:37Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt-911\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-10-05:41:45\n",
      "INFO:tensorflow:Saving dict for global step 911: RMSE = 6.454704, RMSE_same_as_last = 245.8532, global_step = 911, loss = 0.1729577\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 911: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt-911\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default', 'predictions']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt-911\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/export/exporter/temp-1575956505/saved_model.pb\n",
      "INFO:tensorflow:Saving checkpoints for 993 into /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-10T05:41:47Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt-993\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-10-05:41:55\n",
      "INFO:tensorflow:Saving dict for global step 993: RMSE = 6.202011, RMSE_same_as_last = 245.8532, global_step = 993, loss = 0.16565795\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 993: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt-993\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default', 'predictions']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt-993\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/export/exporter/temp-1575956515/saved_model.pb\n",
      "INFO:tensorflow:global_step/sec: 5.33105\n",
      "INFO:tensorflow:loss = 0.3923967, step = 1001 (18.758 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1080 into /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (10 secs).\n",
      "INFO:tensorflow:global_step/sec: 52.4873\n",
      "INFO:tensorflow:loss = 0.25538844, step = 1101 (1.905 sec)\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/summary/summary_iterator.py:68: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "INFO:tensorflow:No decrease in metric \"loss\" for 111 steps, which is greater than or equal to max steps (1) configured for early stopping.\n",
      "INFO:tensorflow:Requesting early stopping at global step 1102\n",
      "INFO:tensorflow:Saving checkpoints for 1102 into /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-10T05:41:58Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt-1102\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-10-05:42:06\n",
      "INFO:tensorflow:Saving dict for global step 1102: RMSE = 4.375767, RMSE_same_as_last = 245.8532, global_step = 1102, loss = 0.2558745\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1102: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt-1102\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default', 'predictions']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/model.ckpt-1102\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj/export/exporter/temp-1575956526/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 0.6880618.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "MODEL_NAME=dnnclass\n",
    "DATADIR=$(pwd)\n",
    "OUTDIR=$(pwd)/trained_${MODEL_NAME}/${PROJ_NAME}\n",
    "rm -rf $OUTDIR\n",
    "gcloud ai-platform local train \\\n",
    "   --module-name=${PROJ_NAME}.trainer.task \\\n",
    "   --package-path=${PWD}/${PROJ_NAME}.trainer \\\n",
    "   -- \\\n",
    "   --train_data_path=\"${DATADIR}/input/train_smote.csv\" \\\n",
    "   --eval_data_path=\"${DATADIR}/input/valid.csv\"  \\\n",
    "   --output_dir=${OUTDIR} \\\n",
    "   --model=${MODEL_NAME} \\\n",
    "   --train_steps=2000 \\\n",
    "   --feature_length=$FEATURE_LEN \\\n",
    "   --learning_rate=0.0001 \\\n",
    "   --eval_delay_secs=1 \\\n",
    "   --min_eval_frequency=10 \\\n",
    "   --stop_loss=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 코드 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ${PROJ_NAME}/trainer\n",
    "touch ${PROJ_NAME}/__init__.py\n",
    "touch ${PROJ_NAME}/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting miniproj/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PROJ_NAME}/trainer/task.py\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Example implementation of code to run on the Cloud ML service.\n",
    "\"\"\"\n",
    "\n",
    "import traceback\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from . import model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  # Input Arguments\n",
    "  parser.add_argument(\n",
    "      '--train_data_path',\n",
    "      help='GCS or local path to training data',\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--eval_data_path',\n",
    "      help='GCS or local path to evaluation data',\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--train_batch_size',\n",
    "      help='Batch size for training steps',\n",
    "      type=int,\n",
    "      default=100\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--learning_rate',\n",
    "      help='Initial learning rate for training',\n",
    "      type=float,\n",
    "      default=0.01\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--train_steps',\n",
    "      help=\"\"\"\\\n",
    "      Steps to run the training job for. A step is one batch-size,\\\n",
    "      \"\"\",\n",
    "      type=int,\n",
    "      default=0\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--feature_length',\n",
    "      help=\"\"\"\\\n",
    "      This model works with fixed length sequences. 1-(N-1) are inputs, last is output\n",
    "      \"\"\",\n",
    "      type=int,\n",
    "      default=10\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--output_dir',\n",
    "      help='GCS location to write checkpoints and export models',\n",
    "      required=True\n",
    "  )\n",
    "  model_names = [name.replace('_model','') \\\n",
    "                   for name in dir(model) \\\n",
    "                     if name.endswith('_model')]\n",
    "  parser.add_argument(\n",
    "      '--model',\n",
    "      help='Type of model. Supported types are {}'.format(model_names),\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--job-dir',\n",
    "      help='this model ignores this field, but it is required by gcloud',\n",
    "      default='junk'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--eval_delay_secs',\n",
    "      help='How long to wait before running first evaluation',\n",
    "      default=10,\n",
    "      type=int\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--min_eval_frequency',\n",
    "      help='Minimum number of training steps between evaluations',\n",
    "      default=60,\n",
    "      type=int\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--stop_loss',\n",
    "      help='Value of Early Stopping Loss',\n",
    "      default=1,\n",
    "      type=int\n",
    "  )\n",
    "\n",
    "  args = parser.parse_args()\n",
    "  hparams = args.__dict__\n",
    "  \n",
    "  # unused args provided by service\n",
    "  hparams.pop('job_dir', None)\n",
    "  hparams.pop('job-dir', None)\n",
    "\n",
    "  output_dir = hparams.pop('output_dir')\n",
    "\n",
    "  # Append trial_id to path if we are doing hptuning\n",
    "  # This code can be removed if you are not using hyperparameter tuning\n",
    "  output_dir = os.path.join(\n",
    "      output_dir,\n",
    "      json.loads(\n",
    "          os.environ.get('TF_CONFIG', '{}')\n",
    "      ).get('task', {}).get('trial', '')\n",
    "  )\n",
    "\n",
    "  # calculate train_steps if not provided\n",
    "  if hparams['train_steps'] < 1:\n",
    "     # 1,000 steps at batch_size of 100\n",
    "     hparams['train_steps'] = (1000 * 100) // hparams['train_batch_size']\n",
    "     print (\"Training for {} steps\".format(hparams['train_steps']))\n",
    "\n",
    "  model.init(hparams)\n",
    "\n",
    "  # Run the training job\n",
    "  model.train_and_evaluate(output_dir, hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting miniproj/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PROJ_NAME}/trainer/model.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "LABLE_COL = 'height'\n",
    "N_OUTPUTS = 1  # in each sequence, 1-49 are features, and 50 is label\n",
    "FEATURE_LEN = None\n",
    "DEFAULTS = None\n",
    "N_INPUTS = None\n",
    "\n",
    "\n",
    "def init(hparams):\n",
    "    global FEATURE_LEN, DEFAULTS, N_INPUTS\n",
    "    FEATURE_LEN = hparams['feature_length']\n",
    "    DEFAULTS = [[0.0] for x in range(0, FEATURE_LEN)]\n",
    "    N_INPUTS = FEATURE_LEN - N_OUTPUTS\n",
    "\n",
    "\n",
    "def linear_model(features, mode, params):\n",
    "    X = features[LABLE_COL]\n",
    "    predictions = tf.layers.dense(X, 1, activation=None)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def dnn_model(features, mode, params):\n",
    "    X = features[LABLE_COL]\n",
    "    h1 = tf.layers.dense(X, 30, activation=tf.nn.relu)\n",
    "    h2 = tf.layers.dense(h1, 3, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h1, 1, activation=None)  # linear output: regression\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def dnnclass_model(features, mode, params):\n",
    "    X = features[LABLE_COL]\n",
    "    h1 = tf.layers.dense(X, 30, activation=tf.nn.relu)\n",
    "    h2 = tf.layers.dense(h1, 10, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h2, 1, activation=tf.nn.softmax)  # linear output: regression\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def cnn_model(features, mode, params):\n",
    "    X = tf.reshape(features[LABLE_COL],\n",
    "                   [-1, N_INPUTS, 1])  # as a 1D \"sequence\" with only one time-series observation (height)\n",
    "    c1 = tf.layers.conv1d(X, filters=N_INPUTS // 2,\n",
    "                          kernel_size=3, strides=1,\n",
    "                          padding='same', activation=tf.nn.relu)\n",
    "    p1 = tf.layers.max_pooling1d(c1, pool_size=2, strides=2)\n",
    "\n",
    "    c2 = tf.layers.conv1d(p1, filters=N_INPUTS // 2,\n",
    "                          kernel_size=3, strides=1,\n",
    "                          padding='same', activation=tf.nn.relu)\n",
    "    p2 = tf.layers.max_pooling1d(c2, pool_size=2, strides=2)\n",
    "\n",
    "    outlen = p2.shape[1] * p2.shape[2]\n",
    "    c2flat = tf.reshape(p2, [-1, outlen])\n",
    "    h1 = tf.layers.dense(c2flat, 3, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h1, 1, activation=None)  # linear output: regression\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def rnn_model(features, mode, params):\n",
    "    CELL_SIZE = N_INPUTS // 3  # size of the internal state in each of the cells\n",
    "\n",
    "    # 1. dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
    "    x = tf.reshape(features[LABLE_COL], [-1, N_INPUTS, 1])\n",
    "\n",
    "    # 2. configure the RNN\n",
    "    cell = tf.nn.rnn_cell.GRUCell(CELL_SIZE)\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)\n",
    "\n",
    "    # 3. pass rnn output through a dense layer\n",
    "    h1 = tf.layers.dense(state, N_INPUTS // 2, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h1, 1, activation=None)  # (?, 1)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# 2-layer RNN\n",
    "def rnn2_model(features, mode, params):\n",
    "    # dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
    "    x = tf.reshape(features[LABLE_COL], [-1, N_INPUTS, 1])\n",
    "\n",
    "    # 2. configure the RNN\n",
    "    cell1 = tf.nn.rnn_cell.GRUCell(N_INPUTS * 2)\n",
    "    cell2 = tf.nn.rnn_cell.GRUCell(N_INPUTS // 2)\n",
    "    cells = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
    "    outputs, state = tf.nn.dynamic_rnn(cells, x, dtype=tf.float32)\n",
    "    # 'state' is now a tuple containing the final state of each cell layer\n",
    "    # we use state[1] below to extract the final state of the final layer\n",
    "    \n",
    "    # 3. pass rnn output through a dense layer\n",
    "    h1 = tf.layers.dense(state[1], cells.output_size // 2, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h1, 1, activation=None)  # (?, 1)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# create N-1 predictions\n",
    "def rnnN_model(features, mode, params):\n",
    "    # dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
    "    x = tf.reshape(features[LABLE_COL], [-1, N_INPUTS, 1])\n",
    "\n",
    "    # 2. configure the RNN\n",
    "    cell1 = tf.nn.rnn_cell.GRUCell(N_INPUTS * 2)\n",
    "    cell2 = tf.nn.rnn_cell.GRUCell(N_INPUTS // 2)\n",
    "    cells = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
    "    outputs, state = tf.nn.dynamic_rnn(cells, x, dtype=tf.float32)\n",
    "    # 'outputs' contains the state of the final layer for every time step\n",
    "    # not just the last time step (?,N_INPUTS, final cell size)\n",
    "    \n",
    "    # 3. pass state for each time step through a DNN, to get a prediction\n",
    "    # for each time step \n",
    "    h1 = tf.layers.dense(outputs, cells.output_size, activation=tf.nn.relu)\n",
    "    h2 = tf.layers.dense(h1, cells.output_size // 2, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h2, 1, activation=None)  # (?, N_INPUTS, 1)\n",
    "    predictions = tf.reshape(predictions, [-1, N_INPUTS])\n",
    "    return predictions # return prediction for each time step\n",
    "\n",
    "\n",
    "# read data and convert to needed format\n",
    "def read_dataset(filename, mode, batch_size=512):\n",
    "    def _input_fn():\n",
    "        def decode_csv(row):\n",
    "            # row is a string tensor containing the contents of one row\n",
    "            features = tf.decode_csv(row, record_defaults=DEFAULTS)  # string tensor -> list of 50 rank 0 float tensors\n",
    "            label = features.pop()  # remove last feature and use as label\n",
    "            features = tf.stack(features)  # list of rank 0 tensors -> single rank 1 tensor\n",
    "            return {LABLE_COL: features}, label\n",
    "\n",
    "        # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "        dataset = tf.data.Dataset.list_files(filename)\n",
    "        # Read in data from files\n",
    "        dataset = dataset.flat_map(tf.data.TextLineDataset)\n",
    "        # Parse text lines as comma-separated values (CSV)\n",
    "        \n",
    "        dataset = dataset.skip(1).map(decode_csv)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None  # loop indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size=10 * batch_size)\n",
    "        else:\n",
    "            num_epochs = 1  # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "    return _input_fn\n",
    "\n",
    "\n",
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "        LABLE_COL: tf.placeholder(tf.float32, [None, N_INPUTS])\n",
    "    }\n",
    "\n",
    "    features = {\n",
    "        key: tf.expand_dims(tensor, -1)\n",
    "        for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    features[LABLE_COL] = tf.squeeze(features[LABLE_COL], axis=[2])\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n",
    "\n",
    "\n",
    "def compute_errors(features, labels, predictions):\n",
    "    labels = tf.expand_dims(labels, -1)  # rank 1 -> rank 2 to match rank of predictions\n",
    "\n",
    "    if predictions.shape[1] == 1:\n",
    "#        loss = tf.losses.mean_squared_error(labels, predictions)\n",
    "        loss = tf.losses.sigmoid_cross_entropy(labels, predictions)\n",
    "        rmse = tf.metrics.root_mean_squared_error(labels, predictions)\n",
    "        return loss, rmse\n",
    "    else:\n",
    "        # one prediction for every input in sequence\n",
    "        # get 1-N of (x + label)\n",
    "        labelsN = tf.concat([features[LABLE_COL], labels], axis=1)\n",
    "        labelsN = labelsN[:, 1:]\n",
    "        # loss is computed from the last 1/3 of the series\n",
    "        N = (2 * N_INPUTS) // 3\n",
    "#        loss = tf.losses.mean_squared_error(labelsN[:, N:], predictions[:, N:])\n",
    "        loss = tf.losses.sigmoid_cross_entropy(labelsN[:, N:], predictions[:, N:])\n",
    "        # rmse is computed from last prediction and last label\n",
    "        lastPred = predictions[:, -1]\n",
    "        rmse = tf.metrics.root_mean_squared_error(labels, lastPred)\n",
    "        return loss, rmse\n",
    "\n",
    "# RMSE when predicting same as last value\n",
    "def same_as_last_benchmark(features, labels):\n",
    "    predictions = features[LABLE_COL][:,-1] # last value in input sequence\n",
    "    return tf.metrics.root_mean_squared_error(labels, predictions)\n",
    "\n",
    "\n",
    "# create the inference model\n",
    "def data_regressor(features, labels, mode, params):\n",
    "    # 1. run the appropriate model\n",
    "    model_functions = {\n",
    "        'linear': linear_model,\n",
    "        'dnn': dnn_model,\n",
    "        'dnnclass': dnn_model,\n",
    "        'cnn': cnn_model,\n",
    "        'rnn': rnn_model,\n",
    "        'rnn2': rnn2_model,\n",
    "        'rnnN': rnnN_model}\n",
    "    model_function = model_functions[params['model']]\n",
    "    predictions = model_function(features, mode, params)\n",
    "\n",
    "    # 2. loss function, training/eval ops\n",
    "    loss = None\n",
    "    train_op = None\n",
    "    eval_metric_ops = None\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
    "        loss, rmse = compute_errors(features, labels, predictions)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            # this is needed for batch normalization, but has no effect otherwise\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                # 2b. set up training operation\n",
    "                train_op = tf.contrib.layers.optimize_loss(\n",
    "                    loss,\n",
    "                    tf.train.get_global_step(),\n",
    "                    learning_rate=params['learning_rate'],\n",
    "                    optimizer=\"Adam\")\n",
    "\n",
    "        # 2c. eval metric\n",
    "        eval_metric_ops = {\n",
    "            \"RMSE\": rmse,\n",
    "            \"RMSE_same_as_last\": same_as_last_benchmark(features, labels),\n",
    "        }\n",
    "\n",
    "    # 3. Create predictions\n",
    "    if predictions.shape[1] != 1:\n",
    "        predictions = predictions[:, -1]  # last predicted value\n",
    "    predictions_dict = {\"predicted\": predictions}\n",
    "\n",
    "    # 4. return EstimatorSpec\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions_dict,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops=eval_metric_ops,\n",
    "        export_outputs={\n",
    "            'predictions': tf.estimator.export.PredictOutput(predictions_dict)}\n",
    "    )\n",
    "\n",
    "\n",
    "def train_and_evaluate(output_dir, hparams):\n",
    "    tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "    get_train = read_dataset(hparams['train_data_path'],\n",
    "                             tf.estimator.ModeKeys.TRAIN,\n",
    "                             hparams['train_batch_size'])\n",
    "    get_valid = read_dataset(hparams['eval_data_path'],\n",
    "                             tf.estimator.ModeKeys.EVAL,\n",
    "                             1000)\n",
    "    \n",
    "    estimator = tf.estimator.Estimator(model_fn=data_regressor,\n",
    "                                       params=hparams,\n",
    "                                       config=tf.estimator.RunConfig(\n",
    "                                           save_checkpoints_secs=\n",
    "                                           hparams['min_eval_frequency']),\n",
    "                                       model_dir=output_dir)\n",
    "   \n",
    "    train_spec = tf.estimator.TrainSpec(input_fn=get_train,\n",
    "                                        max_steps=hparams['train_steps'],\n",
    "                                        hooks=[tf.estimator.experimental.stop_if_no_decrease_hook(estimator, \"loss\", hparams['stop_loss'])])\n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=get_valid,\n",
    "                                      steps=None,\n",
    "                                      exporters=exporter,\n",
    "                                      start_delay_secs=hparams['eval_delay_secs'],\n",
    "                                      throttle_secs=hparams['min_eval_frequency'])\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
