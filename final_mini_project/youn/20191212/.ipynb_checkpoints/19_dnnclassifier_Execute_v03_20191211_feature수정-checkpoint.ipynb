{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"qwiklabs-gcp-ml-49b827b781ab\"  # Replace with your PROJECT\n",
    "BUCKET = PROJECT  # Replace with your BUCKET\n",
    "REGION = \"us-central1\"            # Choose an available region for Cloud CAIP\n",
    "TFVERSION = \"1.14\"                # TF version for CMLE to use\n",
    "PROJ_NAME='miniproj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = TFVERSION\n",
    "os.environ[\"PROJ_NAME\"] = PROJ_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_LEN = 31\n",
    "os.environ['FEATURE_LEN'] = str(FEATURE_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "모델 이름을 이력하세요: dnnclass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnnclass\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    MODEL_NAME=input('모델 이름을 이력하세요:')\n",
    "    if len(MODEL_NAME) < 1:\n",
    "        MODEL_NAME='linear'\n",
    "except ValueError:\n",
    "    MODEL_NAME='linear'\n",
    "os.environ[\"MODEL_NAME\"] = MODEL_NAME\n",
    "print(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use DNN size of [100, 10]\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "DNNClass MOdel : NNSIZE:[100, 10]\n",
      "**************************************************\n",
      "data_regressor : predictions shape Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "compute_errors : features shape Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "compute_errors : predictions shape Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "DNNClass MOdel : NNSIZE:[100, 10]\n",
      "**************************************************\n",
      "data_regressor : predictions shape Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "compute_errors : features shape Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "compute_errors : predictions shape Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "DNNClass MOdel : NNSIZE:[100, 10]\n",
      "**************************************************\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "DNNClass MOdel : NNSIZE:[100, 10]\n",
      "**************************************************\n",
      "data_regressor : predictions shape Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "compute_errors : features shape Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "compute_errors : predictions shape Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "DNNClass MOdel : NNSIZE:[100, 10]\n",
      "**************************************************\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "DNNClass MOdel : NNSIZE:[100, 10]\n",
      "**************************************************\n",
      "data_regressor : predictions shape Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "compute_errors : features shape Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "compute_errors : predictions shape Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "DNNClass MOdel : NNSIZE:[100, 10]\n",
      "**************************************************\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "DNNClass MOdel : NNSIZE:[100, 10]\n",
      "**************************************************\n",
      "data_regressor : predictions shape Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "compute_errors : features shape Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "compute_errors : predictions shape Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "DNNClass MOdel : NNSIZE:[100, 10]\n",
      "**************************************************\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "DNNClass MOdel : NNSIZE:[100, 10]\n",
      "**************************************************\n",
      "data_regressor : predictions shape Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "compute_errors : features shape Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "compute_errors : predictions shape Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "DNNClass MOdel : NNSIZE:[100, 10]\n",
      "**************************************************\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "DNNClass MOdel : NNSIZE:[100, 10]\n",
      "**************************************************\n",
      "data_regressor : predictions shape Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "compute_errors : features shape Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "compute_errors : predictions shape Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "DNNClass MOdel : NNSIZE:[100, 10]\n",
      "**************************************************\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "DNNClass MOdel : NNSIZE:[100, 10]\n",
      "**************************************************\n",
      "data_regressor : predictions shape Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "compute_errors : features shape Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "compute_errors : predictions shape Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "DNNClass MOdel : NNSIZE:[100, 10]\n",
      "**************************************************\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "DNNClass MOdel : NNSIZE:[100, 10]\n",
      "**************************************************\n",
      "data_regressor : predictions shape Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "compute_errors : features shape Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "compute_errors : predictions shape Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "DNNClass MOdel : NNSIZE:[100, 10]\n",
      "**************************************************\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "DNNClass MOdel : NNSIZE:[100, 10]\n",
      "**************************************************\n",
      "data_regressor : predictions shape Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "compute_errors : features shape Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "compute_errors : predictions shape Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "DNNClass MOdel : NNSIZE:[100, 10]\n",
      "**************************************************\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "DNNClass MOdel : NNSIZE:[100, 10]\n",
      "**************************************************\n",
      "data_regressor : predictions shape Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "compute_errors : features shape Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "compute_errors : predictions shape Tensor(\"dense_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "feature shape : Tensor(\"Squeeze:0\", shape=(?, 29), dtype=float32)\n",
      "DNNClass MOdel : NNSIZE:[100, 10]\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From miniproj/trainer/model.py:24: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:24: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:269: The name tf.summary.FileWriterCache is deprecated. Please use tf.compat.v1.summary.FileWriterCache instead.\n",
      "\n",
      "INFO:tensorflow:TF_CONFIG environment variable: {u'environment': u'cloud', u'cluster': {}, u'job': {u'args': [u'--train_data_path=/home/jupyter/google-asl-study/final_mini_project/youn/input/train_smote.csv', u'--eval_data_path=/home/jupyter/google-asl-study/final_mini_project/youn/input/valid.csv', u'--output_dir=/home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2', u'--model=dnnclass', u'--train_steps=2000', u'--feature_length=31', u'--learning_rate=0.0001', u'--eval_delay_secs=1', u'--min_eval_frequency=10', u'--stop_loss=1', u'--nnsize=100,10'], u'job_name': u'miniproj.trainer.task'}, u'task': {}}\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 10, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbd9b085d10>, '_model_dir': '/home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_tf_random_seed': 1, '_save_summary_steps': 100, '_device_fn': None, '_session_creation_timeout_secs': 7200, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_experimental_max_worker_delay_secs': None, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 10.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/training_util.py:236: initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:12: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.decode_csv is deprecated. Please use tf.io.decode_csv instead.\n",
      "\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:142: make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:112: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/layers/core.py:187: apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:167: The name tf.losses.sigmoid_cross_entropy is deprecated. Please use tf.compat.v1.losses.sigmoid_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:169: The name tf.metrics.root_mean_squared_error is deprecated. Please use tf.compat.v1.metrics.root_mean_squared_error instead.\n",
      "\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:170: The name tf.metrics.recall is deprecated. Please use tf.compat.v1.metrics.recall instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/metrics_impl.py:2200: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:171: The name tf.metrics.precision is deprecated. Please use tf.compat.v1.metrics.precision instead.\n",
      "\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:172: The name tf.metrics.true_positives is deprecated. Please use tf.compat.v1.metrics.true_positives instead.\n",
      "\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:173: The name tf.metrics.true_negatives is deprecated. Please use tf.compat.v1.metrics.true_negatives instead.\n",
      "\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:174: The name tf.metrics.false_negatives is deprecated. Please use tf.compat.v1.metrics.false_negatives instead.\n",
      "\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:175: The name tf.metrics.false_positives is deprecated. Please use tf.compat.v1.metrics.false_positives instead.\n",
      "\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:229: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:229: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:234: The name tf.train.get_global_step is deprecated. Please use tf.compat.v1.train.get_global_step instead.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2019-12-11 13:13:22.739626: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2019-12-11 13:13:22.752257: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2019-12-11 13:13:22.753963: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c1d251a370 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2019-12-11 13:13:22.754014: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2019-12-11 13:13:22.755335: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.7168911, step = 1\n",
      "INFO:tensorflow:global_step/sec: 38.4842\n",
      "INFO:tensorflow:loss = 0.6605755, step = 101 (2.599 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.7007\n",
      "INFO:tensorflow:loss = 0.6344159, step = 201 (2.012 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.7822\n",
      "INFO:tensorflow:loss = 0.5671884, step = 301 (2.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.9918\n",
      "INFO:tensorflow:loss = 0.5261495, step = 401 (2.222 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 413 into /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-11T13:13:34Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-413\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-11-13:13:42\n",
      "INFO:tensorflow:Saving dict for global step 413: False_negatibes = 0.0, False_positives = 45092.0, Precision = 0.0018372994, RMSE = 0.24291183, RMSE_same_as_last = 245.8532, Recall = 1.0, True_negatibes = 394.0, True_positives = 83.0, global_step = 413, loss = 0.80147994\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 413: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-413\n",
      "WARNING:tensorflow:From miniproj/trainer/model.py:149: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default', 'predictions']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-413\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/export/exporter/temp-1576070022/saved_model.pb\n",
      "INFO:tensorflow:Saving checkpoints for 449 into /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (10 secs).\n",
      "INFO:tensorflow:global_step/sec: 8.68943\n",
      "INFO:tensorflow:loss = 0.54562545, step = 501 (11.509 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.2894\n",
      "INFO:tensorflow:loss = 0.5535706, step = 601 (2.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.638\n",
      "INFO:tensorflow:loss = 0.5719484, step = 701 (2.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.4351\n",
      "INFO:tensorflow:loss = 0.54329187, step = 801 (2.250 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 898 into /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-11T13:13:54Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-898\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-11-13:14:02\n",
      "INFO:tensorflow:Saving dict for global step 898: False_negatibes = 0.0, False_positives = 45248.0, Precision = 0.0018309766, RMSE = 0.104536116, RMSE_same_as_last = 245.8532, Recall = 1.0, True_negatibes = 238.0, True_positives = 83.0, global_step = 898, loss = 0.7278376\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 898: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-898\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default', 'predictions']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-898\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/export/exporter/temp-1576070042/saved_model.pb\n",
      "INFO:tensorflow:global_step/sec: 8.64212\n",
      "INFO:tensorflow:loss = 0.53254545, step = 901 (11.571 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 931 into /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (10 secs).\n",
      "INFO:tensorflow:global_step/sec: 44.8037\n",
      "INFO:tensorflow:loss = 0.52173984, step = 1001 (2.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.7061\n",
      "INFO:tensorflow:loss = 0.53277326, step = 1101 (2.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.1044\n",
      "INFO:tensorflow:loss = 0.5285824, step = 1201 (2.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.1822\n",
      "INFO:tensorflow:loss = 0.54035395, step = 1301 (2.165 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1390 into /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-11T13:14:14Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-1390\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-11-13:14:22\n",
      "INFO:tensorflow:Saving dict for global step 1390: False_negatibes = 0.0, False_positives = 45413.0, Precision = 0.0018243362, RMSE = 0.06292298, RMSE_same_as_last = 245.8532, Recall = 1.0, True_negatibes = 73.0, True_positives = 83.0, global_step = 1390, loss = 0.70569205\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1390: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-1390\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default', 'predictions']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-1390\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/export/exporter/temp-1576070062/saved_model.pb\n",
      "INFO:tensorflow:global_step/sec: 8.82349\n",
      "INFO:tensorflow:loss = 0.5523716, step = 1401 (11.333 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1432 into /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-11T13:14:24Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-1432\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-11-13:14:33\n",
      "INFO:tensorflow:Saving dict for global step 1432: False_negatibes = 0.0, False_positives = 45436.0, Precision = 0.0018234143, RMSE = 0.06353967, RMSE_same_as_last = 245.8532, Recall = 1.0, True_negatibes = 50.0, True_positives = 83.0, global_step = 1432, loss = 0.7056636\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1432: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-1432\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default', 'predictions']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-1432\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/export/exporter/temp-1576070073/saved_model.pb\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/summary/summary_iterator.py:68: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "INFO:tensorflow:Saving checkpoints for 1433 into /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-11T13:14:34Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-1433\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-11-13:14:43\n",
      "INFO:tensorflow:Saving dict for global step 1433: False_negatibes = 0.0, False_positives = 45447.0, Precision = 0.0018229738, RMSE = 0.065272845, RMSE_same_as_last = 245.8532, Recall = 1.0, True_negatibes = 39.0, True_positives = 83.0, global_step = 1433, loss = 0.7061785\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1433: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-1433\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default', 'predictions']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-1433\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/export/exporter/temp-1576070083/saved_model.pb\n",
      "INFO:tensorflow:Saving checkpoints for 1446 into /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-11T13:14:44Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-1446\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-11-13:14:53\n",
      "INFO:tensorflow:Saving dict for global step 1446: False_negatibes = 0.0, False_positives = 45477.0, Precision = 0.0018217735, RMSE = 0.075771414, RMSE_same_as_last = 245.8532, Recall = 1.0, True_negatibes = 9.0, True_positives = 83.0, global_step = 1446, loss = 0.70889205\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1446: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-1446\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default', 'predictions']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-1446\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/export/exporter/temp-1576070093/saved_model.pb\n",
      "INFO:tensorflow:Saving checkpoints for 1447 into /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-11T13:14:54Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-1447\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-11-13:15:04\n",
      "INFO:tensorflow:Saving dict for global step 1447: False_negatibes = 0.0, False_positives = 45467.0, Precision = 0.0018221735, RMSE = 0.07078169, RMSE_same_as_last = 245.8532, Recall = 1.0, True_negatibes = 19.0, True_positives = 83.0, global_step = 1447, loss = 0.7074562\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1447: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-1447\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default', 'predictions']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-1447\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/export/exporter/temp-1576070104/saved_model.pb\n",
      "INFO:tensorflow:Saving checkpoints for 1448 into /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-11T13:15:04Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-1448\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-11-13:15:14\n",
      "INFO:tensorflow:Saving dict for global step 1448: False_negatibes = 0.0, False_positives = 45450.0, Precision = 0.0018228538, RMSE = 0.06659207, RMSE_same_as_last = 245.8532, Recall = 1.0, True_negatibes = 36.0, True_positives = 83.0, global_step = 1448, loss = 0.70633733\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1448: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-1448\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default', 'predictions']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-1448\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/export/exporter/temp-1576070114/saved_model.pb\n",
      "INFO:tensorflow:Saving checkpoints for 1449 into /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-11T13:15:15Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-1449\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-11-13:15:25\n",
      "INFO:tensorflow:Saving dict for global step 1449: False_negatibes = 0.0, False_positives = 45422.0, Precision = 0.0018239754, RMSE = 0.06308798, RMSE_same_as_last = 245.8532, Recall = 1.0, True_negatibes = 64.0, True_positives = 83.0, global_step = 1449, loss = 0.7053127\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1449: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-1449\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default', 'predictions']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-1449\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/export/exporter/temp-1576070125/saved_model.pb\n",
      "INFO:tensorflow:Saving checkpoints for 1450 into /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-11T13:15:25Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-1450\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-11-13:15:33\n",
      "INFO:tensorflow:Saving dict for global step 1450: False_negatibes = 0.0, False_positives = 45386.0, Precision = 0.0018254195, RMSE = 0.060831595, RMSE_same_as_last = 245.8532, Recall = 1.0, True_negatibes = 100.0, True_positives = 83.0, global_step = 1450, loss = 0.7045934\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1450: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-1450\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default', 'predictions']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/model.ckpt-1450\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/jupyter/google-asl-study/final_mini_project/youn/trained_dnnclass/miniproj_v2/export/exporter/temp-1576070133/saved_model.pb\n",
      "INFO:tensorflow:No decrease in metric \"loss\" for 1 steps, which is greater than or equal to max steps (1) configured for early stopping.\n",
      "INFO:tensorflow:Requesting early stopping at global step 1450\n",
      "INFO:tensorflow:Loss for final step: 0.5598328.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# MODEL_NAME=linear\n",
    "DATADIR=$(pwd)\n",
    "OUTDIR=$(pwd)/trained_${MODEL_NAME}/${PROJ_NAME}_v2\n",
    "rm -rf $OUTDIR\n",
    "gcloud ai-platform local train \\\n",
    "   --module-name=${PROJ_NAME}.trainer.task \\\n",
    "   --package-path=${PWD}/${PROJ_NAME}_v2.trainer \\\n",
    "   -- \\\n",
    "   --train_data_path=\"${DATADIR}/input/train_smote.csv\" \\\n",
    "   --eval_data_path=\"${DATADIR}/input/valid.csv\"  \\\n",
    "   --output_dir=${OUTDIR} \\\n",
    "   --model=${MODEL_NAME} \\\n",
    "   --train_steps=2000 \\\n",
    "   --feature_length=$FEATURE_LEN \\\n",
    "   --learning_rate=0.0001 \\\n",
    "   --eval_delay_secs=1 \\\n",
    "   --min_eval_frequency=10 \\\n",
    "   --stop_loss=1 \\\n",
    "   --nnsize='100,10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Platform JOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwiklabs-gcp-ml-49b827b781ab\n",
      "miniproj\n"
     ]
    }
   ],
   "source": [
    "!echo $BUCKET\n",
    "!echo $PROJ_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://./input/train_smote.csv [Content-Type=text/csv]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "/ [1 files][194.5 MiB/194.5 MiB]                                                \n",
      "Operation completed over 1 objects/194.5 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp ./input/train_smote.csv gs://{BUCKET}/${PROJ_NAME}/input/train_smote.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "모델 이름을 이력하세요: dnnclass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnnclass\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    MODEL_NAME=input('모델 이름을 이력하세요:')\n",
    "    if len(MODEL_NAME) < 1:\n",
    "        MODEL_NAME='linear'\n",
    "except ValueError:\n",
    "    MODEL_NAME='linear'\n",
    "os.environ[\"MODEL_NAME\"] = MODEL_NAME\n",
    "print(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: miniproj_dnnclass_v2_191211_134656\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [miniproj_dnnclass_v2_191211_134656] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe miniproj_dnnclass_v2_191211_134656\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs miniproj_dnnclass_v2_191211_134656\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# MODEL_NAME=linear\n",
    "DATADIR=$(pwd)\n",
    "OUTDIR=gs://${BUCKET}/${PROJ_NAME}/${MODEL_NAME}_v2\n",
    "JOBNAME=${PROJ_NAME}_${MODEL_NAME}_v2_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "rm -rf $OUTDIR\n",
    "gcloud ai-platform jobs submit training $JOBNAME \\\n",
    "    --region=$REGION \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=${PWD}/${PROJ_NAME}_v2/trainer \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --scale-tier=PREMIUM_1 \\\n",
    "    --runtime-version=$TFVERSION \\\n",
    "    -- \\\n",
    "    --train_data_path=\"gs://${BUCKET}/${PROJ_NAME}/input/train_smote.csv\" \\\n",
    "    --eval_data_path=\"gs://${BUCKET}/${PROJ_NAME}/input/valid.csv\"  \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --model=${MODEL_NAME} \\\n",
    "    --train_steps=2000 \\\n",
    "    --feature_length=$FEATURE_LEN \\\n",
    "    --learning_rate=0.0001 \\\n",
    "    --eval_delay_secs=1 \\\n",
    "    --min_eval_frequency=10 \\\n",
    "    --stop_loss=1 \\\n",
    "    --nnsize='100,10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# yamul파일 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hyperparam.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "    scaleTier: PREMIUM_1\n",
    "    hyperparameters:\n",
    "        hyperparameterMetricTag: RMSE\n",
    "        goal: MINIMIZE\n",
    "        maxTrials: 4\n",
    "        maxParallelTrials: 2\n",
    "        enableTrialEarlyStopping: True\n",
    "        params:\n",
    "        - parameterName: nnsize\n",
    "          type: CATEGORICAL\n",
    "          categoricalValues:\n",
    "          - 30,10\n",
    "          - 100,10\n",
    "          - 30,100\n",
    "          - 30,10,30        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: miniproj_dnnclass_191210_071526\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [miniproj_dnnclass_191210_071526] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe miniproj_dnnclass_191210_071526\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs miniproj_dnnclass_191210_071526\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# MODEL_NAME=linear\n",
    "DATADIR=$(pwd)\n",
    "OUTDIR=gs://${BUCKET}/${PROJ_NAME}/${MODEL_NAME}_HParam\n",
    "JOBNAME=${PROJ_NAME}_${MODEL_NAME}_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "rm -rf $OUTDIR\n",
    "gcloud ai-platform jobs submit training $JOBNAME \\\n",
    "    --region=$REGION \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=${PWD}/${PROJ_NAME}/trainer \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --scale-tier=PREMIUM_1 \\\n",
    "    --runtime-version=$TFVERSION \\\n",
    "    --config=hyperparam.yaml \\\n",
    "    -- \\\n",
    "    --train_data_path=\"gs://${BUCKET}/${PROJ_NAME}/input/train_smote.csv\" \\\n",
    "    --eval_data_path=\"gs://${BUCKET}/${PROJ_NAME}/input/valid.csv\"  \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --model=${MODEL_NAME} \\\n",
    "    --train_steps=2000 \\\n",
    "    --feature_length=$FEATURE_LEN \\\n",
    "    --learning_rate=0.0001 \\\n",
    "    --eval_delay_secs=1 \\\n",
    "    --min_eval_frequency=10 \\\n",
    "    --stop_loss=1 \\\n",
    "    --nnsize='30,10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 코드 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ${PROJ_NAME}_v2/trainer\n",
    "touch ${PROJ_NAME}_v2/__init__.py\n",
    "touch ${PROJ_NAME}_v2/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing miniproj_v2/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PROJ_NAME}_v2/trainer/task.py\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Example implementation of code to run on the Cloud ML service.\n",
    "\"\"\"\n",
    "\n",
    "import traceback\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from . import model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  # Input Arguments\n",
    "  parser.add_argument(\n",
    "      '--train_data_path',\n",
    "      help='GCS or local path to training data',\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--eval_data_path',\n",
    "      help='GCS or local path to evaluation data',\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--train_batch_size',\n",
    "      help='Batch size for training steps',\n",
    "      type=int,\n",
    "      default=100\n",
    "  )\n",
    "  parser.add_argument(\n",
    "        \"--nnsize\",\n",
    "        help=\"Hidden layer sizes to use for DNN feature columns -- provide \\\n",
    "        space-separated layers\",\n",
    "        type=str,\n",
    "        default='128,32,4'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--learning_rate',\n",
    "      help='Initial learning rate for training',\n",
    "      type=float,\n",
    "      default=0.01\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--train_steps',\n",
    "      help=\"\"\"\\\n",
    "      Steps to run the training job for. A step is one batch-size,\\\n",
    "      \"\"\",\n",
    "      type=int,\n",
    "      default=0\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--feature_length',\n",
    "      help=\"\"\"\\\n",
    "      This model works with fixed length sequences. 1-(N-1) are inputs, last is output\n",
    "      \"\"\",\n",
    "      type=int,\n",
    "      default=10\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--output_dir',\n",
    "      help='GCS location to write checkpoints and export models',\n",
    "      required=True\n",
    "  )\n",
    "  model_names = [name.replace('_model','') \\\n",
    "                   for name in dir(model) \\\n",
    "                     if name.endswith('_model')]\n",
    "  parser.add_argument(\n",
    "      '--model',\n",
    "      help='Type of model. Supported types are {}'.format(model_names),\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--job-dir',\n",
    "      help='this model ignores this field, but it is required by gcloud',\n",
    "      default='junk'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--eval_delay_secs',\n",
    "      help='How long to wait before running first evaluation',\n",
    "      default=10,\n",
    "      type=int\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--min_eval_frequency',\n",
    "      help='Minimum number of training steps between evaluations',\n",
    "      default=60,\n",
    "      type=int\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--stop_loss',\n",
    "      help='Value of Early Stopping Loss',\n",
    "      default=1,\n",
    "      type=int\n",
    "  )\n",
    "\n",
    "  args = parser.parse_args()\n",
    "  hparams = args.__dict__\n",
    "  \n",
    "  # unused args provided by service\n",
    "  hparams.pop('job_dir', None)\n",
    "  hparams.pop('job-dir', None)\n",
    "\n",
    "  output_dir = hparams.pop('output_dir')\n",
    "  model.BATCH_SIZE = hparams['train_batch_size']\n",
    "  model.NNSIZE = list(map(int, hparams.pop(\"nnsize\").split(\",\")))\n",
    "  print (\"Will use DNN size of {}\".format(model.NNSIZE))\n",
    "\n",
    "  # Append trial_id to path if we are doing hptuning\n",
    "  # This code can be removed if you are not using hyperparameter tuning\n",
    "  output_dir = os.path.join(\n",
    "      output_dir,\n",
    "      json.loads(\n",
    "          os.environ.get('TF_CONFIG', '{}')\n",
    "      ).get('task', {}).get('trial', '')\n",
    "  )\n",
    "\n",
    "  # calculate train_steps if not provided\n",
    "  if hparams['train_steps'] < 1:\n",
    "     # 1,000 steps at batch_size of 100\n",
    "     hparams['train_steps'] = (1000 * 100) // hparams['train_batch_size']\n",
    "     print (\"Training for {} steps\".format(hparams['train_steps']))\n",
    "\n",
    "  model.init(hparams)\n",
    "\n",
    "  # Run the training job\n",
    "  model.train_and_evaluate(output_dir, hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing miniproj_v2/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PROJ_NAME}_v2/trainer/model.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "\n",
    "N_OUTPUTS = 1  # in each sequence, 1-49 are features, and 50 is label\n",
    "FEATURE_LEN = None\n",
    "DEFAULTS = None\n",
    "N_INPUTS = None\n",
    "BATCH_SIZE = 512\n",
    "NNSIZE = [64, 16, 4]\n",
    "\n",
    "CSV_COLUMNS = 'Time,V1,V2,V3,V4,V5,V6,V7,V8,V9,V10,V11,V12,V13,V14,V15,V16,V17,V18,V19,V20,V21,V22,V23,V24,V25,V26,V27,V28,Amount,Class'.split(',')\n",
    "FEATURES_COLUMNS = CSV_COLUMNS[1:len(CSV_COLUMNS) - 1]\n",
    "LABEL_COLUMN = 'Class'\n",
    "CSV_DEFAULTS = [[0.0] for i in range(31)]\n",
    "\n",
    "\n",
    "\n",
    "def init(hparams):\n",
    "    global FEATURE_LEN, DEFAULTS, N_INPUTS\n",
    "    FEATURE_LEN = hparams['feature_length']\n",
    "    DEFAULTS = [[0.0] for x in range(0, FEATURE_LEN)]\n",
    "    N_INPUTS = FEATURE_LEN - N_OUTPUTS\n",
    "\n",
    "def feature_merge(features):\n",
    "    temp_V01 = tf.expand_dims(features[\"V1\"] , -1)\n",
    "    temp_V02 = tf.expand_dims(features[\"V2\"] , -1)\n",
    "    temp_V03 = tf.expand_dims(features[\"V3\"] , -1)\n",
    "    temp_V04 = tf.expand_dims(features[\"V4\"] , -1)\n",
    "    temp_V05 = tf.expand_dims(features[\"V5\"] , -1)\n",
    "    temp_V06 = tf.expand_dims(features[\"V6\"] , -1)\n",
    "    temp_V07 = tf.expand_dims(features[\"V7\"] , -1)\n",
    "    temp_V08 = tf.expand_dims(features[\"V8\"] , -1)\n",
    "    temp_V09 = tf.expand_dims(features[\"V9\"] , -1)\n",
    "    temp_V10 = tf.expand_dims(features[\"V10\"], -1)\n",
    "    temp_V11 = tf.expand_dims(features[\"V11\"], -1)\n",
    "    temp_V12 = tf.expand_dims(features[\"V12\"], -1)\n",
    "    temp_V13 = tf.expand_dims(features[\"V13\"], -1)\n",
    "    temp_V14 = tf.expand_dims(features[\"V14\"], -1)\n",
    "    temp_V15 = tf.expand_dims(features[\"V15\"], -1)\n",
    "    temp_V16 = tf.expand_dims(features[\"V16\"], -1)\n",
    "    temp_V17 = tf.expand_dims(features[\"V17\"], -1)\n",
    "    temp_V18 = tf.expand_dims(features[\"V18\"], -1)\n",
    "    temp_V19 = tf.expand_dims(features[\"V19\"], -1)\n",
    "    temp_V20 = tf.expand_dims(features[\"V20\"], -1)\n",
    "    temp_V21 = tf.expand_dims(features[\"V21\"], -1)\n",
    "    temp_V22 = tf.expand_dims(features[\"V22\"], -1)\n",
    "    temp_V23 = tf.expand_dims(features[\"V23\"], -1)\n",
    "    temp_V24 = tf.expand_dims(features[\"V24\"], -1)\n",
    "    temp_V25 = tf.expand_dims(features[\"V25\"], -1)\n",
    "    temp_V26 = tf.expand_dims(features[\"V26\"], -1)\n",
    "    temp_V27 = tf.expand_dims(features[\"V27\"], -1)\n",
    "    temp_V28 = tf.expand_dims(features[\"V28\"], -1)\n",
    "    temp_Amount = tf.expand_dims(features[\"Amount\"],-1)\n",
    "    features = tf.stack([temp_V01, temp_V02, temp_V03, temp_V04, temp_V05,\n",
    "                  temp_V06, temp_V07, temp_V08, temp_V09, temp_V10,\n",
    "                  temp_V11, temp_V12, temp_V13, temp_V14, temp_V15,\n",
    "                  temp_V16, temp_V17, temp_V18, temp_V19, temp_V10,\n",
    "                  temp_V21, temp_V22, temp_V23, temp_V24, temp_V25,\n",
    "                  temp_V26, temp_V27, temp_V28, temp_Amount\n",
    "                 ], axis=1)\n",
    "    features = tf.squeeze(features, -1)\n",
    "    print('feature shape : {}'.format(features))\n",
    "\n",
    "    return features\n",
    "\n",
    "def linear_model(features, mode, params):\n",
    "    X = features\n",
    "    predictions = tf.layers.dense(X, 1, activation=None)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def dnn_model(features, mode, params):\n",
    "    X = features\n",
    "    print('NNSIZE:{}'.format(NNSIZE))\n",
    "    for unit_num in NNSIZE:\n",
    "        X = tf.layers.dense(X, units=unit_num, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(X, 1, activation=None)  # linear output: regression\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def dnnclass_model(features, mode, params):\n",
    "    \n",
    "    print('feature shape : {}'.format(features))\n",
    "    print('DNNClass MOdel : NNSIZE:{}'.format(NNSIZE))\n",
    "    print('*'*50)\n",
    "    \n",
    "    X = features\n",
    "    for unit_num in NNSIZE:\n",
    "        X = tf.layers.dense(X, units=unit_num, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(X, 1, activation=tf.nn.sigmoid)  # logistic output\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# read data and convert to needed format\n",
    "def read_dataset(filename, mode, batch_size=512):\n",
    "    def _input_fn():\n",
    "        def decode_csv(row):\n",
    "            fields = tf.decode_csv(records = row, record_defaults = CSV_DEFAULTS)\n",
    "            features = dict(zip(CSV_COLUMNS, fields))\n",
    "            features.pop(\"Time\")\n",
    "            label = features.pop(LABEL_COLUMN)\n",
    "            return features, label        \n",
    "        \n",
    "        # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "        dataset = tf.data.Dataset.list_files(filename)\n",
    "        # Read in data from files\n",
    "        dataset = dataset.flat_map(tf.data.TextLineDataset)\n",
    "        # Parse text lines as comma-separated values (CSV)\n",
    "        \n",
    "        dataset = dataset.skip(1).map(decode_csv)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None  # loop indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size=10 * batch_size)\n",
    "        else:\n",
    "            num_epochs = 1  # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "    return _input_fn\n",
    "\n",
    "\n",
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "        key: tf.placeholder(tf.float32, [None]) for key in FEATURES_COLUMNS\n",
    "    }\n",
    "    features = feature_placeholders\n",
    "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n",
    "\n",
    "\n",
    "def compute_errors(features, labels, predictions):\n",
    "    print('compute_errors : features shape {}'.format(features))\n",
    "\n",
    "    labels = tf.expand_dims(labels, -1)  # rank 1 -> rank 2 to match rank of predictions\n",
    "\n",
    "    #if predictions.shape[1] == 1:\n",
    "    #AttributeError: 'NoneType' object has no attribute 'shape'\n",
    "    print('compute_errors : predictions shape {}'.format(predictions))\n",
    "    \n",
    "    \n",
    "    if predictions.shape[1] == 1:\n",
    "        #loss = tf.losses.mean_squared_error(labels, predictions)\n",
    "        loss = tf.losses.sigmoid_cross_entropy(labels, predictions)\n",
    "        #loss = tf.Print(loss, [tf.shape(loss), loss], summarize = -1)\n",
    "        rmse = tf.metrics.root_mean_squared_error(labels, predictions)\n",
    "        recall = tf.metrics.recall(labels, predictions)\n",
    "        precision = tf.metrics.precision(labels, predictions)\n",
    "        True_positives = tf.metrics.true_positives(labels, predictions)\n",
    "        True_negatibes = tf.metrics.true_negatives(labels, predictions)\n",
    "        False_negatibes = tf.metrics.false_negatives(labels, predictions)\n",
    "        False_positives = tf.metrics.false_positives(labels, predictions)\n",
    "        return loss, rmse, recall, precision, True_positives, True_negatibes, False_negatibes, False_positives\n",
    "    else:\n",
    "        # one prediction for every input in sequence\n",
    "        # get 1-N of (x + label)\n",
    "        labelsN = tf.concat([features, labels], axis=1)\n",
    "        labelsN = labelsN[:, 1:]\n",
    "        # loss is computed from the last 1/3 of the series\n",
    "        N = (2 * N_INPUTS) // 3\n",
    "        #loss = tf.losses.mean_squared_error(labelsN[:, N:], predictions[:, N:])\n",
    "        loss = tf.losses.sigmoid_cross_entropy(labelsN[:, N:], predictions[:, N:])\n",
    "        # rmse is computed from last prediction and last label\n",
    "        lastPred = predictions[:, -1]\n",
    "        rmse = tf.metrics.root_mean_squared_error(labels, lastPred)\n",
    "        recall = tf.metrics.recall(labels, lastPred)\n",
    "        precision = tf.metrics.precision(labels, predictions)\n",
    "        True_positives = tf.metrics.true_positives(labels, predictions)\n",
    "        True_negatibes = tf.metrics.true_negatives(labels, predictions)\n",
    "        False_negatibes = tf.metrics.false_negatives(labels, predictions)\n",
    "        False_positives = tf.metrics.false_positives(labels, predictions)\n",
    "        return loss, rmse, recall, precision, True_positives, True_negatibes, False_negatibes, False_positives\n",
    "\n",
    "# RMSE when predicting same as last value\n",
    "def same_as_last_benchmark(features, labels):\n",
    "    predictions = features[:,-1] # last value in input sequence\n",
    "    return tf.metrics.root_mean_squared_error(labels, predictions)\n",
    "\n",
    "\n",
    "# create the inference model\n",
    "def data_regressor(features, labels, mode, params):\n",
    "    # 1. run the appropriate model\n",
    "    model_functions = {\n",
    "        'linear': linear_model,\n",
    "        'dnn': dnn_model,\n",
    "        'dnnclass': dnnclass_model}\n",
    "    model_function = model_functions[params['model']]\n",
    "    \n",
    "    features = feature_merge(features)\n",
    "    #print('2>>>>>>> {}'.format(features))\n",
    "\n",
    "    \n",
    "    predictions = model_function(features, mode, params)\n",
    "    #predictions = tf.Print(predictions, [tf.shape(predictions), predictions], summarize=-1)\n",
    "    \n",
    "    # 2. loss function, training/eval ops\n",
    "    loss = None\n",
    "    train_op = None\n",
    "    eval_metric_ops = None\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
    "        print('data_regressor : predictions shape {}'.format(predictions))\n",
    "        loss, rmse, recall, precision, True_positives, True_negatibes, False_negatibes, False_positives = compute_errors(features, labels, predictions)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            # this is needed for batch normalization, but has no effect otherwise\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                # 2b. set up training operation\n",
    "                train_op = tf.contrib.layers.optimize_loss(\n",
    "                    loss,\n",
    "                    tf.train.get_global_step(),\n",
    "                    learning_rate=params['learning_rate'],\n",
    "                    optimizer=\"Adam\")\n",
    "\n",
    "        # 2c. eval metric\n",
    "        eval_metric_ops = {\n",
    "            \"RMSE\": rmse,\n",
    "            \"RMSE_same_as_last\": same_as_last_benchmark(features, labels),\n",
    "            \"Recall\": recall,\n",
    "            \"Precision\": precision,\n",
    "            \"True_positives\": True_positives,\n",
    "            \"True_negatibes\": True_negatibes,\n",
    "            \"False_negatibes\": False_negatibes,\n",
    "            \"False_positives\": False_positives,\n",
    "        }\n",
    "\n",
    "        \n",
    "    # 3. Create predictions\n",
    "    if predictions.shape[1] != 1:\n",
    "        predictions = predictions[:, -1]  # last predicted value\n",
    "    predictions_dict = {\"predicted\": predictions}\n",
    "\n",
    "    # 4. return EstimatorSpec\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions_dict,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops=eval_metric_ops,\n",
    "        export_outputs={\n",
    "            'predictions': tf.estimator.export.PredictOutput(predictions_dict)}\n",
    "    )\n",
    "\n",
    "\n",
    "def train_and_evaluate(output_dir, hparams):\n",
    "    tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "    get_train = read_dataset(hparams['train_data_path'],\n",
    "                             tf.estimator.ModeKeys.TRAIN,\n",
    "                             hparams['train_batch_size'])\n",
    "    get_valid = read_dataset(hparams['eval_data_path'],\n",
    "                             tf.estimator.ModeKeys.EVAL,\n",
    "                             1000)\n",
    "    \n",
    "    estimator = tf.estimator.Estimator(model_fn=data_regressor,\n",
    "                                       params=hparams,\n",
    "                                       config=tf.estimator.RunConfig(\n",
    "                                           tf_random_seed = 1,\n",
    "                                           save_checkpoints_secs=\n",
    "                                           hparams['min_eval_frequency']),\n",
    "                                       model_dir=output_dir)\n",
    "   \n",
    "    train_spec = tf.estimator.TrainSpec(input_fn=get_train,\n",
    "                                        max_steps=hparams['train_steps'],\n",
    "                                        hooks=[tf.estimator.experimental.stop_if_no_decrease_hook(estimator, \"loss\", hparams['stop_loss'])])\n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=get_valid,\n",
    "                                      steps=None,\n",
    "                                      exporters=exporter,\n",
    "                                      start_delay_secs=hparams['eval_delay_secs'],\n",
    "                                      throttle_secs=hparams['min_eval_frequency'])\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
