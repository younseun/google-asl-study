{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"qwiklabs-gcp-ml-49b827b781ab\"  # Replace with your PROJECT\n",
    "BUCKET = \"qwiklabs-gcp-ml-49b827b781ab\"  # Replace with your BUCKET\n",
    "REGION = \"us-central1\"            # Choose an available region for AI Platform\n",
    "TFVERSION = \"1.14\"                # TF version for AI Platform to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = TFVERSION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move code into python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘taxifaremodel’: File exists\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir taxifaremodel\n",
    "touch taxifaremodel/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing taxifaremodel/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile taxifaremodel/model.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "print(tf.__version__)\n",
    "\n",
    "#1. Train and Evaluate Input Functions\n",
    "CSV_COLUMN_NAMES = [\"fare_amount\",\"dayofweek\",\"hourofday\",\"pickuplon\",\"pickuplat\",\"dropofflon\",\"dropofflat\"]\n",
    "CSV_DEFAULTS = [[0.0],[1],[0],[-74.0],[40.0],[-74.0],[40.7]]\n",
    "\n",
    "def read_dataset(csv_path):\n",
    "    def _parse_row(row):\n",
    "        # Decode the CSV row into list of TF tensors\n",
    "        fields = tf.decode_csv(records = row, record_defaults = CSV_DEFAULTS)\n",
    "\n",
    "        # Pack the result into a dictionary\n",
    "        features = dict(zip(CSV_COLUMN_NAMES, fields))\n",
    "        \n",
    "        # NEW: Add engineered features\n",
    "        features = add_engineered_features(features)\n",
    "        \n",
    "        # Separate the label from the features\n",
    "        label = features.pop(\"fare_amount\") # remove label from features and store\n",
    "\n",
    "        return features, label\n",
    "    \n",
    "    # Create a dataset containing the text lines.\n",
    "    dataset = tf.data.Dataset.list_files(file_pattern = csv_path) # (i.e. data_file_*.csv)\n",
    "    dataset = dataset.flat_map(map_func = lambda filename:tf.data.TextLineDataset(filenames = filename).skip(count = 1))\n",
    "\n",
    "    # Parse each CSV row into correct (features,label) format for Estimator API\n",
    "    dataset = dataset.map(map_func = _parse_row)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def train_input_fn(csv_path, batch_size = 128):\n",
    "    #1. Convert CSV into tf.data.Dataset with (features,label) format\n",
    "    dataset = read_dataset(csv_path)\n",
    "      \n",
    "    #2. Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(buffer_size = 1000).repeat(count = None).batch(batch_size = batch_size)\n",
    "   \n",
    "    return dataset\n",
    "\n",
    "def eval_input_fn(csv_path, batch_size = 128):\n",
    "    #1. Convert CSV into tf.data.Dataset with (features,label) format\n",
    "    dataset = read_dataset(csv_path)\n",
    "\n",
    "    #2.Batch the examples.\n",
    "    dataset = dataset.batch(batch_size = batch_size)\n",
    "   \n",
    "    return dataset\n",
    "  \n",
    "#2. Feature Engineering\n",
    "# One hot encode dayofweek and hourofday\n",
    "fc_dayofweek = tf.feature_column.categorical_column_with_identity(key = \"dayofweek\", num_buckets = 7)\n",
    "fc_hourofday = tf.feature_column.categorical_column_with_identity(key = \"hourofday\", num_buckets = 24)\n",
    "\n",
    "# Cross features to get combination of day and hour\n",
    "fc_day_hr = tf.feature_column.crossed_column(keys = [fc_dayofweek, fc_hourofday], hash_bucket_size = 24 * 7)\n",
    "\n",
    "# Bucketize latitudes and longitudes\n",
    "NBUCKETS = 16\n",
    "latbuckets = np.linspace(start = 38.0, stop = 42.0, num = NBUCKETS).tolist()\n",
    "lonbuckets = np.linspace(start = -76.0, stop = -72.0, num = NBUCKETS).tolist()\n",
    "fc_bucketized_plat = tf.feature_column.bucketized_column(source_column = tf.feature_column.numeric_column(key = \"pickuplon\"), boundaries = lonbuckets)\n",
    "fc_bucketized_plon = tf.feature_column.bucketized_column(source_column = tf.feature_column.numeric_column(key = \"pickuplat\"), boundaries = latbuckets)\n",
    "fc_bucketized_dlat = tf.feature_column.bucketized_column(source_column = tf.feature_column.numeric_column(key = \"dropofflon\"), boundaries = lonbuckets)\n",
    "fc_bucketized_dlon = tf.feature_column.bucketized_column(source_column = tf.feature_column.numeric_column(key = \"dropofflat\"), boundaries = latbuckets)\n",
    "\n",
    "def add_engineered_features(features):\n",
    "    features[\"dayofweek\"] = features[\"dayofweek\"] - 1 # subtract one since our days of week are 1-7 instead of 0-6\n",
    "    \n",
    "    features[\"latdiff\"] = features[\"pickuplat\"] - features[\"dropofflat\"] # East/West\n",
    "    features[\"londiff\"] = features[\"pickuplon\"] - features[\"dropofflon\"] # North/South\n",
    "    features[\"euclidean_dist\"] = tf.sqrt(features[\"latdiff\"]**2 + features[\"londiff\"]**2)\n",
    "\n",
    "    return features\n",
    "\n",
    "feature_cols = [\n",
    "  #1. Engineered using tf.feature_column module\n",
    "  tf.feature_column.indicator_column(categorical_column = fc_day_hr),\n",
    "  fc_bucketized_plat,\n",
    "  fc_bucketized_plon,\n",
    "  fc_bucketized_dlat,\n",
    "  fc_bucketized_dlon,\n",
    "  #2. Engineered in input functions\n",
    "  tf.feature_column.numeric_column(key = \"latdiff\"),\n",
    "  tf.feature_column.numeric_column(key = \"londiff\"),\n",
    "  tf.feature_column.numeric_column(key = \"euclidean_dist\") \n",
    "]\n",
    "\n",
    "#3. Serving Input Receiver Function\n",
    "def serving_input_receiver_fn():\n",
    "    receiver_tensors = {\n",
    "        'dayofweek' : tf.placeholder(dtype = tf.int32, shape = [None]), # shape is vector to allow batch of requests\n",
    "        'hourofday' : tf.placeholder(dtype = tf.int32, shape = [None]),\n",
    "        'pickuplon' : tf.placeholder(dtype = tf.float32, shape = [None]), \n",
    "        'pickuplat' : tf.placeholder(dtype = tf.float32, shape = [None]),\n",
    "        'dropofflat' : tf.placeholder(dtype = tf.float32, shape = [None]),\n",
    "        'dropofflon' : tf.placeholder(dtype = tf.float32, shape = [None]),\n",
    "    }\n",
    "    \n",
    "    features = add_engineered_features(receiver_tensors) # 'features' is what is passed on to the model\n",
    "    \n",
    "    return tf.estimator.export.ServingInputReceiver(features = features, receiver_tensors = receiver_tensors)\n",
    "  \n",
    "#4. Train and Evaluate\n",
    "def train_and_evaluate(params):\n",
    "    OUTDIR = params[\"output_dir\"]\n",
    "\n",
    "    model = tf.estimator.DNNRegressor(\n",
    "        hidden_units = params[\"hidden_units\"].split(\",\"), # NEW: paramaterize architecture\n",
    "        feature_columns = feature_cols, \n",
    "        model_dir = OUTDIR,\n",
    "        config = tf.estimator.RunConfig(\n",
    "            tf_random_seed = 1, # for reproducibility\n",
    "            save_checkpoints_steps = max(100, params[\"train_steps\"] // 10) # checkpoint every N steps\n",
    "        ) \n",
    "    )\n",
    "\n",
    "    # Add custom evaluation metric\n",
    "    def my_rmse(labels, predictions):\n",
    "        pred_values = tf.squeeze(input = predictions[\"predictions\"], axis = -1)\n",
    "        return {\"rmse\": tf.metrics.root_mean_squared_error(labels = labels, predictions = pred_values)}\n",
    "    \n",
    "    model = tf.contrib.estimator.add_metrics(model, my_rmse)  \n",
    "\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = lambda: train_input_fn(params[\"train_data_path\"]),\n",
    "        max_steps = params[\"train_steps\"])\n",
    "\n",
    "    exporter = tf.estimator.FinalExporter(name = \"exporter\", serving_input_receiver_fn = serving_input_receiver_fn) # export SavedModel once at the end of training\n",
    "    # Note: alternatively use tf.estimator.BestExporter to export at every checkpoint that has lower loss than the previous checkpoint\n",
    "\n",
    "\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = lambda: eval_input_fn(params[\"eval_data_path\"]),\n",
    "        steps = None,\n",
    "        start_delay_secs = 1, # wait at least N seconds before first evaluation (default 120)\n",
    "        throttle_secs = 1, # wait at least N seconds before each subsequent evaluation (default 600)\n",
    "        exporters = exporter) # export SavedModel once at the end of training\n",
    "\n",
    "    tf.logging.set_verbosity(v = tf.logging.INFO) # so loss is printed during training\n",
    "    shutil.rmtree(path = OUTDIR, ignore_errors = True) # start fresh each time\n",
    "\n",
    "    tf.estimator.train_and_evaluate(model, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create task.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting taxifaremodel/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile taxifaremodel/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from . import model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--hidden_units\",\n",
    "        help = \"Hidden layer sizes to use for DNN feature columns -- provide space-separated layers\",\n",
    "        type = str,\n",
    "        default = \"10,10\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help = \"GCS or local path to training data\",\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_steps\",\n",
    "        help = \"Steps to run the training job for (default: 1000)\",\n",
    "        type = int,\n",
    "        default = 1000\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_data_path\",\n",
    "        help = \"GCS or local path to evaluation data\",\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help = \"GCS location to write checkpoints and export models\",\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help=\"This is not used by our model, but it is required by gcloud\",\n",
    "    )\n",
    "    args = parser.parse_args().__dict__\n",
    "    \n",
    "    # Append trial_id to path so trials don\"t overwrite each other\n",
    "    args[\"output_dir\"] = os.path.join(\n",
    "        args[\"output_dir\"],\n",
    "        json.loads(\n",
    "            os.environ.get(\"TF_CONFIG\", \"{}\")\n",
    "        ).get(\"task\", {}).get(\"trial\", \"\")\n",
    "    ) \n",
    "        \n",
    "    # Run the training job\n",
    "    model.train_and_evaluate(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create hypertuning configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hyperparam.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "  scaleTier: BASIC\n",
    "  hyperparameters:\n",
    "    goal: MINIMIZE\n",
    "    maxTrials: 5\n",
    "    maxParallelTrials: 5\n",
    "    hyperparameterMetricTag: rmse\n",
    "    enableTrialEarlyStopping: True\n",
    "    algorithm: GRID_SEARCH\n",
    "    params:\n",
    "    - parameterName: hidden_units\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues:\n",
    "      - 10,10\n",
    "      - 64,32\n",
    "      - 128,64,32\n",
    "      - 32,64,128\n",
    "      - 128,128,128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Job [taxifare_191205_152623] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe taxifare_191205_152623\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs taxifare_191205_152623\n",
      "jobId: taxifare_191205_152623\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "OUTDIR=\"gs://{}/taxifare/trained_hp_tune\".format(BUCKET)\n",
    "!gsutil -m rm -rf {OUTDIR} # start fresh each time\n",
    "!gcloud ai-platform jobs submit training taxifare_$(date -u +%y%m%d_%H%M%S) \\\n",
    "    --package-path=taxifaremodel \\\n",
    "    --module-name=taxifaremodel.task \\\n",
    "    --config=hyperparam.yaml \\\n",
    "    --job-dir=gs://{BUCKET}/taxifare \\\n",
    "    --python-version=3.5 \\\n",
    "    --runtime-version={TFVERSION} \\\n",
    "    --region={REGION} \\\n",
    "    -- \\\n",
    "    --train_data_path=gs://{BUCKET}/taxifare/smallinput/taxi-train.csv \\\n",
    "    --eval_data_path=gs://{BUCKET}/taxifare/smallinput/taxi-valid.csv  \\\n",
    "    --train_steps=5000 \\\n",
    "    --output_dir={OUTDIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "## 위의 hyperParameter를 사용하여 최적의 Parameter를 찾은후 아래와 같이 모델을 학습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/#1574741357187743...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/checkpoint#1574742005846972...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/eval/#1574741445976101...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/eval/events.out.tfevents.1574741446.cmle-training-master-d5bcc2d20e-0-hntzx#1574742015682138...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/events.out.tfevents.1574741357.cmle-training-worker-d5bcc2d20e-0-rwl74#1574741999931339...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/events.out.tfevents.1574741360.cmle-training-master-d5bcc2d20e-0-hntzx#1574742008591169...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/export/#1574742016733972...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/export/exporter/#1574742017157997...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/export/exporter/1574742015/variables/#1574742025247572...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/export/exporter/1574742015/variables/variables.data-00000-of-00002#1574742025598444...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/export/exporter/1574742015/#1574742024576790...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/export/exporter/1574742015/saved_model.pb#1574742024909616...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/export/exporter/1574742015/variables/variables.data-00001-of-00002#1574742025900787...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/export/exporter/1574742015/variables/variables.index#1574742026190893...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/graph.pbtxt#1574741368004987...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-120038.data-00000-of-00004#1574741752063431...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-120038.data-00001-of-00004#1574741751572356...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-120038.data-00002-of-00004#1574741751156871...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-120038.data-00003-of-00004#1574741750749219...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-120038.index#1574741752462552...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-140046.index#1574741815347469...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-120038.meta#1574741756004673...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-140046.data-00000-of-00004#1574741814921961...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-140046.data-00001-of-00004#1574741814496999...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-140046.data-00002-of-00004#1574741814082462...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-140046.data-00003-of-00004#1574741813668948...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-140046.meta#1574741818897766...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-160050.data-00000-of-00004#1574741877132225...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-160050.data-00001-of-00004#1574741876728167...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-160050.data-00003-of-00004#1574741875919419...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-160050.index#1574741877554694...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-160050.meta#1574741881080571...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-180058.data-00000-of-00004#1574741941002228...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-180058.data-00001-of-00004#1574741940589168...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-180058.data-00002-of-00004#1574741940197408...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-180058.data-00003-of-00004#1574741939795127...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-180058.index#1574741941461534...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-180058.meta#1574741944858249...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-200007.data-00000-of-00004#1574742003806777...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-200007.data-00001-of-00004#1574742003364400...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-200007.data-00002-of-00004#1574742002918574...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-200007.data-00003-of-00004#1574742002503188...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-160050.data-00002-of-00004#1574741876328304...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-200007.index#1574742004304068...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/trained_large_tuned/model.ckpt-200007.meta#1574742008018381...\n",
      "/ [45/45 objects] 100% Done                                                     \n",
      "Operation completed over 45 objects.                                             \n",
      "Job [taxifare_large_191205_152805] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe taxifare_large_191205_152805\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs taxifare_large_191205_152805\n",
      "jobId: taxifare_large_191205_152805\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "OUTDIR=\"gs://{}/taxifare/trained_large_tuned\".format(BUCKET)\n",
    "!gsutil -m rm -rf {OUTDIR} # start fresh each time\n",
    "!gcloud ai-platform jobs submit training taxifare_large_$(date -u +%y%m%d_%H%M%S) \\\n",
    "    --package-path=taxifaremodel \\\n",
    "    --module-name=taxifaremodel.task \\\n",
    "    --job-dir=gs://{BUCKET}/taxifare \\\n",
    "    --python-version=3.5 \\\n",
    "    --runtime-version={TFVERSION} \\\n",
    "    --region={REGION} \\\n",
    "    --scale-tier=STANDARD_1 \\\n",
    "    -- \\\n",
    "    --train_data_path=gs://cloud-training-demos/taxifare/large/taxi-train*.csv \\\n",
    "    --eval_data_path=gs://cloud-training-demos/taxifare/small/taxi-valid.csv  \\\n",
    "    --train_steps=200000 \\\n",
    "    --output_dir={OUTDIR} \\\n",
    "    --hidden_units=\"128,64,32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
