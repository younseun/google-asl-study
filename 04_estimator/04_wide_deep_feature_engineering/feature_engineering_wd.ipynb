{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-training-demos/taxifare/small/taxi-test.csv...\n",
      "Copying gs://cloud-training-demos/taxifare/small/taxi-train.csv...              \n",
      "Copying gs://cloud-training-demos/taxifare/small/taxi-valid.csv...              \n",
      "/ [3 files][ 10.9 MiB/ 10.9 MiB]                                                \n",
      "Operation completed over 3 objects/10.9 MiB.                                     \n",
      "-rw-r--r-- 1 jupyter jupyter 1799474 Dec  5 15:03 taxi-test.csv\n",
      "-rw-r--r-- 1 jupyter jupyter 7986353 Dec  5 15:03 taxi-train.csv\n",
      "-rw-r--r-- 1 jupyter jupyter 1673742 Dec  5 15:03 taxi-valid.csv\n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://cloud-training-demos/taxifare/small/*.csv .\n",
    "!ls -l *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMN_NAMES = [\"fare_amount\",\"dayofweek\",\"hourofday\",\"pickuplon\",\"pickuplat\",\"dropofflon\",\"dropofflat\"]\n",
    "CSV_DEFAULTS = [[0.0],[1],[0],[-74.0],[40.0],[-74.0],[40.7]]\n",
    "\n",
    "def read_dataset(csv_path):\n",
    "    def _parse_row(row):\n",
    "        # Decode the CSV row into list of TF tensors\n",
    "        fields = tf.decode_csv(records = row, record_defaults = CSV_DEFAULTS)\n",
    "\n",
    "        # Pack the result into a dictionary\n",
    "        features = dict(zip(CSV_COLUMN_NAMES, fields))\n",
    "        \n",
    "        # NEW: Add engineered features\n",
    "        features = add_engineered_features(features)\n",
    "        \n",
    "        # Separate the label from the features\n",
    "        label = features.pop(\"fare_amount\") # remove label from features and store\n",
    "\n",
    "        return features, label\n",
    "    \n",
    "    # Create a dataset containing the text lines.\n",
    "    dataset = tf.data.Dataset.list_files(file_pattern = csv_path) # (i.e. data_file_*.csv)\n",
    "    dataset = dataset.flat_map(map_func = lambda filename:tf.data.TextLineDataset(filenames = filename).skip(count = 1))\n",
    "\n",
    "    # Parse each CSV row into correct (features,label) format for Estimator API\n",
    "    dataset = dataset.map(map_func = _parse_row)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def train_input_fn(csv_path, batch_size = 128):\n",
    "    #1. Convert CSV into tf.data.Dataset with (features,label) format\n",
    "    dataset = read_dataset(csv_path)\n",
    "      \n",
    "    #2. Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(buffer_size = 1000).repeat(count = None).batch(batch_size = batch_size)\n",
    "   \n",
    "    return dataset\n",
    "\n",
    "def eval_input_fn(csv_path, batch_size = 128):\n",
    "    #1. Convert CSV into tf.data.Dataset with (features,label) format\n",
    "    dataset = read_dataset(csv_path)\n",
    "\n",
    "    #2.Batch the examples.\n",
    "    dataset = dataset.batch(batch_size = batch_size)\n",
    "   \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. One hot encode dayofweek and hourofday\n",
    "fc_dayofweek = tf.feature_column.categorical_column_with_identity(key = \"dayofweek\", num_buckets = 7)\n",
    "fc_hourofday = tf.feature_column.categorical_column_with_identity(key = \"hourofday\", num_buckets = 24)\n",
    "\n",
    "# 2. Bucketize latitudes and longitudes\n",
    "NBUCKETS = 16\n",
    "latbuckets = np.linspace(start = 38.0, stop = 42.0, num = NBUCKETS).tolist()\n",
    "lonbuckets = np.linspace(start = -76.0, stop = -72.0, num = NBUCKETS).tolist()\n",
    "fc_bucketized_plat = tf.feature_column.bucketized_column(source_column = tf.feature_column.numeric_column(key = \"pickuplon\"), boundaries = lonbuckets)\n",
    "fc_bucketized_plon = tf.feature_column.bucketized_column(source_column = tf.feature_column.numeric_column(key = \"pickuplat\"), boundaries = latbuckets)\n",
    "fc_bucketized_dlat = tf.feature_column.bucketized_column(source_column = tf.feature_column.numeric_column(key = \"dropofflon\"), boundaries = lonbuckets)\n",
    "fc_bucketized_dlon = tf.feature_column.bucketized_column(source_column = tf.feature_column.numeric_column(key = \"dropofflat\"), boundaries = latbuckets)\n",
    "\n",
    "# 3. Cross features to get combination of day and hour\n",
    "fc_crossed_day_hr = tf.feature_column.crossed_column(keys = [fc_dayofweek, fc_hourofday], hash_bucket_size = 24 * 7)\n",
    "fc_crossed_dloc = tf.feature_column.crossed_column(keys = [fc_bucketized_dlat, fc_bucketized_dlon], hash_bucket_size = NBUCKETS * NBUCKETS)\n",
    "fc_crossed_ploc = tf.feature_column.crossed_column(keys = [fc_bucketized_plat, fc_bucketized_plon], hash_bucket_size = NBUCKETS * NBUCKETS)\n",
    "fc_crossed_pd_pair = tf.feature_column.crossed_column(keys = [fc_crossed_dloc, fc_crossed_ploc], hash_bucket_size = NBUCKETS**4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering add\n",
    "## read_dataset와 serving_fn에 함수를 사용한다.\n",
    "###   - read_dataset : features = add_engineered_features(features)\n",
    "###   - serving_fn : features = add_engineered_features(receiver_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_engineered_features(features):\n",
    "    features[\"dayofweek\"] = features[\"dayofweek\"] - 1 # subtract one since our days of week are 1-7 instead of 0-6\n",
    "    \n",
    "    features[\"latdiff\"] = features[\"pickuplat\"] - features[\"dropofflat\"] # East/West\n",
    "    features[\"londiff\"] = features[\"pickuplon\"] - features[\"dropofflon\"] # North/South\n",
    "    features[\"euclidean_dist\"] = tf.sqrt(x = features[\"latdiff\"]**2 + features[\"londiff\"]**2)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wide_deep():\n",
    "    # Wide columns are sparse, have linear relationship with the output\n",
    "    wide_columns = [\n",
    "        # Feature crosses\n",
    "        fc_crossed_day_hr, fc_crossed_dloc, \n",
    "        fc_crossed_ploc, fc_crossed_pd_pair,\n",
    "        \n",
    "        # Sparse columns\n",
    "        fc_dayofweek, fc_hourofday\n",
    "    ]\n",
    "    \n",
    "    # Continuous columns are deep, have a complex relationship with the output\n",
    "    deep_columns = [\n",
    "        # Embedding_column to \"group\" together ...\n",
    "        tf.feature_column.embedding_column(categorical_column = fc_crossed_pd_pair, dimension = 10),\n",
    "        tf.feature_column.embedding_column(categorical_column = fc_crossed_day_hr, dimension = 10),\n",
    "\n",
    "        # Numeric columns\n",
    "        tf.feature_column.numeric_column(key = \"pickuplat\"),\n",
    "        tf.feature_column.numeric_column(key = \"pickuplon\"),\n",
    "        tf.feature_column.numeric_column(key = \"dropofflon\"),\n",
    "        tf.feature_column.numeric_column(key = \"dropofflat\"),\n",
    "        tf.feature_column.numeric_column(key = \"latdiff\"),\n",
    "        tf.feature_column.numeric_column(key = \"londiff\"),\n",
    "        tf.feature_column.numeric_column(key = \"euclidean_dist\"),\n",
    "        \n",
    "        tf.feature_column.indicator_column(categorical_column = fc_crossed_day_hr),\n",
    "    ]\n",
    "    \n",
    "    return wide_columns, deep_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    receiver_tensors = {\n",
    "        'dayofweek' : tf.placeholder(dtype = tf.int32, shape = [None]), # shape is vector to allow batch of requests\n",
    "        'hourofday' : tf.placeholder(dtype = tf.int32, shape = [None]),\n",
    "        'pickuplon' : tf.placeholder(dtype = tf.float32, shape = [None]), \n",
    "        'pickuplat' : tf.placeholder(dtype = tf.float32, shape = [None]),\n",
    "        'dropofflat' : tf.placeholder(dtype = tf.float32, shape = [None]),\n",
    "        'dropofflon' : tf.placeholder(dtype = tf.float32, shape = [None]),\n",
    "    }\n",
    "    \n",
    "    features = add_engineered_features(receiver_tensors) # 'features' is what is passed on to the model\n",
    "    \n",
    "    return tf.estimator.export.ServingInputReceiver(features = features, receiver_tensors = receiver_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_protocol': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f73ab7f2518>, '_save_summary_steps': 100, '_tf_random_seed': 1, '_log_step_count_steps': 100, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_model_dir': 'taxi_trained_wd/500', '_task_type': 'worker', '_master': '', '_is_chief': True, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_num_worker_replicas': 1, '_service': None, '_experimental_max_worker_delay_secs': None, '_eval_distribute': None, '_evaluation_master': '', '_train_distribute': None, '_device_fn': None, '_session_creation_timeout_secs': 7200, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_steps': 100, '_save_checkpoints_secs': None, '_experimental_distribute': None, '_global_id_in_cluster': 0, '_task_id': 0}\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "INFO:tensorflow:Using config: {'_protocol': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f73ab7ea0f0>, '_save_summary_steps': 100, '_tf_random_seed': 1, '_log_step_count_steps': 100, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_model_dir': 'taxi_trained_wd/500', '_task_type': 'worker', '_master': '', '_is_chief': True, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_num_worker_replicas': 1, '_service': None, '_experimental_max_worker_delay_secs': None, '_eval_distribute': None, '_evaluation_master': '', '_train_distribute': None, '_device_fn': None, '_session_creation_timeout_secs': 7200, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_steps': 100, '_save_checkpoints_secs': None, '_experimental_distribute': None, '_global_id_in_cluster': 0, '_task_id': 0}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 100 or save_checkpoints_secs None.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.decode_csv is deprecated. Please use tf.io.decode_csv instead.\n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/feature_column/feature_column_v2.py:3079: CrossedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/feature_column/feature_column_v2.py:305: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4271: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/canned/linear.py:308: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into taxi_trained_wd/500/model.ckpt.\n",
      "INFO:tensorflow:loss = 9443.549, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 100 into taxi_trained_wd/500/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-05T15:04:26Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained_wd/500/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-05-15:04:31\n",
      "INFO:tensorflow:Saving dict for global step 100: average_loss = 86.44791, global_step = 100, label/mean = 11.229713, loss = 11047.279, prediction/mean = 10.201937, rmse = 9.297736\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 100: taxi_trained_wd/500/model.ckpt-100\n",
      "INFO:tensorflow:global_step/sec: 9.61778\n",
      "INFO:tensorflow:loss = 9202.782, step = 101 (10.400 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into taxi_trained_wd/500/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-05T15:04:34Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained_wd/500/model.ckpt-200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-05-15:04:39\n",
      "INFO:tensorflow:Saving dict for global step 200: average_loss = 85.13286, global_step = 200, label/mean = 11.229713, loss = 10879.227, prediction/mean = 11.509067, rmse = 9.226747\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 200: taxi_trained_wd/500/model.ckpt-200\n",
      "INFO:tensorflow:global_step/sec: 12.4682\n",
      "INFO:tensorflow:loss = 17276.396, step = 201 (8.021 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 300 into taxi_trained_wd/500/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-05T15:04:42Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained_wd/500/model.ckpt-300\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-05-15:04:48\n",
      "INFO:tensorflow:Saving dict for global step 300: average_loss = 84.820206, global_step = 300, label/mean = 11.229713, loss = 10839.273, prediction/mean = 11.270293, rmse = 9.209788\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 300: taxi_trained_wd/500/model.ckpt-300\n",
      "INFO:tensorflow:global_step/sec: 11.7504\n",
      "INFO:tensorflow:loss = 9122.508, step = 301 (8.510 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 400 into taxi_trained_wd/500/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-05T15:04:51Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained_wd/500/model.ckpt-400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-05-15:04:56\n",
      "INFO:tensorflow:Saving dict for global step 400: average_loss = 84.64228, global_step = 400, label/mean = 11.229713, loss = 10816.535, prediction/mean = 11.136086, rmse = 9.200124\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 400: taxi_trained_wd/500/model.ckpt-400\n",
      "INFO:tensorflow:global_step/sec: 11.7487\n",
      "INFO:tensorflow:loss = 6696.9473, step = 401 (8.511 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 500 into taxi_trained_wd/500/model.ckpt.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-05T15:04:59Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained_wd/500/model.ckpt-500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-05-15:05:05\n",
      "INFO:tensorflow:Saving dict for global step 500: average_loss = 84.4876, global_step = 500, label/mean = 11.229713, loss = 10796.77, prediction/mean = 11.026889, rmse = 9.191713\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 500: taxi_trained_wd/500/model.ckpt-500\n",
      "INFO:tensorflow:Performing the final export in the end of training.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Regression input must be a single string Tensor; got {'pickuplon': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'dayofweek': <tf.Tensor 'sub:0' shape=(?,) dtype=int32>, 'hourofday': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=int32>, 'dropofflon': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=float32>, 'latdiff': <tf.Tensor 'sub_1:0' shape=(?,) dtype=float32>, 'dropofflat': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=float32>, 'londiff': <tf.Tensor 'sub_2:0' shape=(?,) dtype=float32>, 'pickuplat': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'euclidean_dist': <tf.Tensor 'Sqrt:0' shape=(?,) dtype=float32>}\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'pickuplon': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'dayofweek': <tf.Tensor 'sub:0' shape=(?,) dtype=int32>, 'hourofday': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=int32>, 'dropofflon': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=float32>, 'latdiff': <tf.Tensor 'sub_1:0' shape=(?,) dtype=float32>, 'dropofflat': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=float32>, 'londiff': <tf.Tensor 'sub_2:0' shape=(?,) dtype=float32>, 'pickuplat': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'euclidean_dist': <tf.Tensor 'Sqrt:0' shape=(?,) dtype=float32>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained_wd/500/model.ckpt-500\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: taxi_trained_wd/500/export/exporter/temp-b'1575558305'/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 10922.328.\n",
      "CPU times: user 3min 50s, sys: 3min 34s, total: 7min 24s\n",
      "Wall time: 52.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'average_loss': 84.4876,\n",
       "  'global_step': 500,\n",
       "  'label/mean': 11.229713,\n",
       "  'loss': 10796.77,\n",
       "  'prediction/mean': 11.026889,\n",
       "  'rmse': 9.191713},\n",
       " [b'taxi_trained_wd/500/export/exporter/1575558305'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "OUTDIR = \"taxi_trained_wd/500\"\n",
    "shutil.rmtree(path = OUTDIR, ignore_errors = True) # start fresh each time\n",
    "tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "tf.logging.set_verbosity(v = tf.logging.INFO) # so loss is printed during training\n",
    "\n",
    "# Collect the wide and deep columns from above\n",
    "wide_columns, deep_columns = get_wide_deep()\n",
    "\n",
    "model = tf.estimator.DNNLinearCombinedRegressor(\n",
    "    model_dir = OUTDIR,\n",
    "    linear_feature_columns = wide_columns,\n",
    "    dnn_feature_columns = deep_columns,\n",
    "    dnn_hidden_units = [10,10], # specify neural architecture\n",
    "    config = tf.estimator.RunConfig(\n",
    "        tf_random_seed = 1, # for reproducibility\n",
    "        save_checkpoints_steps = 100 # checkpoint every N steps\n",
    "    ) \n",
    ")\n",
    "\n",
    "# Add custom evaluation metric\n",
    "def my_rmse(labels, predictions):\n",
    "    pred_values = tf.squeeze(input = predictions[\"predictions\"], axis = -1)\n",
    "    return {\"rmse\": tf.metrics.root_mean_squared_error(labels = labels, predictions = pred_values)}\n",
    "\n",
    "model = tf.contrib.estimator.add_metrics(estimator = model, metric_fn = my_rmse) \n",
    "    \n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn = lambda: train_input_fn(\"./taxi-train.csv\"),\n",
    "    max_steps = 500)\n",
    "\n",
    "exporter = tf.estimator.FinalExporter(name = \"exporter\", serving_input_receiver_fn = serving_input_receiver_fn) # export SavedModel once at the end of training\n",
    "# Note: alternatively use tf.estimator.BestExporter to export at every checkpoint that has lower loss than the previous checkpoint\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn = lambda: eval_input_fn(\"./taxi-valid.csv\"),\n",
    "    steps = None,\n",
    "    start_delay_secs = 1, # wait at least N seconds before first evaluation (default 120)\n",
    "    throttle_secs = 1, # wait at least N seconds before each subsequent evaluation (default 600)\n",
    "    exporters = exporter) # export SavedModel once at the end of training\n",
    "\n",
    "tf.estimator.train_and_evaluate(estimator = model, train_spec = train_spec, eval_spec = eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
