{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-training-demos/taxifare/small/taxi-test.csv...\n",
      "Copying gs://cloud-training-demos/taxifare/small/taxi-train.csv...              \n",
      "Copying gs://cloud-training-demos/taxifare/small/taxi-valid.csv...              \n",
      "- [3 files][ 10.9 MiB/ 10.9 MiB]                                                \n",
      "Operation completed over 3 objects/10.9 MiB.                                     \n",
      "-rw-r--r-- 1 jupyter jupyter 1799474 Dec  5 15:15 taxi-test.csv\n",
      "-rw-r--r-- 1 jupyter jupyter 7986353 Dec  5 15:15 taxi-train.csv\n",
      "-rw-r--r-- 1 jupyter jupyter 1673742 Dec  5 15:15 taxi-valid.csv\n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://cloud-training-demos/taxifare/small/*.csv .\n",
    "!ls -l *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate input functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMN_NAMES = [\"fare_amount\",\"dayofweek\",\"hourofday\",\"pickuplon\",\"pickuplat\",\"dropofflon\",\"dropofflat\"]\n",
    "CSV_DEFAULTS = [[0.0],[1],[0],[-74.0], [40.0], [-74.0], [40.7]]\n",
    "\n",
    "def read_dataset(csv_path):\n",
    "    def parse_row(row):\n",
    "        # Decode the CSV row into list of TF tensors\n",
    "        fields = tf.decode_csv(records = row, record_defaults = CSV_DEFAULTS)\n",
    "\n",
    "        # Pack the result into a dictionary\n",
    "        features = dict(zip(CSV_COLUMN_NAMES, fields))\n",
    "        \n",
    "        # NEW: Add engineered features\n",
    "        features = add_engineered_features(features)\n",
    "        \n",
    "        # Separate the label from the features\n",
    "        label = features.pop(\"fare_amount\") # remove label from features and store\n",
    "\n",
    "        return features, label\n",
    "    \n",
    "    # Create a dataset containing the text lines.\n",
    "    dataset = tf.data.Dataset.list_files(file_pattern = csv_path) # (i.e. data_file_*.csv)\n",
    "    dataset = dataset.flat_map(map_func = lambda filename: tf.data.TextLineDataset(filenames = filename).skip(count = 1))\n",
    "\n",
    "    # Parse each CSV row into correct (features,label) format for Estimator API\n",
    "    dataset = dataset.map(map_func = parse_row)\n",
    "    \n",
    "    return dataset\n",
    "  \n",
    "def create_feature_keras_input(features, label):\n",
    "    features = tf.feature_column.input_layer(features = features, feature_columns = create_feature_columns())\n",
    "    return features, label\n",
    "\n",
    "def train_input_fn(csv_path, batch_size = 128):\n",
    "    #1. Convert CSV into tf.data.Dataset with (features, label) format\n",
    "    dataset = read_dataset(csv_path)\n",
    "      \n",
    "    #2. Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(buffer_size = 1000).repeat(count = None).batch(batch_size = batch_size)\n",
    "    \n",
    "    #3. Create single feature tensor for input to Keras Model\n",
    "    dataset = dataset.map(map_func = create_feature_keras_input)\n",
    "   \n",
    "    return dataset\n",
    "\n",
    "def eval_input_fn(csv_path, batch_size = 128):\n",
    "    #1. Convert CSV into tf.data.Dataset with (features, label) format\n",
    "    dataset = read_dataset(csv_path)\n",
    "\n",
    "    #2.Batch the examples.\n",
    "    dataset = dataset.batch(batch_size = batch_size)\n",
    "    \n",
    "    #3. Create single feature tensor for input to Keras Model\n",
    "    dataset = dataset.map(map_func = create_feature_keras_input)\n",
    "   \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_engineered_features(features):\n",
    "    features[\"dayofweek\"] = features[\"dayofweek\"] - 1 # subtract one since our days of week are 1-7 instead of 0-6\n",
    "    \n",
    "    features[\"latdiff\"] = features[\"pickuplat\"] - features[\"dropofflat\"] # East/West\n",
    "    features[\"londiff\"] = features[\"pickuplon\"] - features[\"dropofflon\"] # North/South\n",
    "    features[\"euclidean_dist\"] = tf.sqrt(x = features[\"latdiff\"]**2 + features[\"londiff\"]**2)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_columns():\n",
    "    # One hot encode dayofweek and hourofday\n",
    "    fc_dayofweek = tf.feature_column.categorical_column_with_identity(key = \"dayofweek\", num_buckets = 7)\n",
    "    fc_hourofday = tf.feature_column.categorical_column_with_identity(key = \"hourofday\", num_buckets = 24)\n",
    "\n",
    "    # Cross features to get combination of day and hour\n",
    "    fc_day_hr = tf.feature_column.crossed_column(keys = [fc_dayofweek, fc_hourofday], hash_bucket_size = 24 * 7)\n",
    "\n",
    "    # Bucketize latitudes and longitudes\n",
    "    NBUCKETS = 16\n",
    "    latbuckets = np.linspace(start = 38.0, stop = 42.0, num = NBUCKETS).tolist()\n",
    "    lonbuckets = np.linspace(start = -76.0, stop = -72.0, num = NBUCKETS).tolist()\n",
    "    fc_bucketized_plat = tf.feature_column.bucketized_column(source_column = tf.feature_column.numeric_column(key = \"pickuplon\"), boundaries = lonbuckets)\n",
    "    fc_bucketized_plon = tf.feature_column.bucketized_column(source_column = tf.feature_column.numeric_column(key = \"pickuplat\"), boundaries = latbuckets)\n",
    "    fc_bucketized_dlat = tf.feature_column.bucketized_column(source_column = tf.feature_column.numeric_column(key = \"dropofflon\"), boundaries = lonbuckets)\n",
    "    fc_bucketized_dlon = tf.feature_column.bucketized_column(source_column = tf.feature_column.numeric_column(key = \"dropofflat\"), boundaries = latbuckets)\n",
    "\n",
    "    feature_columns = [\n",
    "        #1. Engineered using tf.feature_column module\n",
    "        tf.feature_column.indicator_column(categorical_column = fc_day_hr), # 168 columns\n",
    "        fc_bucketized_plat, # 16 + 1 = 17 columns\n",
    "        fc_bucketized_plon, # 16 + 1 = 17 columns\n",
    "        fc_bucketized_dlat, # 16 + 1 = 17 columns\n",
    "        fc_bucketized_dlon, # 16 + 1 = 17 columns\n",
    "        #2. Engineered in input functions\n",
    "        tf.feature_column.numeric_column(key = \"latdiff\"), # 1 column\n",
    "        tf.feature_column.numeric_column(key = \"londiff\"), # 1 column\n",
    "        tf.feature_column.numeric_column(key = \"euclidean_dist\") # 1 column\n",
    "    ]\n",
    "  \n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_feature_columns = 239\n"
     ]
    }
   ],
   "source": [
    "num_feature_columns = 168 + (16 + 1) * 4 + 3\n",
    "print(\"num_feature_columns = {}\".format(num_feature_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Custom Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape = (num_feature_columns,), name = \"dense_input\"))\n",
    "    model.add(tf.keras.layers.Dense(units = 64, activation = \"relu\", name = \"dense0\"))\n",
    "    model.add(tf.keras.layers.Dense(units = 64, activation = \"relu\", name = \"dense1\"))\n",
    "    model.add(tf.keras.layers.Dense(units = 64, activation = \"relu\", name = \"dense2\"))\n",
    "    model.add(tf.keras.layers.Dense(units = 64, activation = \"relu\", name = \"dense3\"))\n",
    "    model.add(tf.keras.layers.Dense(units = 8, activation = \"relu\", name = \"dense4\"))\n",
    "    model.add(tf.keras.layers.Dense(units = 1, activation = None, name = \"logits\"))\n",
    "\n",
    "    def rmse(y_true, y_pred): # Root Mean Squared Error\n",
    "        return tf.sqrt(x = tf.reduce_mean(input_tensor = tf.square(x = y_pred - y_true)))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = tf.train.AdamOptimizer(),\n",
    "        loss = \"mean_squared_error\",\n",
    "        metrics = [rmse])\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create serving input function\n",
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "        \"dayofweek\": tf.placeholder(dtype = tf.int32, shape = [None]),\n",
    "        \"hourofday\": tf.placeholder(dtype = tf.int32, shape = [None]),\n",
    "        \"pickuplon\": tf.placeholder(dtype = tf.float32, shape = [None]),\n",
    "        \"pickuplat\": tf.placeholder(dtype = tf.float32, shape = [None]),\n",
    "        \"dropofflon\": tf.placeholder(dtype = tf.float32, shape = [None]),\n",
    "        \"dropofflat\": tf.placeholder(dtype = tf.float32, shape = [None]),\n",
    "    }\n",
    "\n",
    "    features = {key: tensor for key, tensor in feature_placeholders.items()}\n",
    "\n",
    "    # Perform our feature engineering during inference as well\n",
    "    features, _ = create_feature_keras_input((add_engineered_features(features)), None)\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(features = {\"dense_input\": features}, receiver_tensors = feature_placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(output_dir):\n",
    "    tf.logging.set_verbosity(v = tf.logging.INFO) # so loss is printed during training\n",
    "        \n",
    "    estimator = tf.keras.estimator.model_to_estimator(\n",
    "        keras_model = create_keras_model(),\n",
    "        model_dir = output_dir,\n",
    "        config = tf.estimator.RunConfig(\n",
    "            tf_random_seed = 1, # for reproducibility\n",
    "            save_checkpoints_steps = 100 # checkpoint every N steps\n",
    "        )\n",
    "    )\n",
    "\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = lambda: train_input_fn(csv_path = \"./taxi-train.csv\"),\n",
    "        max_steps = 500)\n",
    "\n",
    "    exporter = tf.estimator.LatestExporter(name = 'exporter', serving_input_receiver_fn = serving_input_fn)\n",
    "\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = lambda: eval_input_fn(csv_path = \"./taxi-valid.csv\"),\n",
    "        steps = None,\n",
    "        start_delay_secs = 10, # wait at least N seconds before first evaluation (default 120)\n",
    "        throttle_secs = 10, # wait at least N seconds before each subsequent evaluation (default 600)\n",
    "        exporters = exporter) # export SavedModel once at the end of training\n",
    "\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        estimator = estimator, \n",
    "        train_spec = train_spec, \n",
    "        eval_spec = eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using the Keras model provided.\n",
      "INFO:tensorflow:Using config: {'_service': None, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_log_step_count_steps': 100, '_task_id': 0, '_keep_checkpoint_every_n_hours': 10000, '_keep_checkpoint_max': 5, '_experimental_distribute': None, '_train_distribute': None, '_protocol': None, '_is_chief': True, '_save_summary_steps': 100, '_device_fn': None, '_session_creation_timeout_secs': 7200, '_global_id_in_cluster': 0, '_tf_random_seed': 1, '_task_type': 'worker', '_evaluation_master': '', '_master': '', '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_eval_distribute': None, '_model_dir': 'taxi_trained', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1d2cee4940>, '_experimental_max_worker_delay_secs': None, '_save_checkpoints_secs': None, '_save_checkpoints_steps': 100}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 100 or save_checkpoints_secs None.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='taxi_trained/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
      "INFO:tensorflow:Warm-starting from: taxi_trained/keras/keras_model.ckpt\n",
      "INFO:tensorflow:Warm-starting variables only in TRAINABLE_VARIABLES.\n",
      "INFO:tensorflow:Warm-started 12 variables.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 140.85356, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 100 into taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-05T15:16:11Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-05-15:16:14\n",
      "INFO:tensorflow:Saving dict for global step 100: global_step = 100, loss = 86.14089, rmse = 9.102576\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 100: taxi_trained/model.ckpt-100\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-100\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: taxi_trained/export/exporter/temp-b'1575558974'/saved_model.pb\n",
      "INFO:tensorflow:global_step/sec: 14.8519\n",
      "INFO:tensorflow:loss = 70.48978, step = 101 (6.736 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (10 secs).\n",
      "INFO:tensorflow:global_step/sec: 52.3372\n",
      "INFO:tensorflow:loss = 137.07913, step = 201 (1.910 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 300 into taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (10 secs).\n",
      "INFO:tensorflow:global_step/sec: 50.5665\n",
      "INFO:tensorflow:loss = 73.13921, step = 301 (1.977 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 400 into taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-05T15:16:21Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-05-15:16:25\n",
      "INFO:tensorflow:Saving dict for global step 400: global_step = 400, loss = 85.86629, rmse = 9.023483\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 400: taxi_trained/model.ckpt-400\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-400\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: taxi_trained/export/exporter/temp-b'1575558985'/saved_model.pb\n",
      "INFO:tensorflow:global_step/sec: 14.6269\n",
      "INFO:tensorflow:loss = 52.208904, step = 401 (6.838 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 500 into taxi_trained/model.ckpt.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (10 secs).\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-05T15:16:28Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-05-15:16:32\n",
      "INFO:tensorflow:Saving dict for global step 500: global_step = 500, loss = 86.001236, rmse = 9.025314\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 500: taxi_trained/model.ckpt-500\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-500\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: taxi_trained/export/exporter/temp-b'1575558992'/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 86.2482.\n",
      "CPU times: user 2min 27s, sys: 2min 51s, total: 5min 18s\n",
      "Wall time: 28.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "OUTDIR = \"taxi_trained\"\n",
    "shutil.rmtree(path = OUTDIR, ignore_errors = True) # start fresh each time\n",
    "tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "\n",
    "train_and_evaluate(OUTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
