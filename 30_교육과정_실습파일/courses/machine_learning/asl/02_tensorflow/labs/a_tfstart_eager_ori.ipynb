{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with TensorFlow (Eager Mode)\n",
    "\n",
    "**Learning Objectives**\n",
    "  - Understand difference between Tensorflow's two modes: Eager Execution and Graph Execution\n",
    "  - Practice defining and performing basic operations on constant Tensors\n",
    "  - Use Tensorflow's automatic differentiation capability\n",
    "\n",
    "## Introduction\n",
    "**Eager Execution**\n",
    "\n",
    "Eager mode evaluates operations immediatley and return concrete values immediately. To enable eager mode simply place `tf.enable_eager_execution()` at the top of your code. We recommend using eager execution when prototyping as it is intuitive, easier to debug, and requires less boilerplate code.\n",
    "\n",
    "**Graph Execution**\n",
    "\n",
    "Graph mode is TensorFlow's default execution mode (although it will change to eager with TF 2.0). In graph mode operations only produce a symbolic graph which doesn't get executed until run within the context of a tf.Session(). This style of coding is less inutitive and has more boilerplate, however it can lead to performance optimizations and is particularly suited for distributing training across multiple devices. We recommend using delayed execution for performance sensitive production code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Two Tensors \n",
    "\n",
    "The value of the tensor, as well as its shape and data type are printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 8  2 10], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(value = [5, 3, 8], dtype = tf.int32)\n",
    "b = tf.constant(value = [3, -1, 2], dtype = tf.int32)\n",
    "c = tf.add(x = a, y = b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overloaded Operators\n",
    "We can also perform a `tf.add()` using the `+` operator. The `/,-,*` and `**` operators are similarly overloaded with the appropriate tensorflow operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 8  2 10], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "c = a + b # this is equivalent to tf.add(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy Interoperability\n",
    "\n",
    "In addition to native TF tensors, tensorflow operations can take native python types and NumPy arrays as operands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'tensorflow.python.framework.ops.EagerTensor'>, Value: [4 6]\n",
      "Type: <class 'tensorflow.python.framework.ops.EagerTensor'>, Value: [4 6]\n",
      "Type: <class 'tensorflow.python.framework.ops.EagerTensor'>, Value: [4 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "a_py = [1,2] # native python list\n",
    "b_py = [3,4] # native python list\n",
    "\n",
    "a_np = np.array(object = [1,2]) # numpy array\n",
    "b_np = np.array(object = [3,4]) # numpy array\n",
    "\n",
    "a_tf = tf.constant(value = [1,2], dtype = tf.int32) # native TF tensor\n",
    "b_tf = tf.constant(value = [3,4], dtype = tf.int32) # native TF tensor\n",
    "\n",
    "for result in [tf.add(x = a_py, y = b_py), tf.add(x = a_np, y = b_np), tf.add(x = a_tf, y = b_tf)]:\n",
    "    print(\"Type: {}, Value: {}\".format(type(result), result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can convert a native TF tensor to a NumPy array using .numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tf.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "Now let's use low level tensorflow operations to implement linear regression.\n",
    "\n",
    "Later in the course you'll see abstracted ways to do this using high level TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toy Dataset\n",
    "\n",
    "We'll model the following function:\n",
    "\n",
    "\\begin{equation}\n",
    "y= 2x + 10\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      "Y:[12. 14. 16. 18. 20. 22. 24. 26. 28. 30.]\n"
     ]
    }
   ],
   "source": [
    "X = tf.constant(value = [1,2,3,4,5,6,7,8,9,10], dtype = tf.float32)\n",
    "Y = 2 * X + 10\n",
    "print(\"X:{}\".format(X))\n",
    "print(\"Y:{}\".format(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function\n",
    "\n",
    "Using mean squared error, our loss function is:\n",
    "\\begin{equation}\n",
    "MSE = \\frac{1}{m}\\sum_{i=1}^{m}(\\hat{Y}_i-Y_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "$\\hat{Y}$ represents the vector containing our model's predictions:\n",
    "\\begin{equation}\n",
    "\\hat{Y} = w_oX + w_1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 1**\n",
    "\n",
    "The function `loss_mse` below takes four arguments: the tensors $X$, $Y$ and the weights $w_0$ and $w_1$. Complete the function below to compute the Mean Square Error (MSE). Hint: [check out](https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean) the `tf.reduce_mean` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mse(X, Y, w0, w1):\n",
    "    # TODO: Your code goes here\n",
    "    Y_hat =(w0 * X + w1)\n",
    "    return tf.reduce_mean((Y_hat - Y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Function\n",
    "\n",
    "To use gradient descent we need to take the partial derivative of the loss function with respect to each of the weights. We could manually compute the derivatives, but with Tensorflow's automatic differentiation capabilities we don't have to!\n",
    "\n",
    "During gradient descent we think of the loss as a function of the parameters $w_0$ and $w_1$. Thus, we want to compute the partial derivative with respect to these variables. The `params=[2,3]` argument tells TensorFlow to only compute derivatives with respect to the 2nd and 3rd arguments to the loss function (counting from 0, so really the 3rd and 4th)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting from 0, the 2nd and 3rd parameter to the loss function are our weights\n",
    "grad_f = tf.contrib.eager.gradients_function(f = loss_mse, params = [2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop\n",
    "\n",
    "Here we have a very simple training loop that converges. Note we are ignoring best practices like batching, creating a separate test set, and random weight initialization for the sake of simplicity.\n",
    "\n",
    "#### **Exercise 2**\n",
    "\n",
    "Complete the code to update the parameters $w_0$ and $w_1$ according to the gradients `d_w0` and `d_w1` and the specified learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 0 MSE: 167.6111602783203\n",
      "STEP: 100 MSE: 3.5321757793426514\n",
      "STEP: 200 MSE: 0.6537718176841736\n",
      "STEP: 300 MSE: 0.12100745737552643\n",
      "STEP: 400 MSE: 0.022397063672542572\n",
      "STEP: 500 MSE: 0.004145539831370115\n",
      "STEP: 600 MSE: 0.0007674093940295279\n",
      "STEP: 700 MSE: 0.00014202017337083817\n",
      "STEP: 800 MSE: 2.628635775181465e-05\n",
      "STEP: 900 MSE: 4.86889211970265e-06\n",
      "STEP: 1000 MSE: 9.178326081382693e-07\n",
      "w0:2.0003\n",
      "w1:9.9979\n"
     ]
    }
   ],
   "source": [
    "STEPS = 1000\n",
    "LEARNING_RATE = .02\n",
    "\n",
    "# Initialize weights\n",
    "w0 = tf.constant(value = 0.0, dtype = tf.float32)\n",
    "w1 = tf.constant(value = 0.0, dtype = tf.float32)\n",
    "\n",
    "for step in range(STEPS):\n",
    "    #1. Calculate gradients\n",
    "    d_w0, d_w1 = grad_f(X, Y, w0, w1) # derivatives calculated by tensorflow!\n",
    "\n",
    "    #2. Update weights\n",
    "    w0 = w0 - LEARNING_RATE * d_w0  # TODO: Your code goes here\n",
    "    w1 = w1 - LEARNING_RATE * d_w1  # TODO: Your code goes here\n",
    "\n",
    "    #3. Periodically print MSE\n",
    "    if step % 100 == 0:\n",
    "        print(\"STEP: {} MSE: {}\".format(step,loss_mse(X, Y, w0, w1)))\n",
    "\n",
    "# Print final MSE and weights\n",
    "print(\"STEP: {} MSE: {}\".format(STEPS,loss_mse(X, Y, w0, w1)))\n",
    "print(\"w0:{}\".format(round(float(w0), 4)))\n",
    "print(\"w1:{}\".format(round(float(w1), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try modelling a non-linear function such as: $y=xe^{-x^2}$\n",
    "\n",
    "Hint: Creating more training data will help. Also, you will need to build non-linear features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3bd04e1978>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X = tf.constant(value = np.linspace(0,2,1000), dtype = tf.float32)\n",
    "Y = X*np.exp(-X**2) * X\n",
    "plt.plot(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.append(tf.ones_like(X))  # Bias.\n",
    "features.append(tf.square(X))\n",
    "features.append(tf.sqrt(X))\n",
    "features.append(tf.exp(X))\n",
    "# return tf.stack(features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.002002002, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(4.0080117e-06, shape=(), dtype=float32)\n",
      "tf.Tensor(0.044743735, shape=(), dtype=float32)\n",
      "tf.Tensor(1.002004, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(features[0][1])\n",
    "print(features[1][1])\n",
    "print(features[2][1])\n",
    "print(features[3][1])\n",
    "print(features[4][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=106190, shape=(1000,), dtype=float32, numpy=\n",
       " array([0.        , 0.002002  , 0.004004  , 0.00600601, 0.00800801,\n",
       "        0.01001001, 0.01201201, 0.01401401, 0.01601602, 0.01801802,\n",
       "        0.02002002, 0.02202202, 0.02402402, 0.02602603, 0.02802803,\n",
       "        0.03003003, 0.03203203, 0.03403403, 0.03603604, 0.03803804,\n",
       "        0.04004004, 0.04204204, 0.04404404, 0.04604604, 0.04804805,\n",
       "        0.05005005, 0.05205205, 0.05405406, 0.05605606, 0.05805806,\n",
       "        0.06006006, 0.06206206, 0.06406406, 0.06606606, 0.06806806,\n",
       "        0.07007007, 0.07207207, 0.07407407, 0.07607608, 0.07807808,\n",
       "        0.08008008, 0.08208209, 0.08408409, 0.08608609, 0.08808809,\n",
       "        0.09009009, 0.09209209, 0.0940941 , 0.0960961 , 0.0980981 ,\n",
       "        0.1001001 , 0.1021021 , 0.1041041 , 0.1061061 , 0.10810811,\n",
       "        0.11011011, 0.11211211, 0.11411411, 0.11611611, 0.11811811,\n",
       "        0.12012012, 0.12212212, 0.12412412, 0.12612613, 0.12812813,\n",
       "        0.13013013, 0.13213213, 0.13413413, 0.13613613, 0.13813815,\n",
       "        0.14014015, 0.14214215, 0.14414415, 0.14614615, 0.14814815,\n",
       "        0.15015015, 0.15215215, 0.15415415, 0.15615615, 0.15815815,\n",
       "        0.16016015, 0.16216215, 0.16416417, 0.16616617, 0.16816817,\n",
       "        0.17017017, 0.17217217, 0.17417417, 0.17617618, 0.17817818,\n",
       "        0.18018018, 0.18218218, 0.18418418, 0.18618618, 0.1881882 ,\n",
       "        0.1901902 , 0.1921922 , 0.1941942 , 0.1961962 , 0.1981982 ,\n",
       "        0.2002002 , 0.2022022 , 0.2042042 , 0.2062062 , 0.2082082 ,\n",
       "        0.2102102 , 0.2122122 , 0.21421422, 0.21621622, 0.21821822,\n",
       "        0.22022022, 0.22222222, 0.22422422, 0.22622623, 0.22822823,\n",
       "        0.23023023, 0.23223223, 0.23423423, 0.23623623, 0.23823825,\n",
       "        0.24024025, 0.24224225, 0.24424425, 0.24624625, 0.24824825,\n",
       "        0.25025025, 0.25225225, 0.25425425, 0.25625625, 0.25825825,\n",
       "        0.26026025, 0.26226225, 0.26426426, 0.26626626, 0.26826826,\n",
       "        0.27027026, 0.27227226, 0.27427426, 0.2762763 , 0.2782783 ,\n",
       "        0.2802803 , 0.2822823 , 0.2842843 , 0.2862863 , 0.2882883 ,\n",
       "        0.2902903 , 0.2922923 , 0.2942943 , 0.2962963 , 0.2982983 ,\n",
       "        0.3003003 , 0.3023023 , 0.3043043 , 0.3063063 , 0.3083083 ,\n",
       "        0.3103103 , 0.3123123 , 0.3143143 , 0.3163163 , 0.3183183 ,\n",
       "        0.3203203 , 0.3223223 , 0.3243243 , 0.32632634, 0.32832834,\n",
       "        0.33033034, 0.33233234, 0.33433434, 0.33633634, 0.33833835,\n",
       "        0.34034035, 0.34234235, 0.34434435, 0.34634635, 0.34834835,\n",
       "        0.35035035, 0.35235235, 0.35435435, 0.35635635, 0.35835835,\n",
       "        0.36036035, 0.36236235, 0.36436436, 0.36636636, 0.36836836,\n",
       "        0.37037036, 0.37237236, 0.37437436, 0.3763764 , 0.3783784 ,\n",
       "        0.3803804 , 0.3823824 , 0.3843844 , 0.3863864 , 0.3883884 ,\n",
       "        0.3903904 , 0.3923924 , 0.3943944 , 0.3963964 , 0.3983984 ,\n",
       "        0.4004004 , 0.4024024 , 0.4044044 , 0.4064064 , 0.4084084 ,\n",
       "        0.4104104 , 0.4124124 , 0.4144144 , 0.4164164 , 0.4184184 ,\n",
       "        0.4204204 , 0.4224224 , 0.4244244 , 0.42642644, 0.42842844,\n",
       "        0.43043044, 0.43243244, 0.43443444, 0.43643644, 0.43843845,\n",
       "        0.44044045, 0.44244245, 0.44444445, 0.44644645, 0.44844845,\n",
       "        0.45045045, 0.45245245, 0.45445445, 0.45645645, 0.45845845,\n",
       "        0.46046045, 0.46246246, 0.46446446, 0.46646646, 0.46846846,\n",
       "        0.47047046, 0.47247246, 0.47447446, 0.4764765 , 0.4784785 ,\n",
       "        0.4804805 , 0.4824825 , 0.4844845 , 0.4864865 , 0.4884885 ,\n",
       "        0.4904905 , 0.4924925 , 0.4944945 , 0.4964965 , 0.4984985 ,\n",
       "        0.5005005 , 0.5025025 , 0.5045045 , 0.5065065 , 0.5085085 ,\n",
       "        0.5105105 , 0.5125125 , 0.5145145 , 0.5165165 , 0.5185185 ,\n",
       "        0.5205205 , 0.5225225 , 0.5245245 , 0.5265265 , 0.5285285 ,\n",
       "        0.5305305 , 0.5325325 , 0.5345345 , 0.5365365 , 0.5385385 ,\n",
       "        0.5405405 , 0.5425425 , 0.5445445 , 0.5465465 , 0.5485485 ,\n",
       "        0.5505506 , 0.5525526 , 0.5545546 , 0.5565566 , 0.5585586 ,\n",
       "        0.5605606 , 0.5625626 , 0.5645646 , 0.5665666 , 0.5685686 ,\n",
       "        0.5705706 , 0.5725726 , 0.5745746 , 0.5765766 , 0.5785786 ,\n",
       "        0.5805806 , 0.5825826 , 0.5845846 , 0.5865866 , 0.5885886 ,\n",
       "        0.5905906 , 0.5925926 , 0.5945946 , 0.5965966 , 0.5985986 ,\n",
       "        0.6006006 , 0.6026026 , 0.6046046 , 0.6066066 , 0.6086086 ,\n",
       "        0.6106106 , 0.6126126 , 0.6146146 , 0.6166166 , 0.6186186 ,\n",
       "        0.6206206 , 0.6226226 , 0.6246246 , 0.6266266 , 0.6286286 ,\n",
       "        0.6306306 , 0.6326326 , 0.6346346 , 0.6366366 , 0.6386386 ,\n",
       "        0.6406406 , 0.6426426 , 0.6446446 , 0.6466466 , 0.6486486 ,\n",
       "        0.6506507 , 0.6526527 , 0.6546547 , 0.6566567 , 0.6586587 ,\n",
       "        0.6606607 , 0.6626627 , 0.6646647 , 0.6666667 , 0.6686687 ,\n",
       "        0.6706707 , 0.6726727 , 0.6746747 , 0.6766767 , 0.6786787 ,\n",
       "        0.6806807 , 0.6826827 , 0.6846847 , 0.6866867 , 0.6886887 ,\n",
       "        0.6906907 , 0.6926927 , 0.6946947 , 0.6966967 , 0.6986987 ,\n",
       "        0.7007007 , 0.7027027 , 0.7047047 , 0.7067067 , 0.7087087 ,\n",
       "        0.7107107 , 0.7127127 , 0.7147147 , 0.7167167 , 0.7187187 ,\n",
       "        0.7207207 , 0.7227227 , 0.7247247 , 0.7267267 , 0.7287287 ,\n",
       "        0.7307307 , 0.7327327 , 0.7347347 , 0.7367367 , 0.7387387 ,\n",
       "        0.7407407 , 0.7427427 , 0.7447447 , 0.7467467 , 0.7487487 ,\n",
       "        0.7507508 , 0.7527528 , 0.7547548 , 0.7567568 , 0.7587588 ,\n",
       "        0.7607608 , 0.7627628 , 0.7647648 , 0.7667668 , 0.7687688 ,\n",
       "        0.7707708 , 0.7727728 , 0.7747748 , 0.7767768 , 0.7787788 ,\n",
       "        0.7807808 , 0.7827828 , 0.7847848 , 0.7867868 , 0.7887888 ,\n",
       "        0.7907908 , 0.7927928 , 0.7947948 , 0.7967968 , 0.7987988 ,\n",
       "        0.8008008 , 0.8028028 , 0.8048048 , 0.8068068 , 0.8088088 ,\n",
       "        0.8108108 , 0.8128128 , 0.8148148 , 0.8168168 , 0.8188188 ,\n",
       "        0.8208208 , 0.8228228 , 0.8248248 , 0.8268268 , 0.8288288 ,\n",
       "        0.8308308 , 0.8328328 , 0.8348348 , 0.8368368 , 0.8388388 ,\n",
       "        0.8408408 , 0.8428428 , 0.8448448 , 0.8468468 , 0.8488488 ,\n",
       "        0.8508509 , 0.8528529 , 0.8548549 , 0.8568569 , 0.8588589 ,\n",
       "        0.8608609 , 0.8628629 , 0.8648649 , 0.8668669 , 0.8688689 ,\n",
       "        0.8708709 , 0.8728729 , 0.8748749 , 0.8768769 , 0.8788789 ,\n",
       "        0.8808809 , 0.8828829 , 0.8848849 , 0.8868869 , 0.8888889 ,\n",
       "        0.8908909 , 0.8928929 , 0.8948949 , 0.8968969 , 0.8988989 ,\n",
       "        0.9009009 , 0.9029029 , 0.9049049 , 0.9069069 , 0.9089089 ,\n",
       "        0.9109109 , 0.9129129 , 0.9149149 , 0.9169169 , 0.9189189 ,\n",
       "        0.9209209 , 0.9229229 , 0.9249249 , 0.9269269 , 0.9289289 ,\n",
       "        0.9309309 , 0.9329329 , 0.9349349 , 0.9369369 , 0.9389389 ,\n",
       "        0.9409409 , 0.9429429 , 0.9449449 , 0.9469469 , 0.9489489 ,\n",
       "        0.950951  , 0.952953  , 0.954955  , 0.956957  , 0.958959  ,\n",
       "        0.960961  , 0.962963  , 0.964965  , 0.966967  , 0.968969  ,\n",
       "        0.970971  , 0.972973  , 0.974975  , 0.976977  , 0.978979  ,\n",
       "        0.980981  , 0.982983  , 0.984985  , 0.986987  , 0.988989  ,\n",
       "        0.990991  , 0.992993  , 0.994995  , 0.996997  , 0.998999  ,\n",
       "        1.001001  , 1.003003  , 1.005005  , 1.007007  , 1.009009  ,\n",
       "        1.011011  , 1.013013  , 1.015015  , 1.017017  , 1.019019  ,\n",
       "        1.021021  , 1.023023  , 1.025025  , 1.027027  , 1.029029  ,\n",
       "        1.031031  , 1.033033  , 1.035035  , 1.037037  , 1.039039  ,\n",
       "        1.041041  , 1.043043  , 1.045045  , 1.047047  , 1.049049  ,\n",
       "        1.051051  , 1.053053  , 1.055055  , 1.057057  , 1.059059  ,\n",
       "        1.061061  , 1.063063  , 1.065065  , 1.067067  , 1.069069  ,\n",
       "        1.071071  , 1.073073  , 1.075075  , 1.077077  , 1.079079  ,\n",
       "        1.081081  , 1.083083  , 1.085085  , 1.087087  , 1.089089  ,\n",
       "        1.091091  , 1.093093  , 1.095095  , 1.097097  , 1.099099  ,\n",
       "        1.1011012 , 1.1031032 , 1.1051052 , 1.1071072 , 1.1091092 ,\n",
       "        1.1111112 , 1.1131132 , 1.1151152 , 1.1171172 , 1.1191192 ,\n",
       "        1.1211212 , 1.1231232 , 1.1251252 , 1.1271272 , 1.1291292 ,\n",
       "        1.1311312 , 1.1331332 , 1.1351352 , 1.1371372 , 1.1391392 ,\n",
       "        1.1411412 , 1.1431432 , 1.1451452 , 1.1471472 , 1.1491492 ,\n",
       "        1.1511512 , 1.1531532 , 1.1551552 , 1.1571572 , 1.1591592 ,\n",
       "        1.1611612 , 1.1631632 , 1.1651652 , 1.1671672 , 1.1691692 ,\n",
       "        1.1711712 , 1.1731732 , 1.1751752 , 1.1771772 , 1.1791792 ,\n",
       "        1.1811812 , 1.1831832 , 1.1851852 , 1.1871872 , 1.1891892 ,\n",
       "        1.1911912 , 1.1931932 , 1.1951952 , 1.1971972 , 1.1991992 ,\n",
       "        1.2012012 , 1.2032032 , 1.2052052 , 1.2072072 , 1.2092092 ,\n",
       "        1.2112112 , 1.2132132 , 1.2152152 , 1.2172172 , 1.2192192 ,\n",
       "        1.2212212 , 1.2232232 , 1.2252252 , 1.2272272 , 1.2292292 ,\n",
       "        1.2312312 , 1.2332332 , 1.2352352 , 1.2372372 , 1.2392392 ,\n",
       "        1.2412412 , 1.2432432 , 1.2452452 , 1.2472472 , 1.2492492 ,\n",
       "        1.2512512 , 1.2532532 , 1.2552552 , 1.2572572 , 1.2592592 ,\n",
       "        1.2612612 , 1.2632632 , 1.2652652 , 1.2672672 , 1.2692692 ,\n",
       "        1.2712712 , 1.2732732 , 1.2752752 , 1.2772772 , 1.2792792 ,\n",
       "        1.2812812 , 1.2832832 , 1.2852852 , 1.2872872 , 1.2892892 ,\n",
       "        1.2912912 , 1.2932932 , 1.2952952 , 1.2972972 , 1.2992992 ,\n",
       "        1.3013014 , 1.3033034 , 1.3053054 , 1.3073074 , 1.3093094 ,\n",
       "        1.3113114 , 1.3133134 , 1.3153154 , 1.3173174 , 1.3193194 ,\n",
       "        1.3213214 , 1.3233234 , 1.3253254 , 1.3273274 , 1.3293294 ,\n",
       "        1.3313314 , 1.3333334 , 1.3353354 , 1.3373374 , 1.3393394 ,\n",
       "        1.3413414 , 1.3433434 , 1.3453454 , 1.3473474 , 1.3493494 ,\n",
       "        1.3513514 , 1.3533534 , 1.3553554 , 1.3573574 , 1.3593594 ,\n",
       "        1.3613614 , 1.3633634 , 1.3653654 , 1.3673674 , 1.3693694 ,\n",
       "        1.3713714 , 1.3733734 , 1.3753754 , 1.3773774 , 1.3793794 ,\n",
       "        1.3813814 , 1.3833834 , 1.3853854 , 1.3873874 , 1.3893894 ,\n",
       "        1.3913914 , 1.3933934 , 1.3953954 , 1.3973974 , 1.3993994 ,\n",
       "        1.4014014 , 1.4034034 , 1.4054054 , 1.4074074 , 1.4094094 ,\n",
       "        1.4114114 , 1.4134134 , 1.4154154 , 1.4174174 , 1.4194194 ,\n",
       "        1.4214214 , 1.4234234 , 1.4254254 , 1.4274274 , 1.4294294 ,\n",
       "        1.4314314 , 1.4334334 , 1.4354354 , 1.4374374 , 1.4394394 ,\n",
       "        1.4414414 , 1.4434434 , 1.4454454 , 1.4474474 , 1.4494494 ,\n",
       "        1.4514514 , 1.4534534 , 1.4554554 , 1.4574574 , 1.4594594 ,\n",
       "        1.4614614 , 1.4634634 , 1.4654654 , 1.4674674 , 1.4694694 ,\n",
       "        1.4714714 , 1.4734734 , 1.4754754 , 1.4774774 , 1.4794794 ,\n",
       "        1.4814814 , 1.4834834 , 1.4854854 , 1.4874874 , 1.4894894 ,\n",
       "        1.4914914 , 1.4934934 , 1.4954954 , 1.4974974 , 1.4994994 ,\n",
       "        1.5015016 , 1.5035036 , 1.5055056 , 1.5075076 , 1.5095096 ,\n",
       "        1.5115116 , 1.5135136 , 1.5155156 , 1.5175176 , 1.5195196 ,\n",
       "        1.5215216 , 1.5235236 , 1.5255256 , 1.5275276 , 1.5295296 ,\n",
       "        1.5315316 , 1.5335336 , 1.5355356 , 1.5375376 , 1.5395396 ,\n",
       "        1.5415416 , 1.5435436 , 1.5455456 , 1.5475476 , 1.5495496 ,\n",
       "        1.5515516 , 1.5535536 , 1.5555556 , 1.5575576 , 1.5595596 ,\n",
       "        1.5615616 , 1.5635636 , 1.5655656 , 1.5675676 , 1.5695696 ,\n",
       "        1.5715716 , 1.5735736 , 1.5755756 , 1.5775776 , 1.5795796 ,\n",
       "        1.5815816 , 1.5835836 , 1.5855856 , 1.5875876 , 1.5895896 ,\n",
       "        1.5915916 , 1.5935936 , 1.5955956 , 1.5975976 , 1.5995996 ,\n",
       "        1.6016016 , 1.6036036 , 1.6056056 , 1.6076076 , 1.6096096 ,\n",
       "        1.6116116 , 1.6136136 , 1.6156156 , 1.6176176 , 1.6196196 ,\n",
       "        1.6216216 , 1.6236236 , 1.6256256 , 1.6276276 , 1.6296296 ,\n",
       "        1.6316316 , 1.6336336 , 1.6356356 , 1.6376376 , 1.6396396 ,\n",
       "        1.6416416 , 1.6436436 , 1.6456456 , 1.6476476 , 1.6496496 ,\n",
       "        1.6516516 , 1.6536536 , 1.6556556 , 1.6576576 , 1.6596596 ,\n",
       "        1.6616616 , 1.6636636 , 1.6656656 , 1.6676676 , 1.6696696 ,\n",
       "        1.6716716 , 1.6736736 , 1.6756756 , 1.6776776 , 1.6796796 ,\n",
       "        1.6816816 , 1.6836836 , 1.6856856 , 1.6876876 , 1.6896896 ,\n",
       "        1.6916916 , 1.6936936 , 1.6956956 , 1.6976976 , 1.6996996 ,\n",
       "        1.7017018 , 1.7037038 , 1.7057058 , 1.7077078 , 1.7097098 ,\n",
       "        1.7117118 , 1.7137138 , 1.7157158 , 1.7177178 , 1.7197198 ,\n",
       "        1.7217218 , 1.7237238 , 1.7257258 , 1.7277278 , 1.7297298 ,\n",
       "        1.7317318 , 1.7337338 , 1.7357358 , 1.7377378 , 1.7397398 ,\n",
       "        1.7417418 , 1.7437438 , 1.7457458 , 1.7477478 , 1.7497498 ,\n",
       "        1.7517518 , 1.7537538 , 1.7557558 , 1.7577578 , 1.7597598 ,\n",
       "        1.7617618 , 1.7637638 , 1.7657658 , 1.7677678 , 1.7697698 ,\n",
       "        1.7717718 , 1.7737738 , 1.7757758 , 1.7777778 , 1.7797798 ,\n",
       "        1.7817818 , 1.7837838 , 1.7857858 , 1.7877878 , 1.7897898 ,\n",
       "        1.7917918 , 1.7937938 , 1.7957958 , 1.7977978 , 1.7997998 ,\n",
       "        1.8018018 , 1.8038038 , 1.8058058 , 1.8078078 , 1.8098098 ,\n",
       "        1.8118118 , 1.8138138 , 1.8158158 , 1.8178178 , 1.8198198 ,\n",
       "        1.8218218 , 1.8238238 , 1.8258258 , 1.8278278 , 1.8298298 ,\n",
       "        1.8318318 , 1.8338338 , 1.8358358 , 1.8378378 , 1.8398398 ,\n",
       "        1.8418418 , 1.8438438 , 1.8458458 , 1.8478478 , 1.8498498 ,\n",
       "        1.8518518 , 1.8538538 , 1.8558558 , 1.8578578 , 1.8598598 ,\n",
       "        1.8618618 , 1.8638638 , 1.8658658 , 1.8678678 , 1.8698698 ,\n",
       "        1.8718718 , 1.8738738 , 1.8758758 , 1.8778778 , 1.8798798 ,\n",
       "        1.8818818 , 1.8838838 , 1.8858858 , 1.8878878 , 1.8898898 ,\n",
       "        1.8918918 , 1.8938938 , 1.8958958 , 1.8978978 , 1.8998998 ,\n",
       "        1.901902  , 1.903904  , 1.905906  , 1.907908  , 1.90991   ,\n",
       "        1.911912  , 1.913914  , 1.915916  , 1.917918  , 1.91992   ,\n",
       "        1.921922  , 1.923924  , 1.925926  , 1.927928  , 1.92993   ,\n",
       "        1.931932  , 1.933934  , 1.935936  , 1.937938  , 1.93994   ,\n",
       "        1.941942  , 1.943944  , 1.945946  , 1.947948  , 1.94995   ,\n",
       "        1.951952  , 1.953954  , 1.955956  , 1.957958  , 1.95996   ,\n",
       "        1.961962  , 1.963964  , 1.965966  , 1.967968  , 1.96997   ,\n",
       "        1.971972  , 1.973974  , 1.975976  , 1.977978  , 1.97998   ,\n",
       "        1.981982  , 1.983984  , 1.985986  , 1.987988  , 1.98999   ,\n",
       "        1.991992  , 1.993994  , 1.995996  , 1.997998  , 2.        ],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=106270, shape=(), dtype=float32, numpy=0.002002002>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensorflow.python.framework.ops.EagerTensor, TensorShape([Dimension(1000)]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X), X.shape\n",
    "X*np.exp(-X**2) * X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_features(X):\n",
    "    features = [X]\n",
    "    features.append(tf.ones_like(X))  # Bias.\n",
    "    features.append()\n",
    "    # TODO: add new features.\n",
    "    return tf.stack(features, axis=1)\n",
    "\n",
    "def make_weights(n_weights):\n",
    "    W = [tf.constant(value = 0.0, dtype = tf.float32) for _ in range(n_weights)]\n",
    "    return tf.expand_dims(tf.stack(W),-1)\n",
    "\n",
    "def predict(X, W):\n",
    "    Y_hat = tf.matmul(# TODO)\n",
    "    return tf.squeeze(Y_hat, axis=-1)\n",
    "\n",
    "def loss_mse(X, Y, W):\n",
    "    Y_hat = predict(X, W)\n",
    "    return tf.reduce_mean(input_tensor = (Y_hat - Y)**2)\n",
    "\n",
    "X = tf.constant(value = np.linspace(0,2,1000), dtype = tf.float32)\n",
    "Y = np.exp(-X**2) * X\n",
    "\n",
    "grad_f = tf.contrib.eager.gradients_function(f = loss_mse, params=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "STEPS = 2000\n",
    "LEARNING_RATE = .02\n",
    "\n",
    "# Weights/features.\n",
    "Xf = make_features(X)\n",
    "# Xf = Xf[:,0:2]  # Linear features only.\n",
    "W = make_weights(Xf.get_shape()[1].value)\n",
    "\n",
    "# For plotting\n",
    "steps = []\n",
    "losses = []\n",
    "\n",
    "plt.figure()\n",
    "for step in range(STEPS):\n",
    "    #1. Calculate gradients\n",
    "    dW = grad_f(Xf, Y, W)[0]\n",
    "    #2. Update weights\n",
    "    W -= dW * LEARNING_RATE\n",
    "    #3. Periodically print MSE\n",
    "    if step % 100 == 0:\n",
    "        loss = loss_mse(Xf, Y, W)\n",
    "        steps.append(step)\n",
    "        losses.append(loss)\n",
    "        plt.clf()\n",
    "        plt.plot(steps, losses)\n",
    "# Print final MSE and weights\n",
    "print(\"STEP: {} MSE: {}\".format(STEPS,loss_mse(Xf, Y, W)))\n",
    "\n",
    "# Plot results\n",
    "plt.figure()\n",
    "plt.plot(X, Y, label='actual')\n",
    "plt.plot(X, predict(Xf, W), label='predicted')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
