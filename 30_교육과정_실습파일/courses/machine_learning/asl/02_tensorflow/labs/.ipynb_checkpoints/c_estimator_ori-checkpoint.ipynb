{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing tf.estimator\n",
    "\n",
    "**Learning Objectives**\n",
    "  - Understand where the `tf.estimator` module sits in the hierarchy of Tensorflow APIs\n",
    "  - Understand the workflow of creating a `tf.estimator` model\n",
    "    1. Create Feature Columns\n",
    "    2. Create Input Functions\n",
    "    3. Create Estimator\n",
    "    4. Train/Evaluate/Predict\n",
    "  - Understand how to swap in/out different types of Estimators\n",
    "  \n",
    "## Introduction \n",
    "Tensorflow is a hierarchical framework. The further down the hierarchy you go, the more flexibility you have, but that more code you have to write. Generally one starts at the highest level of abstraction. Then if you need additional flexibility drop down one layer.\n",
    "\n",
    "<img src='../assets/TFHierarchy.png' width='50%'>\n",
    "<sup>(image: https://www.tensorflow.org/guide/premade_estimators)</sup>\n",
    "\n",
    "In this notebook we will be operating at the highest level of Tensorflow abstraction, using the Estimator API to predict taxifare prices on the sampled dataset we created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data \n",
    "\n",
    "First let's download the raw .csv data. These are the same files created in the `create_datasets.ipynb` notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-training-demos/taxifare/small/taxi-test.csv...\n",
      "Copying gs://cloud-training-demos/taxifare/small/taxi-train.csv...              \n",
      "Copying gs://cloud-training-demos/taxifare/small/taxi-valid.csv...              \n",
      "/ [3 files][ 10.9 MiB/ 10.9 MiB]                                                \n",
      "Operation completed over 3 objects/10.9 MiB.                                     \n",
      "-rw-r--r-- 1 jupyter jupyter 1799474 Nov 21 03:12 taxi-test.csv\n",
      "-rw-r--r-- 1 jupyter jupyter 7986353 Nov 21 03:12 taxi-train.csv\n",
      "-rw-r--r-- 1 jupyter jupyter 1673742 Nov 21 03:12 taxi-valid.csv\n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://cloud-training-demos/taxifare/small/*.csv .\n",
    "!ls -l *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the files are small we can load them into in-memory Pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fare_amount', 'dayofweek', 'hourofday', 'pickuplon', 'pickuplat', 'dropofflon', 'dropofflat']\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(filepath_or_buffer = \"./taxi-train.csv\")\n",
    "df_valid = pd.read_csv(filepath_or_buffer = \"./taxi-valid.csv\")\n",
    "df_test = pd.read_csv(filepath_or_buffer = \"./taxi-test.csv\")\n",
    "\n",
    "CSV_COLUMN_NAMES = list(df_train)\n",
    "print(CSV_COLUMN_NAMES)\n",
    "\n",
    "FEATURE_NAMES = CSV_COLUMN_NAMES[1:] # all but first column\n",
    "LABEL_NAME = CSV_COLUMN_NAMES[0] # first column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feature columns\n",
    "\n",
    "Feature columns make it easy to perform common type of feature engineering on your raw data. For example you can one-hot encode categorical data, create feature crosses, embeddings and more. We'll cover these later in the course, but if you want to a sneak peak browse the official TensorFlow [feature columns guide](https://www.tensorflow.org/guide/feature_columns).\n",
    "\n",
    "In our case we won't do any feature engineering. However we still need to create a list of feature columns because the Estimator we will use requires one. To specify the numeric values should be passed on without modification we use `tf.feature_column.numeric_column()`\n",
    "\n",
    "#### **Exercise 1**\n",
    "\n",
    "Use a [python list comprehension](https://www.pythonforbeginners.com/basics/list-comprehensions-in-python) or a `for` loop to create the feature columns for all features in `FEATURE_NAMES`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    tf.feature_column.categorical_column_with_vocabulary_list(\"dayofweek\",[1,2,3,4,5,6,7]),\n",
    "    tf.feature_column.categorical_column_with_vocabulary_list(\"hourofday\",range(23)),\n",
    "    tf.feature_column.numeric_column(\"pickuplon\"),\n",
    "    tf.feature_column.numeric_column(\"pickuplat\"),\n",
    "    tf.feature_column.numeric_column(\"dropofflon\"),\n",
    "    tf.feature_column.numeric_column(\"dropofflat\")\n",
    "]\n",
    "feature_columns = [tf.feature_column.numeric_column(head) for head in FEATURE_NAMES]\n",
    "# TODO: Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NumericColumn(key='dayofweek', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='hourofday', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='pickuplon', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='pickuplat', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='dropofflon', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='dropofflat', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fare_amount',\n",
       " 'dayofweek',\n",
       " 'hourofday',\n",
       " 'pickuplon',\n",
       " 'pickuplat',\n",
       " 'dropofflon',\n",
       " 'dropofflat']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "df_train_c = df_train.copy()\n",
    "aa = df_train_c.hourofday.unique()\n",
    "np.sort(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define input function\n",
    "\n",
    "Now that your estimator knows what type of data to expect and how to intepret it, you need to actually pass the data to it! This is the job of the input function.\n",
    "\n",
    "The input function returns a new batch of (features, label) tuples each time it is called by the Estimator.\n",
    "\n",
    "- features: A python dictionary. Each key is a feature column name and its value is the tensor containing the data for that feature\n",
    "- label: A Tensor containing the labels\n",
    "\n",
    "So how do we get from our current Pandas dataframes to (features, label) tuples that return one batch at a time?\n",
    "\n",
    "The `tf.data` module contains a collection of classes that allows you to easily load data, manipulate it, and pipe it into your model. https://www.tensorflow.org/guide/datasets_for_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 2**\n",
    "\n",
    "The code cell below has a few TODOs for you to complete. \n",
    "\n",
    "The first TODO in the `train_input_fn` asks you to create a tf.dataset using the [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) API for input pipelines. Complete the code so that the variable `dataset` creates a tf.data.Dataset element using the [tf.from_tensor_slices method](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices). The argument `tensors` should be a tuple of a dict of the features and the label taken from the Pandas dataframe. \n",
    "\n",
    "The second TODO in the `train_input_fn` asks you to add a shuffle, repeat and batch operation to the dataset object you created above. Have a look at [the usage of these methods in the tf.data.Datasets API](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#methods)\n",
    "\n",
    "The next TODO is in the `eval_input_fn`. Here you are asked to create a dataset object for the validation data. It should look similar to the pipeline you created for the `train_input_fn`. Note that for the `eval_input_fn` we don't add a shuffle or repeat step as we'll just evaluation a given batch during each validation step.\n",
    "\n",
    "The last TODO is in the `predict_input_fn` where you are asked to once again use the Tensorflow Dataset API to set up a dataset for the prediction stage using the same `from_tensor_slices` as before. Note, during `PREDICT` we don't have the label, only features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn(df, batch_size = 128):\n",
    "    #1. Convert dataframe into correct (features, label) format for Estimator API\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(          (dict(df[FEATURE_NAMES]), df[LABEL_NAME]))  # TODO: Your code goes here\n",
    "#    dataset = tf.data.Dataset.from_tensor_slices(tensors = (dict(df[FEATURE_NAMES]), df[LABEL_NAME]))\n",
    "    \n",
    "    # Note:\n",
    "    # If we returned now, the Dataset would iterate over the data once  \n",
    "    # in a fixed order, and only produce a single element at a time.\n",
    "    \n",
    "    #2. Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(buffer_size = 1000).batch(batch_size=128).repeat(count=1)\n",
    "   \n",
    "    return dataset\n",
    "\n",
    "def eval_input_fn(df, batch_size = 128):\n",
    "    #1. Convert dataframe into correct (features, label) format for Estimator API\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(          (dict(df[FEATURE_NAMES]), df[LABEL_NAME]))  # TODO: Your code goes here\n",
    "    \n",
    "    #2.Batch the examples.\n",
    "    dataset = dataset.batch(batch_size = batch_size)\n",
    "   \n",
    "    return dataset\n",
    "\n",
    "def predict_input_fn(df, batch_size = 128):\n",
    "    #1. Convert dataframe into correct (features) format for Estimator API\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(tensors = dict(df[FEATURE_NAMES]))  # TODO: Your code goes here\n",
    "\n",
    "    #2.Batch the examples.\n",
    "    dataset = dataset.batch(batch_size = batch_size)\n",
    "   \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Estimator\n",
    "\n",
    "Tensorflow has several premade estimators for you to choose from:\n",
    "\n",
    "- LinearClassifier/Regressor\n",
    "- BoostedTreesClassifier/Regressor\n",
    "- DNNClassifier/Regressor\n",
    "- DNNLinearCombinedClassifier/Regressor\n",
    "\n",
    "If none of these meet your needs you can implement a custom estimator using `tf.Keras`. We'll cover that later in the course.\n",
    "\n",
    "For now we will use the premade LinearRegressor. To instantiate an estimator simply pass it what feature columns to expect and specify an directory for it to output checkpoint files to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 3**\n",
    "\n",
    "Comlete the code in the cell below to define a Linear Regression model using the TF Estimator API. Have a [look at the documentation](https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor#__init__) to see what variables you must pass to initialize a `LinearRegressor` instance. You'll want to add values for `feature_columns`, `model_dir` and `config`. When setting up `config`, have a look at the [documentation for tf.estimator.RunConfig](https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig) and be sure to set `tf.random_seed` to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpcok2w0x3\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_save_summary_steps': 100, '_service': None, '_model_dir': '/tmp/tmpcok2w0x3', '_experimental_distribute': None, '_master': '', '_num_worker_replicas': 1, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f54db4344a8>, '_evaluation_master': '', '_tf_random_seed': None, '_is_chief': True, '_log_step_count_steps': 100, '_experimental_max_worker_delay_secs': None, '_task_type': 'worker', '_train_distribute': None, '_task_id': 0, '_protocol': None, '_global_id_in_cluster': 0, '_keep_checkpoint_max': 5, '_session_creation_timeout_secs': 7200, '_keep_checkpoint_every_n_hours': 10000, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_eval_distribute': None, '_save_checkpoints_steps': None, '_device_fn': None, '_num_ps_replicas': 0}\n"
     ]
    }
   ],
   "source": [
    "OUTDIR = \"taxi_trained\"\n",
    "\n",
    "model = tf.estimator.LinearRegressor(\n",
    "feature_columns=feature_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Simply invoke the estimator's `train()` function. Specify the `input_fn` which tells it how to load in data, and specify the number of steps to train for.\n",
    "\n",
    "By default estimators check the output directory for checkpoint files before beginning training, so it can pickup where it last left off. To prevent this we'll delete the output directory before starting training each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/feature_column/feature_column_v2.py:305: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpcok2w0x3/model.ckpt.\n",
      "INFO:tensorflow:loss = 38158.773, step = 0\n",
      "INFO:tensorflow:global_step/sec: 239.768\n",
      "INFO:tensorflow:loss = 9008.448, step = 100 (0.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 299.846\n",
      "INFO:tensorflow:loss = 10395.254, step = 200 (0.334 sec)\n",
      "INFO:tensorflow:global_step/sec: 292.17\n",
      "INFO:tensorflow:loss = 13044.207, step = 300 (0.343 sec)\n",
      "INFO:tensorflow:global_step/sec: 296.515\n",
      "INFO:tensorflow:loss = 12192.686, step = 400 (0.338 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 500 into /tmp/tmpcok2w0x3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 5994.822.\n",
      "CPU times: user 6.97 s, sys: 788 ms, total: 7.76 s\n",
      "Wall time: 5.32 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.linear.LinearRegressor at 0x7f54db434780>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tf.logging.set_verbosity(tf.logging.INFO) # so loss is printed during training\n",
    "shutil.rmtree(path = OUTDIR, ignore_errors = True) # start fresh each time\n",
    "\n",
    "model.train(\n",
    "    input_fn = lambda: train_input_fn(df = df_train), \n",
    "    steps = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "Estimators similarly have an `evaluate()` function. In this case we don't need to specify the number of steps to train because we didn't tell our input function to repeat the data. Once the input function reaches the end of the data evaluation will end. \n",
    "\n",
    "Loss is reported as MSE by default so we take the square root before printing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 4**\n",
    "\n",
    "Complete the code in the cell below to run evaluation on the model you just trained. You'll use the `evaluate` method of the `LinearRegressor` model you created and trained above. Have a look at [the documentation of the evaluate method](https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor#evaluate) here to see what it expects. Note you'll need to pass the evaluation input function as a lambda function processing the Pandas dataframe `df_valid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-11-21T03:12:42Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpcok2w0x3/model.ckpt-500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-11-21-03:12:43\n",
      "INFO:tensorflow:Saving dict for global step 500: average_loss = 91.64192, global_step = 500, label/mean = 11.229713, loss = 11711.028, prediction/mean = 8.747708\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 500: /tmp/tmpcok2w0x3/model.ckpt-500\n",
      "RMSE on dataset = 9.57297874211942\n"
     ]
    }
   ],
   "source": [
    "def print_rmse(model, df):\n",
    "    metrics = model.evaluate(\n",
    "        #input_fn=eval_input_fn(df)\n",
    "        input_fn = lambda: eval_input_fn(df)\n",
    "    )\n",
    "    print(\"RMSE on dataset = {}\".format(metrics[\"average_loss\"]**.5))\n",
    "print_rmse(model = model, df = df_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE of 9.43 is  worse than our rules based benchmark (RMSE of $7.70). However given that we haven't done any feature engineering or hyperparameter tuning, and we're training on a small dataset using a simple linear model, we shouldn't yet expect good performance. \n",
    "\n",
    "The goal at this point is to demonstrate the mechanics of the Estimator API. In subsequent notebooks we'll improve on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "To run prediction on the test set `df_test` we use the `predict_input_fn` you created above, passsing the `df_test` dataframe for prediction. We'll use our model to make predicitons on the first 10 elements of the `df_test` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpcok2w0x3/model.ckpt-500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "{'predictions': array([7.8707433], dtype=float32)}\n",
      "{'predictions': array([8.009936], dtype=float32)}\n",
      "{'predictions': array([7.918727], dtype=float32)}\n",
      "{'predictions': array([7.8709655], dtype=float32)}\n",
      "{'predictions': array([7.871076], dtype=float32)}\n",
      "{'predictions': array([8.148855], dtype=float32)}\n",
      "{'predictions': array([8.148524], dtype=float32)}\n",
      "{'predictions': array([8.006475], dtype=float32)}\n",
      "{'predictions': array([7.869037], dtype=float32)}\n",
      "{'predictions': array([8.147127], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(input_fn = lambda: predict_input_fn(df = df_test[:10]))\n",
    "for items in predictions:\n",
    "    print(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further evidence of the primitiveness of our model, it predicts almost the same amount for every trip!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Estimator type\n",
    "\n",
    "One of the payoffs for using the Estimator API is we can swap in a different model type with just a few lines of code. Let's try a DNN. Note how now we need to specify the number of neurons in each hidden layer. Have a look at [the documentation for the DNN Regressor](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor) to see what other variables you can set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_save_summary_steps': 100, '_service': None, '_model_dir': 'taxi_trained', '_experimental_distribute': None, '_master': '', '_num_worker_replicas': 1, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5570d0d0f0>, '_evaluation_master': '', '_tf_random_seed': 1, '_is_chief': True, '_log_step_count_steps': 100, '_experimental_max_worker_delay_secs': None, '_task_type': 'worker', '_train_distribute': None, '_task_id': 0, '_protocol': None, '_global_id_in_cluster': 0, '_keep_checkpoint_max': 5, '_session_creation_timeout_secs': 7200, '_keep_checkpoint_every_n_hours': 10000, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_eval_distribute': None, '_save_checkpoints_steps': None, '_device_fn': None, '_num_ps_replicas': 0}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 881560.75, step = 0\n",
      "INFO:tensorflow:global_step/sec: 247.17\n",
      "INFO:tensorflow:loss = 6486.4023, step = 100 (0.408 sec)\n",
      "INFO:tensorflow:global_step/sec: 291.367\n",
      "INFO:tensorflow:loss = 5954.363, step = 200 (0.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 295.082\n",
      "INFO:tensorflow:loss = 5927.697, step = 300 (0.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 286.486\n",
      "INFO:tensorflow:loss = 10361.783, step = 400 (0.348 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 500 into taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 6151.0137.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-11-21T03:12:48Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-11-21-03:12:49\n",
      "INFO:tensorflow:Saving dict for global step 500: average_loss = 85.51908, global_step = 500, label/mean = 11.229713, loss = 10928.583, prediction/mean = 10.971489\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 500: taxi_trained/model.ckpt-500\n",
      "RMSE on dataset = 9.247652735463344\n",
      "CPU times: user 8.8 s, sys: 800 ms, total: 9.6 s\n",
      "Wall time: 6.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "shutil.rmtree(path = OUTDIR, ignore_errors = True)\n",
    "\n",
    "model = tf.estimator.DNNRegressor(\n",
    "    hidden_units = [10,10], # specify neural architecture\n",
    "    feature_columns = feature_columns, \n",
    "    model_dir = OUTDIR,\n",
    "    config = tf.estimator.RunConfig(tf_random_seed = 1)\n",
    ")\n",
    "model.train(\n",
    "    input_fn = lambda: train_input_fn(df = df_train), \n",
    "    steps = 500)\n",
    "print_rmse(model = model, df = df_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our performance is only slightly better at 9.26, and still far worse than our rules based model.  This illustrates an important tenant of machine learning: A more complex model can't outrun bad data. \n",
    "\n",
    "Currently since we're not doing any feature engineering our input data has very little signal to learn from, so using a DNN doesn't help much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results summary\n",
    "\n",
    "We can summarize our results in a table here.\n",
    "\n",
    "#### **Exercise 5** \n",
    "\n",
    "Insert the results you found for the `LinearRegressor` and `DNNRegressor` model performance here.\n",
    "   \n",
    "|Model | RMSE on validation set|\n",
    "|------|-----------------|\n",
    "|Rules Based Benchmark| 7.76|\n",
    "|Linear Model| 9.380533035196564 |\n",
    "|DNN Model|9.247652735463344 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge exercise\n",
    "\n",
    "Create a neural network that is capable of finding the volume of a cylinder given the radius of its base (r) and its height (h). Assume that the radius and height of the cylinder are both in the range 0.5 to 2.0. Simulate the necessary training dataset.\n",
    "<p>\n",
    "Hint (highlight to see):\n",
    "<p style='color:white'>\n",
    "The input features will be r and h and the label will be $\\pi r^2 h$\n",
    "Create random values for r and h and compute V.\n",
    "Your dataset will consist of r, h and V.\n",
    "Then, use a DNN regressor.\n",
    "Make sure to generate enough data.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(100)*1.5 + 0.5\n",
    "Y = 3.141592 * (X **2) * X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_a = pd.DataFrame({'radius': X, 'height': X, 'Volume':Y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Volume</th>\n",
       "      <td>100.0</td>\n",
       "      <td>8.208837</td>\n",
       "      <td>7.222506</td>\n",
       "      <td>0.401446</td>\n",
       "      <td>2.170539</td>\n",
       "      <td>4.929509</td>\n",
       "      <td>12.732549</td>\n",
       "      <td>24.193290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <td>100.0</td>\n",
       "      <td>1.239313</td>\n",
       "      <td>0.437225</td>\n",
       "      <td>0.503685</td>\n",
       "      <td>0.884017</td>\n",
       "      <td>1.161977</td>\n",
       "      <td>1.594366</td>\n",
       "      <td>1.974763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius</th>\n",
       "      <td>100.0</td>\n",
       "      <td>1.239313</td>\n",
       "      <td>0.437225</td>\n",
       "      <td>0.503685</td>\n",
       "      <td>0.884017</td>\n",
       "      <td>1.161977</td>\n",
       "      <td>1.594366</td>\n",
       "      <td>1.974763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count      mean       std       min       25%       50%        75%  \\\n",
       "Volume  100.0  8.208837  7.222506  0.401446  2.170539  4.929509  12.732549   \n",
       "height  100.0  1.239313  0.437225  0.503685  0.884017  1.161977   1.594366   \n",
       "radius  100.0  1.239313  0.437225  0.503685  0.884017  1.161977   1.594366   \n",
       "\n",
       "              max  \n",
       "Volume  24.193290  \n",
       "height   1.974763  \n",
       "radius   1.974763  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_a.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Volume', 'height', 'radius']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[NumericColumn(key='height', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='radius', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSV_COLUMN_NAMES = list(df_train_a)\n",
    "print(CSV_COLUMN_NAMES)\n",
    "FEATURE_NAMES = CSV_COLUMN_NAMES[1:] # all but first column\n",
    "LABEL_NAME = CSV_COLUMN_NAMES[0] # first column\n",
    "\n",
    "feature_columns = [tf.feature_column.numeric_column(key = k) for k in FEATURE_NAMES]\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpwfxj96cj\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_save_summary_steps': 100, '_service': None, '_model_dir': '/tmp/tmpwfxj96cj', '_experimental_distribute': None, '_master': '', '_num_worker_replicas': 1, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5570d07e10>, '_evaluation_master': '', '_tf_random_seed': None, '_is_chief': True, '_log_step_count_steps': 100, '_experimental_max_worker_delay_secs': None, '_task_type': 'worker', '_train_distribute': None, '_task_id': 0, '_protocol': None, '_global_id_in_cluster': 0, '_keep_checkpoint_max': 5, '_session_creation_timeout_secs': 7200, '_keep_checkpoint_every_n_hours': 10000, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_eval_distribute': None, '_save_checkpoints_steps': None, '_device_fn': None, '_num_ps_replicas': 0}\n"
     ]
    }
   ],
   "source": [
    "myopt = tf.train.AdamOptimizer(learning_rate = 0.01)\n",
    "model = tf.estimator.DNNRegressor(\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=[16, 8, 2],\n",
    "    activation_fn=tf.nn.relu,\n",
    "    optimizer = myopt,\n",
    "    dropout = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpwfxj96cj/model.ckpt-1\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpwfxj96cj/model.ckpt.\n",
      "INFO:tensorflow:loss = 12009.251, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 2 into /tmp/tmpwfxj96cj/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 12009.251.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-11-21T03:13:16Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpwfxj96cj/model.ckpt-2\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-11-21-03:13:17\n",
      "INFO:tensorflow:Saving dict for global step 2: average_loss = 118.70002, global_step = 2, label/mean = 8.2088375, loss = 11870.002, prediction/mean = 0.019997854\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2: /tmp/tmpwfxj96cj/model.ckpt-2\n",
      "RMSE on dataset = 10.89495387032115\n"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    input_fn = lambda: train_input_fn(df = df_train_a), \n",
    "    steps = 500)\n",
    "print_rmse(model = model, df = df_train_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
