{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with TensorFlow (Eager Mode)\n",
    "\n",
    "**Learning Objectives**\n",
    "  - Understand difference between Tensorflow's two modes: Eager Execution and Graph Execution\n",
    "  - Practice defining and performing basic operations on constant Tensors\n",
    "  - Use Tensorflow's automatic differentiation capability\n",
    "\n",
    "## Introduction\n",
    "**Eager Execution**\n",
    "\n",
    "Eager mode evaluates operations immediatley and return concrete values immediately. To enable eager mode simply place `tf.enable_eager_execution()` at the top of your code. We recommend using eager execution when prototyping as it is intuitive, easier to debug, and requires less boilerplate code.\n",
    "\n",
    "**Graph Execution**\n",
    "\n",
    "Graph mode is TensorFlow's default execution mode (although it will change to eager with TF 2.0). In graph mode operations only produce a symbolic graph which doesn't get executed until run within the context of a tf.Session(). This style of coding is less inutitive and has more boilerplate, however it can lead to performance optimizations and is particularly suited for distributing training across multiple devices. We recommend using delayed execution for performance sensitive production code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Two Tensors \n",
    "\n",
    "The value of the tensor, as well as its shape and data type are printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 8  2 10], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(value = [5, 3, 8], dtype = tf.int32)\n",
    "b = tf.constant(value = [3, -1, 2], dtype = tf.int32)\n",
    "c = tf.add(x = a, y = b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overloaded Operators\n",
    "We can also perform a `tf.add()` using the `+` operator. The `/,-,*` and `**` operators are similarly overloaded with the appropriate tensorflow operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 8  2 10], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "c = a + b # this is equivalent to tf.add(a,b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy Interoperability\n",
    "\n",
    "In addition to native TF tensors, tensorflow operations can take native python types and NumPy arrays as operands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'tensorflow.python.framework.ops.EagerTensor'>, Value: [4 6]\n",
      "Type: <class 'tensorflow.python.framework.ops.EagerTensor'>, Value: [4 6]\n",
      "Type: <class 'tensorflow.python.framework.ops.EagerTensor'>, Value: [4 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "a_py = [1,2] # native python list\n",
    "b_py = [3,4] # native python list\n",
    "\n",
    "a_np = np.array(object = [1,2]) # numpy array\n",
    "b_np = np.array(object = [3,4]) # numpy array\n",
    "\n",
    "a_tf = tf.constant(value = [1,2], dtype = tf.int32) # native TF tensor\n",
    "b_tf = tf.constant(value = [3,4], dtype = tf.int32) # native TF tensor\n",
    "\n",
    "for result in [tf.add(x = a_py, y = b_py), tf.add(x = a_np, y = b_np), tf.add(x = a_tf, y = b_tf)]:\n",
    "    print(\"Type: {}, Value: {}\".format(type(result), result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can convert a native TF tensor to a NumPy array using .numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tf.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=0, shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "Now let's use low level tensorflow operations to implement linear regression.\n",
    "\n",
    "Later in the course you'll see abstracted ways to do this using high level TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toy Dataset\n",
    "\n",
    "We'll model the following function:\n",
    "\n",
    "\\begin{equation}\n",
    "y= 2x + 10\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      "Y:[12. 14. 16. 18. 20. 22. 24. 26. 28. 30.]\n"
     ]
    }
   ],
   "source": [
    "X = tf.constant(value = [1,2,3,4,5,6,7,8,9,10], dtype = tf.float32)\n",
    "Y = 2 * X + 10\n",
    "print(\"X:{}\".format(X))\n",
    "print(\"Y:{}\".format(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function\n",
    "\n",
    "Using mean squared error, our loss function is:\n",
    "\\begin{equation}\n",
    "MSE = \\frac{1}{m}\\sum_{i=1}^{m}(\\hat{Y}_i-Y_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "$\\hat{Y}$ represents the vector containing our model's predictions:\n",
    "\\begin{equation}\n",
    "\\hat{Y} = w_oX + w_1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 1**\n",
    "\n",
    "The function `loss_mse` below takes four arguments: the tensors $X$, $Y$ and the weights $w_0$ and $w_1$. Complete the function below to compute the Mean Square Error (MSE). Hint: [check out](https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean) the `tf.reduce_mean` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 잘못함.\n",
    "def loss_mse(X, Y, w0, w1):\n",
    "    # TODO: Your code goes here\n",
    "    mse = 0.0\n",
    "    print(type(mse))\n",
    "    for i in range(X.shape[0]):\n",
    "        mse = mse + ((w0*X[i]+w1)-(Y[i]))*((w0*X[i]+w1)-(Y[i]))\n",
    "    mse = mse / X.shape[0].astype(float32)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mse(X, Y, w0, w1):\n",
    "    # TODO: Your code goes here\n",
    "    Y_hat = w0 * X + w1\n",
    "    return tf.reduce_mean((Y_hat - Y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Function\n",
    "\n",
    "To use gradient descent we need to take the partial derivative of the loss function with respect to each of the weights. We could manually compute the derivatives, but with Tensorflow's automatic differentiation capabilities we don't have to!\n",
    "\n",
    "During gradient descent we think of the loss as a function of the parameters $w_0$ and $w_1$. Thus, we want to compute the partial derivative with respect to these variables. The `params=[2,3]` argument tells TensorFlow to only compute derivatives with respect to the 2nd and 3rd arguments to the loss function (counting from 0, so really the 3rd and 4th)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting from 0, the 2nd and 3rd parameter to the loss function are our weights\n",
    "grad_f = tf.contrib.eager.gradients_function(f = loss_mse, params = [2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop\n",
    "\n",
    "Here we have a very simple training loop that converges. Note we are ignoring best practices like batching, creating a separate test set, and random weight initialization for the sake of simplicity.\n",
    "\n",
    "#### **Exercise 2**\n",
    "\n",
    "Complete the code to update the parameters $w_0$ and $w_1$ according to the gradients `d_w0` and `d_w1` and the specified learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 0 MSE: 167.6111602783203\n",
      "STEP: 100 MSE: 3.5321757793426514\n",
      "STEP: 200 MSE: 0.6537718176841736\n",
      "STEP: 300 MSE: 0.12100745737552643\n",
      "STEP: 400 MSE: 0.022397063672542572\n",
      "STEP: 500 MSE: 0.004145539831370115\n",
      "STEP: 600 MSE: 0.0007674093940295279\n",
      "STEP: 700 MSE: 0.00014202017337083817\n",
      "STEP: 800 MSE: 2.628635775181465e-05\n",
      "STEP: 900 MSE: 4.86889211970265e-06\n",
      "STEP: 1000 MSE: 9.178326081382693e-07\n",
      "w0:2.0003\n",
      "w1:9.9979\n"
     ]
    }
   ],
   "source": [
    "STEPS = 1000\n",
    "LEARNING_RATE = .02\n",
    "\n",
    "# Initialize weights\n",
    "w0 = tf.constant(value = 0.0, dtype = tf.float32)\n",
    "w1 = tf.constant(value = 0.0, dtype = tf.float32)\n",
    "\n",
    "for step in range(STEPS):\n",
    "    #1. Calculate gradients\n",
    "    d_w0, d_w1 = grad_f(X, Y, w0, w1) # derivatives calculated by tensorflow!\n",
    "\n",
    "    #2. Update weights\n",
    "    w0 = w0 + LEARNING_RATE * d_w0 * -1\n",
    "    w1 = w1 + LEARNING_RATE * d_w1 * -1\n",
    "\n",
    "    #3. Periodically print MSE\n",
    "    if step % 100 == 0:\n",
    "        print(\"STEP: {} MSE: {}\".format(step,loss_mse(X, Y, w0, w1)))\n",
    "\n",
    "# Print final MSE and weights\n",
    "print(\"STEP: {} MSE: {}\".format(STEPS,loss_mse(X, Y, w0, w1)))\n",
    "print(\"w0:{}\".format(round(float(w0), 4)))\n",
    "print(\"w1:{}\".format(round(float(w1), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try modelling a non-linear function such as: $y=xe^{-x^2}$\n",
    "\n",
    "Hint: Creating more training data will help. Also, you will need to build non-linear features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff2ad56e5f8>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X = tf.constant(value = np.linspace(0,2,1000), dtype = tf.float32)\n",
    "Y = X*np.exp(-X**2) * X\n",
    "plt.plot(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(1000)])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(X):\n",
    "    features = [X]\n",
    "    features.append(tf.ones_like(X))  # Bias.\n",
    "    # TODO: add new features.\n",
    "    return tf.stack(features, axis=1)\n",
    "\n",
    "def make_weights(n_weights):\n",
    "    W = [tf.constant(value = 0.0, dtype = tf.float32) for _ in range(n_weights)]\n",
    "    return tf.expand_dims(tf.stack(W),-1)\n",
    "\n",
    "def predict(X, W):\n",
    "    Y_hat = tf.matmul(X, W)\n",
    "    return tf.squeeze(Y_hat, axis=-1)\n",
    "\n",
    "def loss_mse(X, Y, W):\n",
    "    Y_hat = predict(X, W)\n",
    "    return tf.reduce_mean(input_tensor = (Y_hat - Y)**2)\n",
    "\n",
    "X = tf.constant(value = np.linspace(0,2,1000), dtype = tf.float32)\n",
    "Y = np.exp(-X**2) * X\n",
    "\n",
    "grad_f = tf.contrib.eager.gradients_function(f = loss_mse, params=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Blas GEMV launch failed:  m=2, n=1000 [Op:MatMul] name: MatMul/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-1fe1574d4270>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#1. Calculate gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;31m#2. Update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mW\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mdW\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;34m\"\"\"Computes the gradient of the decorated function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_and_grad_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    466\u001b[0m       raise ValueError(\"Functions to be differentiated cannot \"\n\u001b[1;32m    467\u001b[0m                        \"receive keyword arguments.\")\n\u001b[0;32m--> 468\u001b[0;31m     \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvjp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_vjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0msources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_tape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         raise ValueError(\"Cannot differentiate a function that returns None; \"\n",
      "\u001b[0;32m<ipython-input-48-219e4967d434>\u001b[0m in \u001b[0;36mloss_mse\u001b[0;34m(X, Y, W)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mY_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mY_hat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-219e4967d434>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(X, W)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mY_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   2752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2753\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[0;32m-> 2754\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   2755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   6124\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6125\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6126\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6127\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6128\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtranspose_a\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Blas GEMV launch failed:  m=2, n=1000 [Op:MatMul] name: MatMul/"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "STEPS = 2000\n",
    "LEARNING_RATE = .02\n",
    "\n",
    "# Weights/features.\n",
    "Xf = make_features(X)\n",
    "# Xf = Xf[:,0:2]  # Linear features only.\n",
    "W = make_weights(Xf.get_shape()[1].value)\n",
    "\n",
    "# For plotting\n",
    "steps = []\n",
    "losses = []\n",
    "\n",
    "plt.figure()\n",
    "for step in range(STEPS):\n",
    "    #1. Calculate gradients\n",
    "    dW = grad_f(Xf, Y, W)[0]\n",
    "    #2. Update weights\n",
    "    W -= dW * LEARNING_RATE\n",
    "    #3. Periodically print MSE\n",
    "    if step % 100 == 0:\n",
    "        loss = loss_mse(Xf, Y, W)\n",
    "        steps.append(step)\n",
    "        losses.append(loss)\n",
    "        plt.clf()\n",
    "        plt.plot(steps, losses)\n",
    "# Print final MSE and weights\n",
    "print(\"STEP: {} MSE: {}\".format(STEPS,loss_mse(Xf, Y, W)))\n",
    "\n",
    "# Plot results\n",
    "plt.figure()\n",
    "plt.plot(X, Y, label='actual')\n",
    "plt.plot(X, predict(Xf, W), label='predicted')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
