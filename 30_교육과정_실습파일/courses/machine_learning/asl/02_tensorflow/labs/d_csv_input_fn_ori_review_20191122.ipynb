{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading large datasets\n",
    "\n",
    "**Learning Objectives**\n",
    "  - Understand difference between loading data entirely in-memory and loading in batches from disk\n",
    "  - Practice loading a `.csv` file from disk in batches using the `tf.data` module\n",
    " \n",
    "## Introduction\n",
    "\n",
    "In the previous notebook, we read the the whole taxifare .csv files into memory, specifically a Pandas dataframe, before invoking `tf.data.from_tensor_slices` from the tf.data API. We could get away with this because it was a small sample of the dataset, but on the full taxifare dataset this wouldn't be feasible.\n",
    "\n",
    "In this notebook we demonstrate how to read .csv files directly from disk, one batch at a time, using `tf.data.TextLineDataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell and restart the kernel if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전 노트북에서는 tf.data.from_tensor_slicestf.data API에서 호출하기 전에 전체 택시 요금 .csv 파일을 메모리, 특히 Pandas 데이터 프레임으로 읽습니다. 데이터 세트의 작은 샘플이기 때문에이 문제를 해결할 수 있었지만 전체 택시 요금 데이터 세트에서는 이것이 가능하지 않았습니다.\n",
    "\n",
    "이 노트북에서는 디스크에서 .csv 파일을 한 번에 한 번에 하나씩 직접 읽는 방법을 보여줍니다. tf.data.TextLineDataset\n",
    "\n",
    "다음 셀을 실행하고 필요한 경우 커널을 다시 시작하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input function reading from CSV\n",
    "\n",
    "We define `read_dataset()` which given a csv file path returns a `tf.data.Dataset` in which each row represents a (features,label) in the Estimator API required format \n",
    "- features: A python dictionary. Each key is a feature column name and its value is the tensor containing the data for that feature\n",
    "- label: A Tensor containing the labels\n",
    "\n",
    "We then invoke `read_dataset()` function from within the `train_input_fn()` and `eval_input_fn()`. The remaining code is as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 1**\n",
    "\n",
    "In the next cell, implement a `parse_row` function that takes as input a csv row (as a string) \n",
    "and returns a tuple (features, labels) as described above.\n",
    "\n",
    "First, use the [tf.decode_csv function](https://www.tensorflow.org/api_docs/python/tf/io/decode_csv) to read in the features from a csv file. Next, once `fields` has been read from the `.csv` file, create a dictionary of features and values. Lastly, define the label and remove it from the `features` dict you created. This can be done in one step with pythons pop operation.\n",
    "\n",
    "The column names and the default values you'll need for these operations are given by global variables `CSV_COLUMN_NAMES`\n",
    "and `CSV_DEFAULTS`. The labels are stored in the first column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV에서 입력 함수 읽기 \n",
    "우리는 read_dataset()주어진 csv 파일 경로가 tf.data.DatasetEstimator API 필수 형식으로 각 행이 (기능, 레이블)을 나타내는 a를 반환하도록 정의 합니다.\n",
    "\n",
    "특징 : 파이썬 사전. 각 키는 기능 열 이름이며 해당 값은 해당 기능에 대한 데이터를 포함하는 텐서입니다.\n",
    "라벨 : 라벨이 포함 된 텐서\n",
    "그런 다음 and read_dataset()내에서 함수 를 호출 합니다 . 나머지 코드는 이전과 같습니다.train_input_fn()eval_input_fn()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "연습 1 \n",
    "다음 셀에서는 parse_rowcsv 행을 입력으로 사용하고 (문자열로) 위에서 설명한대로 튜플 (기능, 레이블)을 반환하는 함수를 구현하십시오 .\n",
    "\n",
    "먼저 tf.decode_csv 함수 를 사용하여 csv 파일에서 기능 을 읽으십시오. 다음으로 파일 fields에서 한 번 읽은 후 .csv기능 및 값의 사전을 작성하십시오. 마지막으로, 레이블을 정의하고 features작성한 dict 에서 레이블을 제거하십시오 . 이것은 pythons pop 조작으로 한 단계로 수행 할 수 있습니다.\n",
    "\n",
    "이러한 작업에 필요한 열 이름과 기본값은 전역 변수 CSV_COLUMN_NAMES 및 로 지정됩니다 CSV_DEFAULTS. 레이블은 첫 번째 열에 저장됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMN_NAMES = [\"fare_amount\",\"dayofweek\",\"hourofday\",\"pickuplon\",\"pickuplat\",\"dropofflon\",\"dropofflat\"]\n",
    "CSV_DEFAULTS = [[0.0],[1],[0],[-74.0], [40.0], [-74.0], [40.7]]\n",
    "\n",
    "def parse_row(row):\n",
    "    fields = tf.decode_csv(records=row, record_defaults=CSV_DEFAULTS)\n",
    "    features = {'dayofweek' : fields[1],'hourofday' : fields[2],'pickuplon' : fields[3],'pickuplat' : fields[4],'dropofflon' : fields[5],'dropofflat' : fields[6]}\n",
    "    labels = fields[0]\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following test to make sure your implementation is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You rock!\n"
     ]
    }
   ],
   "source": [
    "a_row = \"0.0,1,0,-74.0,40.0,-74.0,40.7\"\n",
    "features, labels = parse_row(a_row)\n",
    "\n",
    "assert labels.numpy() == 0.0\n",
    "assert features[\"pickuplon\"].numpy() == -74.0\n",
    "print(\"You rock!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 2**\n",
    "\n",
    "Use the function `parse_row` you implemented in the previous exercise to \n",
    "implement a `read_dataset` function that\n",
    "- takes as input the path to a csv file\n",
    "- returns a `tf.data.Dataset` object containing the features, labels\n",
    "\n",
    "Assume that the .csv file has a header, and that your `read_dataset` will skip it. Have a look at the [tf.data.TextLineDataset documentation](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset) to see what variables to pass when initializing the dataset pipeline. Then use the `parse_row` operation we created above to read the values from the .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(csv_path):  \n",
    "    dataset = tf.data.TextLineDataset(csv_path)\n",
    "    dataset = dataset.skip(1).map(parse_row)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests\n",
    "\n",
    "Let's create a test dataset to test our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.csv\n",
    "fare_amount,dayofweek,hourofday,pickuplon,pickuplat,dropofflon,dropofflat\n",
    "28,1,0,-73.0,41.0,-74.0,20.7\n",
    "12.3,1,0,-72.0,44.0,-75.0,40.6\n",
    "10,1,0,-71.0,41.0,-71.0,42.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to iterate over what's returned by `read_dataset`. We'll print the `dropofflat` and `fare_amount` for each entry in `./test.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropofflat: 20.7\n",
      "fare_amount: 28.0\n",
      "dropofflat: 40.6\n",
      "fare_amount: 12.3\n",
      "dropofflat: 42.9\n",
      "fare_amount: 10.0\n"
     ]
    }
   ],
   "source": [
    "for feature, label in read_dataset(\"./test.csv\"):\n",
    "    print(\"dropofflat:\", feature[\"dropofflat\"].numpy())\n",
    "    print(\"fare_amount:\", label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following test cell to make sure you function works properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-23-b86c3510ac7e>:2: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "You rock!\n"
     ]
    }
   ],
   "source": [
    "dataset= read_dataset(\"./test.csv\")\n",
    "dataset_iterator = dataset.make_one_shot_iterator()\n",
    "features, labels = dataset_iterator.get_next()\n",
    "\n",
    "assert features['dayofweek'].numpy() == 1\n",
    "assert labels.numpy() == 28\n",
    "print(\"You rock!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 3**\n",
    "\n",
    "In the code cell below, implement a `train_input_fn` function that\n",
    "- takes as input a path to a csv file along with a batch_size\n",
    "- returns a dataset object that shuffle the rows and returns them in batches of `batch_size`\n",
    "\n",
    "**Hint:** Reuse the `read_dataset` function you implemented above. \n",
    "\n",
    "Once you've initialized the `dataset`, be sure to add a step to `shuffle`, `repeat` and `batch` to your pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드 셀에서 train_input_fn 함수를 구현하십시오.\n",
    "\n",
    "batch_size와 함께 csv 파일의 경로를 입력으로 사용합니다.\n",
    "행을 섞는 데이터 세트 객체를 반환하고 batch_size의 배치로 반환합니다\n",
    "힌트 : 위에서 구현 한 read_dataset 함수를 재사용하십시오.\n",
    "\n",
    "데이터 세트를 초기화 한 후 파이프 라인에 셔플, 반복 및 배치 단계를 추가해야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn(csv_path, batch_size = 128):\n",
    "    dataset = read_dataset(csv_path)\n",
    "    dataset = dataset.shuffle(buffer_size=1000,seed=1).repeat(count=None).batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 4**\n",
    "\n",
    "Next, implement as `eval_input_fn` similar to the `train_input_fn` you implemented above. Remember, the only difference is that this function does not need to shuffle the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_input_fn(csv_path, batch_size = 128):\n",
    "    dataset = read_dataset(csv_path)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feature columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features of our models are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dayofweek', 'hourofday', 'pickuplon', 'pickuplat', 'dropofflon', 'dropofflat']\n"
     ]
    }
   ],
   "source": [
    "FEATURE_NAMES = CSV_COLUMN_NAMES[1:] # all but first column\n",
    "print(FEATURE_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 5**\n",
    "\n",
    "In the cell below, create a variable called `feature_cols` which contains a list of the appropriate `tf.feature_column` to be passed to a `tf.estimator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NumericColumn(key='dayofweek', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='hourofday', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='pickuplon', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='pickuplat', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='dropofflon', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='dropofflat', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [tf.feature_column.numeric_column(key=feature) for feature in FEATURE_NAMES]\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Estimator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 6**\n",
    "\n",
    "In the cell below, create an instance of a `tf.estimator.DNNRegressor` such that\n",
    "- it has two layers of 10 units each\n",
    "- it uses the features defined in the previous exercise\n",
    "- it saves the trained model into the directory `./taxi_trained`\n",
    "- it has a random seed set to 1 for replicability and debugging\n",
    "\n",
    "Have a look at [the documentation for Tensorflow's DNNRegressor](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor) to remind you of the implementation.\n",
    "\n",
    "**Hint:** Remember, the random seed is set by passing a `tf.estimator.RunConfig` object\n",
    "  to the `config` parameter of the `tf.estimator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_task_type': 'worker', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f37ac2a2668>, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_experimental_max_worker_delay_secs': None, '_model_dir': 'taxi_trained', '_evaluation_master': '', '_log_step_count_steps': 100, '_task_id': 0, '_save_checkpoints_steps': None, '_protocol': None, '_experimental_distribute': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_num_worker_replicas': 1, '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_tf_random_seed': 1, '_keep_checkpoint_max': 5, '_is_chief': True, '_num_ps_replicas': 0, '_device_fn': None, '_service': None, '_train_distribute': None, '_session_creation_timeout_secs': 7200, '_eval_distribute': None, '_global_id_in_cluster': 0}\n"
     ]
    }
   ],
   "source": [
    "OUTDIR = \"taxi_trained\"\n",
    "\n",
    "model = tf.estimator.DNNRegressor(\n",
    "           feature_columns=feature_cols,\n",
    "           hidden_units=[10,10],\n",
    "           model_dir=OUTDIR,\n",
    "           config=tf.estimator.RunConfig(tf_random_seed=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Next, we'll train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 7**\n",
    "\n",
    "Complete the code in the cell below to train the `DNNRegressor` model you instantiated above on our data. Have a look at [the documentation for the `train` method of the `DNNRegressor`](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor#train) to see what variables you should pass. You'll use the `train_input_function` you created above and the `./taxi-train.csv` dataset. \n",
    "\n",
    "If you train your model for 500 steps. How many epochs of the dataset does this represent? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 169441.03, step = 0\n",
      "INFO:tensorflow:global_step/sec: 85.1597\n",
      "INFO:tensorflow:loss = 6933.5566, step = 100 (1.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.46\n",
      "INFO:tensorflow:loss = 14215.314, step = 200 (1.131 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.6774\n",
      "INFO:tensorflow:loss = 7375.914, step = 300 (1.141 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.4496\n",
      "INFO:tensorflow:loss = 6521.3965, step = 400 (1.131 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 500 into taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 8966.378.\n",
      "CPU times: user 12.9 s, sys: 2.43 s, total: 15.3 s\n",
      "Wall time: 7.03 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.dnn.DNNRegressor at 0x7f37aaf61470>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tf.logging.set_verbosity(tf.logging.INFO) # so loss is printed during training\n",
    "shutil.rmtree(path = OUTDIR, ignore_errors = True) # start fresh each time\n",
    "\n",
    "model.train(\n",
    "    input_fn = lambda: train_input_fn(csv_path='./taxi-train.csv', batch_size = 128),\n",
    "    steps = 500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "Lastly, we'll evaluate our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 8**\n",
    "\n",
    "In the cell below, evaluate the model using its `.evaluate` method and the `eval_input_fn` function you implemented above on the `./taxi-valid.csv` dataset. Capture the result of running evaluation on the evaluation set in a variable called `metrics`. Then, extract the `average_loss` for the dictionary returned by `model.evaluate` and contained in `metrics`. This is the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-11-22T13:36:24Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-11-22-13:36:28\n",
      "INFO:tensorflow:Saving dict for global step 500: average_loss = 88.53384, global_step = 500, label/mean = 11.229713, loss = 11313.843, prediction/mean = 11.632246\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 500: taxi_trained/model.ckpt-500\n",
      "RMSE on dataset = 9.409242071747654\n"
     ]
    }
   ],
   "source": [
    "metrics = model.evaluate(input_fn = lambda: eval_input_fn(csv_path='./taxi-valid.csv'))\n",
    "print(\"RMSE on dataset = {}\".format(metrics['average_loss']**0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge exercise\n",
    "\n",
    "Create a neural network that is capable of finding the volume of a cylinder given the radius of its base (r) and its height (h). Assume that the radius and height of the cylinder are both in the range 0.5 to 2.0. Unlike in the challenge exercise for c_estimator.ipynb, assume that your measurements of r, h and V are all rounded off to the nearest 0.1. Simulate the necessary training dataset. This time, you will need a lot more data to get a good predictor.\n",
    "\n",
    "Hint (highlight to see):\n",
    "<p style='color:white'>\n",
    "Create random values for r and h and compute V. Then, round off r, h and V (i.e., the volume is computed from the true value of r and h; it's only your measurement that is rounded off). Your dataset will consist of the round values of r, h and V. Do this for both the training and evaluation datasets.\n",
    "</p>\n",
    "\n",
    "Now modify the \"noise\" so that instead of just rounding off the value, there is up to a 10% error (uniformly distributed) in the measurement followed by rounding off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
