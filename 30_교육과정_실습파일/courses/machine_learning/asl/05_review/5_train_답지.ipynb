{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on Cloud AI Platform\n",
    "\n",
    "**Learning Objectives**\n",
    "- Use CAIP to run a distributed training job\n",
    "\n",
    "## Introduction \n",
    "After having testing our training pipeline both locally and in the cloud on a susbset of the data, we can submit another (much larger) training job to the cloud. It is also a good idea to run a hyperparameter tuning job to make sure we have optimized the hyperparameters of our model. \n",
    "\n",
    "This notebook illustrates how to do distributed training and hyperparameter tuning on Cloud AI Platform. \n",
    "\n",
    "To start, we'll set up our environment variables as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"qwiklabs-gcp-ml-49b827b781ab\"  # Replace with your PROJECT\n",
    "BUCKET = \"qwiklabs-gcp-ml-49b827b781ab\"  # Replace with your BUCKET\n",
    "\n",
    "REGION = \"us-central1\"            # Choose an available region for Cloud CAIP\n",
    "TFVERSION = \"1.14\"                # TF version for CMLE to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = TFVERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll look for the preprocessed data for the babyweight model and copy it over if it's not there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! gsutil ls -r gs://$BUCKET | grep -q gs://$BUCKET/babyweight/preproc; then\n",
    "    gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "    # copy canonical set of preprocessed files if you didn't do previous notebook\n",
    "    gsutil -m cp -R gs://cloud-training-demos/babyweight gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-ml-49b827b781ab/babyweight/preproc/eval.csv-00000-of-00002\n",
      "gs://qwiklabs-gcp-ml-49b827b781ab/babyweight/preproc/train.csv-00000-of-00028\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/babyweight/preproc/*-00000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous labs we developed our TensorFlow model and got it working on a subset of the data. Now we can package the TensorFlow code up as a Python module and train it on Cloud AI Platform.\n",
    "\n",
    "## Train on Cloud AI Platform\n",
    "\n",
    "Training on Cloud AI Platform requires two things:\n",
    "- Configuring our code as a Python package\n",
    "- Using gcloud to submit the training code to Cloud AI Platform\n",
    "\n",
    "### Move code into a Python package\n",
    "\n",
    "A Python package is simply a collection of one or more `.py` files along with an `__init__.py` file to identify the containing directory as a package. The `__init__.py` sometimes contains initialization code but for our purposes an empty file suffices.\n",
    "\n",
    "The bash command `touch` creates an empty file in the specified location, the directory `babyweight` should already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "touch babyweight/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the `%%writefile` magic to write the contents of the cell below to a file called `task.py` in the `babyweight/trainer` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting babyweight/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile babyweight/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from . import model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--bucket\",\n",
    "        help=\"GCS path to data. We assume that data is in \\\n",
    "        gs://BUCKET/babyweight/preproc/\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help=\"Number of examples to compute gradient over.\",\n",
    "        type=int,\n",
    "        default=512\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help=\"this model ignores this field, but it is required by gcloud\",\n",
    "        default=\"junk\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nnsize\",\n",
    "        help=\"Hidden layer sizes to use for DNN feature columns -- provide \\\n",
    "        space-separated layers\",\n",
    "        nargs=\"+\",\n",
    "        type=int,\n",
    "        default=[128, 32, 4]\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nembeds\",\n",
    "        help=\"Embedding size of a cross of n key real-valued parameters\",\n",
    "        type=int,\n",
    "        default=3\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_examples\",\n",
    "        help=\"Number of examples (in thousands) to run the training job over. \\\n",
    "        If this is more than actual \\\n",
    "        So specifying 1000 here when you have only 100k examples \\\n",
    "        makes this 10 epochs.\",\n",
    "        type=int,\n",
    "        default=5000\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pattern\",\n",
    "        help=\"Specify a pattern that has to be in input files. \\\n",
    "        For example 00001-of \\\n",
    "        will process only one shard\",\n",
    "        default=\"of\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_steps\",\n",
    "        help=\"Positive number of steps for which to evaluate model. \\\n",
    "        Default to None, which means to evaluate until \\\n",
    "        input_fn raises an end-of-input exception\",\n",
    "        type=int,\n",
    "        default=None\n",
    "    )\n",
    "\n",
    "    # Parse arguments\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    # Pop unnecessary args needed for gcloud\n",
    "    arguments.pop(\"job-dir\", None)\n",
    "\n",
    "    # Assign the arguments to the model variables\n",
    "    output_dir = arguments.pop(\"output_dir\")\n",
    "    print('output_dir >> {}'.format(output_dir))\n",
    "    model.BUCKET = arguments.pop(\"bucket\")\n",
    "    model.BATCH_SIZE = arguments.pop(\"batch_size\")\n",
    "    model.TRAIN_STEPS = (\n",
    "        arguments.pop(\"train_examples\") * 1000) / model.BATCH_SIZE\n",
    "    model.EVAL_STEPS = arguments.pop(\"eval_steps\")\n",
    "    print (\"Will train for {} steps using batch_size={}\".format(\n",
    "        model.TRAIN_STEPS, model.BATCH_SIZE))\n",
    "    model.PATTERN = arguments.pop(\"pattern\")\n",
    "    model.NEMBEDS = arguments.pop(\"nembeds\")\n",
    "    model.NNSIZE = arguments.pop(\"nnsize\")\n",
    "    print (\"Will use DNN size of {}\".format(model.NNSIZE))\n",
    "\n",
    "    # Append trial_id to path if we are doing hptuning\n",
    "    # This code can be removed if you are not using hyperparameter tuning\n",
    "    output_dir = os.path.join(\n",
    "        output_dir,\n",
    "        json.loads(\n",
    "            os.environ.get(\"TF_CONFIG\", \"{}\")\n",
    "        ).get(\"task\", {}).get(\"trial\", \"\")\n",
    "    )\n",
    "\n",
    "    # Run the training job\n",
    "    model.train_and_evaluate(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way we can write to the file `model.py` the model that we developed in the previous notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting babyweight/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile babyweight/trainer/model.py\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "BUCKET = None  # set from task.py\n",
    "PATTERN = \"of\"\n",
    "\n",
    "CSV_COLUMNS = [\n",
    "    \"weight_pounds\",\n",
    "    \"is_male\",\n",
    "    \"mother_age\",\n",
    "    \"plurality\",\n",
    "    \"gestation_weeks\",\n",
    "]\n",
    "LABEL_COLUMN = \"weight_pounds\"\n",
    "DEFAULTS = [[0.0], [\"null\"], [0.0], [\"null\"], [0.0]]\n",
    "\n",
    "TRAIN_STEPS = 10000\n",
    "EVAL_STEPS = None\n",
    "BATCH_SIZE = 512\n",
    "NEMBEDS = 3\n",
    "NNSIZE = [64, 16, 4]\n",
    "\n",
    "\n",
    "def read_dataset(filename_pattern, mode, batch_size):\n",
    "    def _input_fn():\n",
    "        def decode_csv(value_column):\n",
    "            columns = tf.decode_csv(\n",
    "                records=value_column,\n",
    "                record_defaults=DEFAULTS\n",
    "            )\n",
    "            features = dict(zip(CSV_COLUMNS, columns))\n",
    "            label = features.pop(LABEL_COLUMN)\n",
    "            return features, label\n",
    "\n",
    "        file_path = \"gs://{}/babyweight/preproc/{}*{}*\".format(\n",
    "            BUCKET, filename_pattern, PATTERN)\n",
    "        file_list = tf.gfile.Glob(filename=file_path)\n",
    "\n",
    "        dataset = (\n",
    "            tf.data.TextLineDataset(filenames=file_list).map(\n",
    "                map_func=decode_csv)\n",
    "        )\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None  # indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size=10*batch_size)\n",
    "        else:\n",
    "            num_epochs = 1\n",
    "        dataset = dataset.repeat(count=num_epochs).batch(batch_size=batch_size)\n",
    "        return dataset\n",
    "    return _input_fn\n",
    "\n",
    "\n",
    "def get_wide_deep():\n",
    "\n",
    "    fc_is_male = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key=\"is_male\",\n",
    "        vocabulary_list=[\"True\", \"False\", \"Unknown\"]\n",
    "    )\n",
    "\n",
    "    fc_plurality = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key=\"plurality\",\n",
    "        vocabulary_list=[\n",
    "            \"Single(1)\",\n",
    "            \"Twins(2)\",\n",
    "            \"Triplets(3)\",\n",
    "            \"Quadruplets(4)\",\n",
    "            \"Quintuplets(5)\",\n",
    "            \"Multiple(2+)\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    fc_mother_age = tf.feature_column.numeric_column(\"mother_age\")\n",
    "\n",
    "    fc_gestation_weeks = tf.feature_column.numeric_column(\"gestation_weeks\")\n",
    "\n",
    "    fc_age_buckets = tf.feature_column.bucketized_column(\n",
    "        source_column=fc_mother_age, \n",
    "        boundaries=np.arange(start=15, stop=45, step=1).tolist()\n",
    "    )\n",
    "\n",
    "    fc_gestation_buckets = tf.feature_column.bucketized_column(\n",
    "        source_column=fc_gestation_weeks,\n",
    "        boundaries=np.arange(start=17, stop=47, step=1).tolist())\n",
    "\n",
    "    wide = [\n",
    "        fc_is_male,\n",
    "        fc_plurality,\n",
    "        fc_age_buckets,\n",
    "        fc_gestation_buckets\n",
    "    ]\n",
    "\n",
    "    # Feature cross all the wide columns and embed into a lower dimension\n",
    "    crossed = tf.feature_column.crossed_column(\n",
    "        keys=wide, hash_bucket_size=20000\n",
    "    )\n",
    "    fc_embed = tf.feature_column.embedding_column(\n",
    "        categorical_column=crossed,\n",
    "        dimension=3\n",
    "    )\n",
    "\n",
    "    # Continuous columns are deep, have a complex relationship with the output\n",
    "    deep = [\n",
    "        fc_mother_age,\n",
    "        fc_gestation_weeks,\n",
    "        fc_embed\n",
    "    ]\n",
    "\n",
    "    return wide, deep\n",
    "\n",
    "\n",
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "        \"is_male\": tf.placeholder(dtype=tf.string, shape=[None]),\n",
    "        \"mother_age\": tf.placeholder(dtype=tf.float32, shape=[None]),\n",
    "        \"plurality\": tf.placeholder(dtype=tf.string, shape=[None]),\n",
    "        \"gestation_weeks\": tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "    }\n",
    "\n",
    "    features = {\n",
    "        key: tf.expand_dims(input=tensor, axis=-1)\n",
    "        for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(\n",
    "        features=features, \n",
    "        receiver_tensors=feature_placeholders\n",
    "    )\n",
    "\n",
    "\n",
    "def my_rmse(labels, predictions):\n",
    "    pred_values = predictions[\"predictions\"]\n",
    "    return {\n",
    "        \"rmse\": tf.metrics.root_mean_squared_error(\n",
    "            labels=labels,\n",
    "            predictions=pred_values\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "def train_and_evaluate(output_dir):\n",
    "    wide, deep = get_wide_deep()\n",
    "    EVAL_INTERVAL = 300  # seconds\n",
    "\n",
    "    run_config = tf.estimator.RunConfig(\n",
    "        save_checkpoints_secs=EVAL_INTERVAL,\n",
    "        keep_checkpoint_max=3)\n",
    "\n",
    "    estimator = tf.estimator.DNNLinearCombinedRegressor(\n",
    "        model_dir=output_dir,\n",
    "        linear_feature_columns=wide,\n",
    "        dnn_feature_columns=deep,\n",
    "        dnn_hidden_units=NNSIZE,\n",
    "        config=run_config)\n",
    "\n",
    "    estimator = tf.contrib.estimator.add_metrics(estimator, my_rmse)\n",
    "\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=read_dataset(\n",
    "            \"train\", tf.estimator.ModeKeys.TRAIN, BATCH_SIZE),\n",
    "        max_steps=TRAIN_STEPS)\n",
    "\n",
    "    exporter = tf.estimator.LatestExporter(\n",
    "        name=\"exporter\",\n",
    "        serving_input_receiver_fn=serving_input_fn,\n",
    "        exports_to_keep=None)\n",
    "\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn=read_dataset(\n",
    "            \"eval\", tf.estimator.ModeKeys.EVAL, 2**15),\n",
    "        steps=EVAL_STEPS,\n",
    "        start_delay_secs=60,  # start evaluating after N seconds\n",
    "        throttle_secs=EVAL_INTERVAL,  # evaluate every N seconds\n",
    "        exporters=exporter)\n",
    "\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        estimator=estimator,\n",
    "        train_spec=train_spec,\n",
    "        eval_spec=eval_spec\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train locally\n",
    "\n",
    "After moving the code to a package, make sure it works as a standalone. Note, we incorporated the `--pattern` and `--train_examples` flags so that we don't try to train on the entire dataset while we are developing our pipeline. Once we are sure that everything is working on a subset, we can change the pattern so that we can train on all the data. Even for this subset, this takes about *3 minutes* in which you won't see any output ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket=qwiklabs-gcp-ml-49b827b781ab\n",
      "output_dir >> babyweight_trained\n",
      "Will train for 1 steps using batch_size=512\n",
      "Will use DNN size of [128, 32, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-30 04:51:47.029219: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "WARNING:tensorflow:From /home/jupyter/training-data-analyst/courses/machine_learning/deepdive/05_review/babyweight/trainer/model.py:6: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter/training-data-analyst/courses/machine_learning/deepdive/05_review/babyweight/trainer/model.py:6: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 300, '_num_ps_replicas': 0, '_keep_checkpoint_max': 3, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f789dcc2cd0>, '_model_dir': 'babyweight_trained/', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_session_creation_timeout_secs': 7200, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_experimental_max_worker_delay_secs': None, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 300, '_num_ps_replicas': 0, '_keep_checkpoint_max': 3, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f78957c4a10>, '_model_dir': 'babyweight_trained/', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_session_creation_timeout_secs': 7200, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_experimental_max_worker_delay_secs': None, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 300.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/training_util.py:236: initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From /home/jupyter/training-data-analyst/courses/machine_learning/deepdive/05_review/babyweight/trainer/model.py:41: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:12: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.decode_csv is deprecated. Please use tf.io.decode_csv instead.\n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/feature_column/feature_column_v2.py:3079: _num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/feature_column/feature_column_v2.py:305: add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling __init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/embedding_ops.py:802: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/canned/linear.py:308: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/adagrad.py:76: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2019-11-30 04:51:51.336972: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-11-30 04:51:51.343780: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2019-11-30 04:51:51.344180: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5614f9e17af0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2019-11-30 04:51:51.344214: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2019-11-30 04:51:51.346939: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2019-11-30 04:51:51.421875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-30 04:51:51.422271: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5614f9ee6ea0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2019-11-30 04:51:51.422301: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2019-11-30 04:51:51.422554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-30 04:51:51.422801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "2019-11-30 04:51:51.422841: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-11-30 04:51:51.424365: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "2019-11-30 04:51:51.425492: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
      "2019-11-30 04:51:51.425915: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
      "2019-11-30 04:51:51.427799: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2019-11-30 04:51:51.429140: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2019-11-30 04:51:51.432630: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2019-11-30 04:51:51.432744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-30 04:51:51.433170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-30 04:51:51.433388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
      "2019-11-30 04:51:51.433475: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-11-30 04:51:51.943028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-11-30 04:51:51.943078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
      "2019-11-30 04:51:51.943088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
      "2019-11-30 04:51:51.943351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-30 04:51:51.943694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-30 04:51:51.943970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 198 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into babyweight_trained/model.ckpt.\n",
      "2019-11-30 04:51:53.645493: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "INFO:tensorflow:loss = 24382.188, step = 0\n",
      "INFO:tensorflow:Saving checkpoints for 1 into babyweight_trained/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /home/jupyter/training-data-analyst/courses/machine_learning/deepdive/05_review/babyweight/trainer/model.py:137: The name tf.metrics.root_mean_squared_error is deprecated. Please use tf.compat.v1.metrics.root_mean_squared_error instead.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-11-30T04:51:55Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2019-11-30 04:51:56.060881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-30 04:51:56.061246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "2019-11-30 04:51:56.061318: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-11-30 04:51:56.061358: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "2019-11-30 04:51:56.061405: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
      "2019-11-30 04:51:56.061431: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
      "2019-11-30 04:51:56.061459: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2019-11-30 04:51:56.061484: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2019-11-30 04:51:56.061509: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2019-11-30 04:51:56.061603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-30 04:51:56.061926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-30 04:51:56.062258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
      "2019-11-30 04:51:56.062344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-11-30 04:51:56.062363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
      "2019-11-30 04:51:56.062370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
      "2019-11-30 04:51:56.062527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-30 04:51:56.062836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-30 04:51:56.063205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 198 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "INFO:tensorflow:Restoring parameters from babyweight_trained/model.ckpt-1\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2019-11-30-04:52:00\n",
      "INFO:tensorflow:Saving dict for global step 1: average_loss = 42.166138, global_step = 1, label/mean = 7.2616043, loss = 1381700.0, prediction/mean = 0.8893795, rmse = 6.493546\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1: babyweight_trained/model.ckpt-1\n",
      "WARNING:tensorflow:From /home/jupyter/training-data-analyst/courses/machine_learning/deepdive/05_review/babyweight/trainer/model.py:117: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Regression input must be a single string Tensor; got {'plurality': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=string>, 'mother_age': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>, 'gestation_weeks': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'is_male': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>}\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'plurality': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=string>, 'mother_age': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>, 'gestation_weeks': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'is_male': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "2019-11-30 04:52:01.157529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-30 04:52:01.157953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "2019-11-30 04:52:01.157994: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-11-30 04:52:01.158032: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "2019-11-30 04:52:01.158049: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
      "2019-11-30 04:52:01.158074: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
      "2019-11-30 04:52:01.158089: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2019-11-30 04:52:01.158108: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2019-11-30 04:52:01.158128: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2019-11-30 04:52:01.158193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-30 04:52:01.158445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-30 04:52:01.158663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
      "2019-11-30 04:52:01.158698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-11-30 04:52:01.158710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
      "2019-11-30 04:52:01.158731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
      "2019-11-30 04:52:01.158882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-30 04:52:01.159152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-30 04:52:01.159384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 198 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "INFO:tensorflow:Restoring parameters from babyweight_trained/model.ckpt-1\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: babyweight_trained/export/exporter/temp-1575089520/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 24382.188.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo \"bucket=$BUCKET\"\n",
    "rm -rf babyweight_trained\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/babyweight\n",
    "python -m trainer.task \\\n",
    "    --bucket=$BUCKET \\\n",
    "    --output_dir=babyweight_trained \\\n",
    "    --job-dir=./tmp \\\n",
    "    --pattern=\"00000-of-\"\\\n",
    "    --train_examples=1 \\\n",
    "    --eval_steps=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "The JSON below represents an input into your prediction model. Write the input.json file below with the next cell, then run the prediction locally to assess whether it produces predictions correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing inputs.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile inputs.json\n",
    "{\"is_male\": \"True\", \"mother_age\": 26.0, \"plurality\": \"Single(1)\", \"gestation_weeks\": 39}\n",
    "{\"is_male\": \"False\", \"mother_age\": 26.0, \"plurality\": \"Single(1)\", \"gestation_weeks\": 39}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/training-data-analyst/courses/machine_learning/deepdive/05_review/babyweight_trained/export/exporter/1574918836\n",
      "PREDICTIONS\n",
      "[-0.8591848015785217]\n",
      "[-0.8590583801269531]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The `gcloud ml-engine` commands have been renamed and will soon be removed. Please use `gcloud ai-platform` instead.\n",
      "If the signature defined in the model is not serving_default then you must specify it via --signature-name flag, otherwise the command may fail.\n",
      "WARNING: 2019-11-28 05:27:38.413301: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:48: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:50: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:601: The name tf.gfile.IsDirectory is deprecated. Please use tf.io.gfile.isdir instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:221: The name tf.saved_model.loader.maybe_saved_model_directory is deprecated. Please use tf.compat.v1.saved_model.loader.maybe_saved_model_directory instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:228: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:228: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2019-11-28 05:27:39.943023: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2019-11-28 05:27:41.860489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-28 05:27:41.861010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "2019-11-28 05:27:41.861057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-11-28 05:27:41.864804: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "2019-11-28 05:27:41.866595: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
      "2019-11-28 05:27:41.867137: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
      "2019-11-28 05:27:41.869868: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2019-11-28 05:27:41.871286: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2019-11-28 05:27:41.877720: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2019-11-28 05:27:41.877824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-28 05:27:41.878305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-28 05:27:41.878671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
      "2019-11-28 05:27:41.880285: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-11-28 05:27:41.887612: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2019-11-28 05:27:41.887935: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f879e3cb10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2019-11-28 05:27:41.887981: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2019-11-28 05:27:41.950242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-28 05:27:41.950821: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f879e55b80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2019-11-28 05:27:41.950857: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2019-11-28 05:27:41.951084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-28 05:27:41.951509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "2019-11-28 05:27:41.951545: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-11-28 05:27:41.951604: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "2019-11-28 05:27:41.951626: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
      "2019-11-28 05:27:41.951639: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
      "2019-11-28 05:27:41.951652: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2019-11-28 05:27:41.951670: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2019-11-28 05:27:41.951684: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2019-11-28 05:27:41.951743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-28 05:27:41.952135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-28 05:27:41.952474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
      "2019-11-28 05:27:41.952563: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-11-28 05:27:42.448565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-11-28 05:27:42.448614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
      "2019-11-28 05:27:42.448631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
      "2019-11-28 05:27:42.449049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-28 05:27:42.449501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-28 05:27:42.449877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10747 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:230: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:230: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "2019-11-28 05:27:43.169862: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "MODEL_LOCATION=$(ls -d $(pwd)/babyweight_trained/export/exporter/* | tail -1)\n",
    "echo $MODEL_LOCATION\n",
    "gcloud ml-engine local predict --model-dir=$MODEL_LOCATION --json-instances=inputs.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the Cloud with CAIP\n",
    "\n",
    "Once the code works in standalone mode, you can run it on Cloud AI Platform.  Because this is on the entire dataset, it will take a while. The training run took about <b> an hour </b> for me. You can monitor the job from the GCP console in the Cloud AI Platform section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-ml-49b827b781ab/babyweight/trained_model us-central1 babyweight_191128_064251\n",
      "jobId: babyweight_191128_064251\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Job [babyweight_191128_064251] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe babyweight_191128_064251\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs babyweight_191128_064251\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/babyweight/trained_model\n",
    "JOBNAME=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ai-platform jobs submit training $JOBNAME \\\n",
    "    --region=$REGION \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=$(pwd)/babyweight/trainer \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --scale-tier=PREMIUM_1 \\\n",
    "    --runtime-version=$TFVERSION \\\n",
    "    -- \\\n",
    "    --bucket=${BUCKET} \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --train_examples=200000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran it, I used train_examples=2000000. When training finished, I filtered in the Stackdriver log on the word \"dict\" and saw that the last line was:\n",
    "<pre>\n",
    "Saving dict for global step 5714290: average_loss = 1.06473, global_step = 5714290, loss = 34882.4, rmse = 1.03186\n",
    "</pre>\n",
    "The final RMSE was 1.03 pounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Hyperparameter tuning </h2>\n",
    "<p>\n",
    "All of these are command-line parameters to my program.  To do hyperparameter tuning, create hyperparam.xml and pass it as --configFile.\n",
    "This step will take <b>1 hour</b> -- you can increase maxParallelTrials or reduce maxTrials to get it done faster.  Since maxParallelTrials is the number of initial seeds to start searching from, you don't want it to be too large; otherwise, all you have is a random search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hyperparam.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "    scaleTier: STANDARD_1\n",
    "    hyperparameters:\n",
    "        hyperparameterMetricTag: rmse\n",
    "        goal: MINIMIZE\n",
    "        maxTrials: 20\n",
    "        maxParallelTrials: 5\n",
    "        enableTrialEarlyStopping: True\n",
    "        params:\n",
    "        - parameterName: batch_size\n",
    "          type: INTEGER\n",
    "          minValue: 8\n",
    "          maxValue: 512\n",
    "          scaleType: UNIT_LOG_SCALE\n",
    "        - parameterName: nembeds\n",
    "          type: INTEGER\n",
    "          minValue: 3\n",
    "          maxValue: 30\n",
    "          scaleType: UNIT_LINEAR_SCALE\n",
    "        - parameterName: nnsize\n",
    "          type: INTEGER\n",
    "          minValue: 64\n",
    "          maxValue: 512\n",
    "          scaleType: UNIT_LOG_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-ml-49b827b781ab/babyweight/hyperparam us-central1 babyweight_191128_053112\n",
      "jobId: babyweight_191128_053112\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Job [babyweight_191128_053112] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe babyweight_191128_053112\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs babyweight_191128_053112\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/babyweig%ht/hyperparam\n",
    "JOBNAME=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ai-platform jobs submit training $JOBNAME \\\n",
    "    --region=$REGION \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=$(pwd)/babyweight/trainer \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --scale-tier=STANDARD_1 \\\n",
    "    --config=hyperparam.yaml \\\n",
    "    --runtime-version=$TFVERSION \\\n",
    "    -- \\\n",
    "    --bucket=${BUCKET} \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --eval_steps=10 \\\n",
    "    --train_examples=20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat training \n",
    "\n",
    "Now that we've determined the optimal hyparameters, we'll retrain with these tuned parameters. Note the last line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-ml-49b827b781ab/babyweight/trained_model_tuned us-central1 babyweight_191128_055053\n",
      "jobId: babyweight_191128_055053\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/babyweight/trained_model_tuned/#1574919367673406...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/babyweight/trained_model_tuned/checkpoint#1574920092894293...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/babyweight/trained_model_tuned/events.out.tfevents.1574919367.cmle-training-master-0aef35cb78-0-7pjlq#1574919368626074...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/babyweight/trained_model_tuned/events.out.tfevents.1574919375.cmle-training-worker-0aef35cb78-0-84hv9#1574919376236831...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/babyweight/trained_model_tuned/events.out.tfevents.1574919710.cmle-training-master-208d288642-0-9xxnq#1574920094798381...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/babyweight/trained_model_tuned/events.out.tfevents.1574919719.cmle-training-worker-208d288642-0-48d9g#1574920200748604...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/babyweight/trained_model_tuned/graph.pbtxt#1574919776910455...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/babyweight/trained_model_tuned/model.ckpt-0.data-00000-of-00004#1574919782381763...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/babyweight/trained_model_tuned/model.ckpt-217175.data-00000-of-00004#1574920090749882...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/babyweight/trained_model_tuned/model.ckpt-0.data-00002-of-00004#1574919781401034...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/babyweight/trained_model_tuned/model.ckpt-0.meta#1574919785383483...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/babyweight/trained_model_tuned/model.ckpt-0.data-00003-of-00004#1574919780973394...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/babyweight/trained_model_tuned/model.ckpt-0.data-00001-of-00004#1574919781976104...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/babyweight/trained_model_tuned/model.ckpt-217175.meta#1574920094076194...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/babyweight/trained_model_tuned/model.ckpt-0.index#1574919782936560...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/babyweight/trained_model_tuned/model.ckpt-217175.index#1574920091244372...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/babyweight/trained_model_tuned/model.ckpt-217175.data-00001-of-00004#1574920090322003...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/babyweight/trained_model_tuned/model.ckpt-217175.data-00002-of-00004#1574920089867844...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/babyweight/trained_model_tuned/model.ckpt-217175.data-00003-of-00004#1574920089416653...\n",
      "/ [19/19 objects] 100% Done                                                     \n",
      "Operation completed over 19 objects.                                             \n",
      "Job [babyweight_191128_055053] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe babyweight_191128_055053\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs babyweight_191128_055053\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/babyweight/trained_model_tuned\n",
    "JOBNAME=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ai-platform jobs submit training $JOBNAME \\\n",
    "    --region=$REGION \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=$(pwd)/babyweight/trainer \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --scale-tier=PREMIUM_1 \\\n",
    "    --runtime-version=$TFVERSION \\\n",
    "    -- \\\n",
    "    --bucket=${BUCKET} \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --train_examples=20000 --batch_size=35 --nembeds=16 --nnsize=281"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2017 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
