{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Machine Learning\n",
    "\n",
    "**Learning Objectives**\n",
    "* Understand the different approaches for data preprocessing in developing ML models\n",
    "* Use Dataflow to perform data preprocessing steps\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous notebook we achieved an RMSE of **3.85**. Let's see if we can improve upon that by creating a data preprocessing pipeline in Cloud Dataflow.\n",
    "\n",
    "Preprocessing data for a machine learning model involves both data engineering and feature engineering. During data engineering, we convert raw data into prepared data which is necessary for the model. Feature engineering then takes that prepared data and creates the features expected by the model. We have already seen various ways we can engineer new features for a machine learning model and where those steps take place. We also have flexibility as to where data preprocessing steps can take place; for example, BigQuery, Cloud Dataflow and Tensorflow. In this lab, we'll explore different data preprocessing strategies and see how they can be accomplished with Cloud Dataflow.\n",
    "\n",
    "One perspective in which to categorize different types of data preprocessing operations is in terms of the granularity of the operation. Here, we will consider the following three types of operations:\n",
    "1. Instance-level transformations\n",
    "2. Full-pass transformations\n",
    "3. Time-windowed aggregations\n",
    "\n",
    "Cloud Dataflow can perform each of these types of operations and is particularly useful when performing computationally expensive operations as it is an autoscaling service for batch and streaming data processing pipelines. We'll say a few words about each of these below. For more information, have a look at this article about [data preprocessing for machine learning from Google Cloud](https://cloud.google.com/solutions/machine-learning/data-preprocessing-for-ml-with-tf-transform-pt1).\n",
    "\n",
    "**1. Instance-level transformations**\n",
    "These are transformations which take place during training and prediction, looking only at values from a single data point. For example, they might include clipping the value of a feature, polynomially expand a feature, multiply two features, or compare two features to create a Boolean flag.\n",
    "\n",
    "It is necessary to apply the same transformations at training time and at prediction time. Failure to do this results in training/serving skew and will negatively affect the performance of the model.\n",
    "\n",
    "**2. Full-pass transformations**\n",
    "These transformations occur during training, but occur as instance-level operations during prediction. That is, during training you must analyze the entirety of the training data to compute quantities such as maximum, minimum, mean or variance while at prediction time you need only use those values to rescale or normalize a single data point. \n",
    "\n",
    "A good example to keep in mind is standard scaling (z-score normalization) of features for training. You need to compute the mean and standard deviation of that feature across the whole training data set, thus it is called a full-pass transformation. At prediction time you use those previously computed values to appropriately normalize the new data point. Failure to do so results in training/serving skew.\n",
    "\n",
    "**3. Time-windowed aggregations**\n",
    "These types of transformations occur during training and at prediction time. They involve creating a feature by summarizing real-time values by aggregating over some temporal window clause. For example, if we wanted our model to estimate the taxi trip time based on the traffic metrics for the route in the last 5 minutes, in the last 10 minutes or the last 30 minutes we would want to create a time-window to aggreagate these values. \n",
    "\n",
    "At prediction time these aggregations have to be computed in real-time from a data stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment variables and load necessary libraries\n",
    "\n",
    "Apache Beam only works in Python 2 at the moment, so switch to the Python 2 kernel in the upper right hand side. Then execute the following cells to install the necessary libraries if they have not been installed already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apache-beam==2.12.0\n"
     ]
    }
   ],
   "source": [
    "#Ensure that we have the correct version of Apache Beam installed\n",
    "!pip freeze | grep apache-beam || sudo pip install apache-beam[gcp]==2.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /usr/local/lib/python3.5/dist-packages (19.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: apache-beam[gcp]==2.16.0 in /usr/local/lib/python3.5/dist-packages (2.16.0)\n",
      "Requirement already satisfied, skipping upgrade: pyarrow<0.15.0,>=0.11.1; python_version >= \"3.0\" or platform_system != \"Windows\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]==2.16.0) (0.11.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]==2.16.0) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf<4,>=3.5.0.post1 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]==2.16.0) (3.10.0)\n",
      "Requirement already satisfied, skipping upgrade: mock<3.0.0,>=1.0.1 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]==2.16.0) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: httplib2<=0.12.0,>=0.8 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]==2.16.0) (0.11.3)\n",
      "Requirement already satisfied, skipping upgrade: oauth2client<4,>=2.0.1 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]==2.16.0) (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: dill<0.3.1,>=0.3.0 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]==2.16.0) (0.3.0)\n",
      "Requirement already satisfied, skipping upgrade: avro-python3<2.0.0,>=1.8.1; python_version >= \"3.0\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]==2.16.0) (1.9.1)\n",
      "Requirement already satisfied, skipping upgrade: crcmod<2.0,>=1.7 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]==2.16.0) (1.7)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml<4.0.0,>=3.12 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]==2.16.0) (3.13)\n",
      "Requirement already satisfied, skipping upgrade: pydot<2,>=1.2.0 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]==2.16.0) (1.2.4)\n",
      "Requirement already satisfied, skipping upgrade: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]==2.16.0) (2.5.8)\n",
      "Requirement already satisfied, skipping upgrade: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]==2.16.0) (3.9.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2018.3 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]==2.16.0) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: fastavro<0.22,>=0.21.4 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]==2.16.0) (0.21.24)\n",
      "Requirement already satisfied, skipping upgrade: grpcio<2,>=1.12.1 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]==2.16.0) (1.25.0)\n",
      "Requirement already satisfied, skipping upgrade: future<1.0.0,>=0.16.0 in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]==2.16.0) (0.18.2)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-datastore<1.8.0,>=1.7.1; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]==2.16.0) (1.7.4)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-core<2,>=0.28.1; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]==2.16.0) (0.28.1)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-pubsub<1.1.0,>=0.39.0; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]==2.16.0) (0.39.0)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<4,>=3.1.0; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]==2.16.0) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: google-apitools<0.5.29,>=0.5.28; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]==2.16.0) (0.5.28)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-bigtable<1.1.0,>=0.31.1; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]==2.16.0) (0.31.1)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-bigquery<1.18.0,>=1.6.0; extra == \"gcp\" in /usr/local/lib/python3.5/dist-packages (from apache-beam[gcp]==2.16.0) (1.6.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.14 in /usr/local/lib/python3.5/dist-packages (from pyarrow<0.15.0,>=0.11.1; python_version >= \"3.0\" or platform_system != \"Windows\"->apache-beam[gcp]==2.16.0) (1.17.4)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.0.0 in /usr/local/lib/python3.5/dist-packages (from pyarrow<0.15.0,>=0.11.1; python_version >= \"3.0\" or platform_system != \"Windows\"->apache-beam[gcp]==2.16.0) (1.13.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.5/dist-packages (from protobuf<4,>=3.5.0.post1->apache-beam[gcp]==2.16.0) (41.6.0)\n",
      "Requirement already satisfied, skipping upgrade: pbr>=0.11 in /usr/local/lib/python3.5/dist-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]==2.16.0) (5.4.4)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.7 in /usr/local/lib/python3.5/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.16.0) (0.4.7)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.5/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.16.0) (0.2.7)\n",
      "Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python3.5/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.16.0) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.1.4 in /usr/local/lib/python3.5/dist-packages (from pydot<2,>=1.2.0->apache-beam[gcp]==2.16.0) (2.4.5)\n",
      "Requirement already satisfied, skipping upgrade: requests>=2.7.0 in /usr/local/lib/python3.5/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.16.0) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: docopt in /usr/local/lib/python3.5/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.16.0) (0.6.2)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core[grpc]<2.0.0dev,>=1.6.0 in /usr/local/lib/python3.5/dist-packages (from google-cloud-datastore<1.8.0,>=1.7.1; extra == \"gcp\"->apache-beam[gcp]==2.16.0) (1.14.3)\n",
      "Requirement already satisfied, skipping upgrade: grpc-google-iam-v1<0.12dev,>=0.11.4 in /usr/local/lib/python3.5/dist-packages (from google-cloud-pubsub<1.1.0,>=0.39.0; extra == \"gcp\"->apache-beam[gcp]==2.16.0) (0.11.4)\n",
      "Requirement already satisfied, skipping upgrade: fasteners>=0.14 in /usr/local/lib/python3.5/dist-packages (from google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"->apache-beam[gcp]==2.16.0) (0.15)\n",
      "Requirement already satisfied, skipping upgrade: google-resumable-media<0.5.0dev,>=0.2.1 in /usr/local/lib/python3.5/dist-packages (from google-cloud-bigquery<1.18.0,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]==2.16.0) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.5/dist-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.16.0) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.5/dist-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.16.0) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.5/dist-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.16.0) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.5/dist-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.16.0) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2.0dev,>=0.4.0 in /usr/local/lib/python3.5/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.6.0->google-cloud-datastore<1.8.0,>=1.7.1; extra == \"gcp\"->apache-beam[gcp]==2.16.0) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.5/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.6.0->google-cloud-datastore<1.8.0,>=1.7.1; extra == \"gcp\"->apache-beam[gcp]==2.16.0) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: monotonic>=0.1 in /usr/local/lib/python3.5/dist-packages (from fasteners>=0.14->google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"->apache-beam[gcp]==2.16.0) (1.5)\n"
     ]
    }
   ],
   "source": [
    "!sudo pip install --upgrade apache-beam[gcp]==2.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apache-beam==2.12.0\n"
     ]
    }
   ],
   "source": [
    "#Ensure that we have the correct version of Apache Beam installed\n",
    "!pip freeze | grep apache-beam || sudo pip install --upgrade apache-beam[gcp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import apache_beam as beam\n",
    "import shutil\n",
    "import os\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set the environment variables related to your GCP Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"qwiklabs-gcp-ml-49b827b781ab\"  # Replace with your PROJECT\n",
    "BUCKET = \"qwiklabs-gcp-ml-49b827b781ab\"  # Replace with your BUCKET\n",
    "REGION = \"us-central1\"            # Choose an available region for Cloud MLE\n",
    "TFVERSION = \"1.13\"                # TF version for CMLE to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = TFVERSION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n",
      "Updated property [ml_engine/local_python].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION\n",
    "\n",
    "## ensure we predict locally with our current Python environment\n",
    "gcloud config set ml_engine/local_python `which python`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data preprocessing job with Cloud Dataflow\n",
    "\n",
    "The following code reads from BigQuery and saves the data as-is on Google Cloud Storage. We could also do additional preprocessing and cleanup inside Dataflow. Note that, in this case we'd have to remember to repeat that prepreprocessing at prediction time to avoid training/serving skew. In general, it is better to use tf.transform which will do this book-keeping for you, or to do preprocessing within your TensorFlow model. We will look at how tf.transform works in another notebook. For now, we are simply moving data from BigQuery to CSV using Dataflow.\n",
    "\n",
    "It's worth noting that while we could read from [BQ directly from TensorFlow](https://www.tensorflow.org/api_docs/python/tf/contrib/cloud/BigQueryReader), it is quite convenient to export to CSV and do the training off CSV. We can do this at scale with Cloud Dataflow. Furthermore, because we are running this on the cloud, you should go to the [GCP Console](https://console.cloud.google.com/dataflow) to view the status of the job. It will take several minutes for the preprocessing job to launch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our query and pipeline functions\n",
    "\n",
    "To start we'll copy over the `create_query` function we created in the `01_bigquery/c_extract_and_benchmark` notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query(phase, sample_size):\n",
    "    basequery = \"\"\"\n",
    "    SELECT\n",
    "        (tolls_amount + fare_amount) AS fare_amount,\n",
    "        EXTRACT(DAYOFWEEK from pickup_datetime) AS dayofweek,\n",
    "        EXTRACT(HOUR from pickup_datetime) AS hourofday,\n",
    "        pickup_longitude AS pickuplon,\n",
    "        pickup_latitude AS pickuplat,\n",
    "        dropoff_longitude AS dropofflon,\n",
    "        dropoff_latitude AS dropofflat\n",
    "    FROM\n",
    "        `nyc-tlc.yellow.trips`\n",
    "    WHERE\n",
    "        trip_distance > 0\n",
    "        AND fare_amount >= 2.5\n",
    "        AND pickup_longitude > -78\n",
    "        AND pickup_longitude < -70\n",
    "        AND dropoff_longitude > -78\n",
    "        AND dropoff_longitude < -70\n",
    "        AND pickup_latitude > 37\n",
    "        AND pickup_latitude < 45\n",
    "        AND dropoff_latitude > 37\n",
    "        AND dropoff_latitude < 45\n",
    "        AND passenger_count > 0\n",
    "        AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), EVERY_N) = 1\n",
    "    \"\"\"\n",
    "\n",
    "    if phase == \"TRAIN\":\n",
    "        subsample = \"\"\"\n",
    "        AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), EVERY_N * 100) >= (EVERY_N * 0)\n",
    "        AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), EVERY_N * 100) <  (EVERY_N * 70)\n",
    "        \"\"\"\n",
    "    elif phase == \"VALID\":\n",
    "        subsample = \"\"\"\n",
    "        AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), EVERY_N * 100) >= (EVERY_N * 70)\n",
    "        AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), EVERY_N * 100) <  (EVERY_N * 85)\n",
    "        \"\"\"\n",
    "    elif phase == \"TEST\":\n",
    "        subsample = \"\"\"\n",
    "        AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), EVERY_N * 100) >= (EVERY_N * 85)\n",
    "        AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), EVERY_N * 100) <  (EVERY_N * 100)\n",
    "        \"\"\"\n",
    "\n",
    "    query = basequery + subsample\n",
    "    return query.replace(\"EVERY_N\", sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll write the csv we create to a Cloud Storage bucket. So, we'll look to see that the location is empty, and if not clear out its contents so that it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if gsutil ls | grep -q gs://${BUCKET}/taxifare/ch4/taxi_preproc/; then\n",
    "    gsutil -m rm -rf gs://$BUCKET/taxifare/ch4/taxi_preproc/\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a function and pipeline for preprocessing the data. First, we'll define a `to_csv` function which takes a row dictionary (a dictionary created from a BigQuery reader representing each row of a dataset) and returns a comma separated string for each record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(rowdict):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        -rowdict: Dictionary. The beam bigquery reader returns a PCollection in\n",
    "        which each row is represented as a python dictionary\n",
    "    Returns:\n",
    "        -rowstring: a comma separated string representation of the record\n",
    "    \"\"\"\n",
    "    days = [\"null\", \"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"]\n",
    "    CSV_COLUMNS = \"fare_amount,dayofweek,hourofday,pickuplon,pickuplat,dropofflon,dropofflat\".split(',')\n",
    "    rowstring = ','.join([str(rowdict[k]) for k in CSV_COLUMNS])\n",
    "    return rowstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100,1,3,1,2,3,4'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_csv({\"dayofweek\": 1, \"hourofday\":3,\n",
    "       \"fare_amount\":100,\"pickuplon\":1,\"pickuplat\":2,\"dropofflon\":3,\"dropofflat\":4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our primary preprocessing function. Reading through the code this creates a pipeline to read data from BigQuery, use our `to_csv` function above to make a comma separated string, then write to a file in Google Cloud Storage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 1**\n",
    "\n",
    "In the code below, complete the pipeline to accomplish the tasks stated above. Have a look at [the Apache Beam documentation](https://beam.apache.org/documentation/) to remind yourself how to [read data from BigQuery](https://beam.apache.org/documentation/io/built-in/google-bigquery/) and apply map functions. Then write the comma separated string to a file in Cloud Storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def preprocess(EVERY_N, RUNNER):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        -EVERY_N: Integer. Sample one out of every N rows from the full dataset.\n",
    "        Larger values will yield smaller sample\n",
    "        -RUNNER: \"DirectRunner\" or \"DataflowRunner\". Specfy to run the pipeline\n",
    "        locally or on Google Cloud respectively. \n",
    "    Side-effects:\n",
    "        -Creates and executes dataflow pipeline. \n",
    "        See https://beam.apache.org/documentation/programming-guide/#creating-a-pipeline\n",
    "    \"\"\"\n",
    "    job_name = \"preprocess-taxifeatures\" + \"-\" + datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "    print(\"Launching Dataflow job {} ... hang on\".format(job_name))\n",
    "    OUTPUT_DIR = \"gs://{0}/taxifare/ch4/taxi_preproc/\".format(BUCKET)\n",
    "    print(OUTPUT_DIR)\n",
    "    #dictionary of pipeline options\n",
    "    options = {\n",
    "        \"staging_location\": os.path.join(OUTPUT_DIR, \"tmp\", \"staging\"),\n",
    "        \"temp_location\": os.path.join(OUTPUT_DIR, \"tmp\"),\n",
    "        \"job_name\": \"preprocess-taxifeatures\" + \"-\" + datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\"),\n",
    "        \"project\": PROJECT,\n",
    "        \"runner\": RUNNER\n",
    "    }\n",
    "  \n",
    "    #instantiate PipelineOptions object using options dictionary\n",
    "    opts = beam.pipeline.PipelineOptions(flags = [], **options)\n",
    "\n",
    "    #instantantiate Pipeline object using PipelineOptions\n",
    "    with beam.Pipeline(options=opts) as p:\n",
    "        for phase in [\"TRAIN\", \"VALID\", \"TEST\"]:\n",
    "            query = create_query(phase, EVERY_N)\n",
    "            outfile = os.path.join(OUTPUT_DIR, \"{}.csv\".format(phase))\n",
    "            (\n",
    "                p | \"read_{}\".format(phase) >> beam.io.Read(beam.io.BigQuerySource(query=query, use_standard_sql = True))\n",
    "                  | \"tocsv_{}\".format(phase) >> beam.Map(to_csv)\n",
    "                  | \"write_{}\".format(phase) >> beam.io.Write(beam.io.WriteToText(outfile))\n",
    "           \n",
    "            )\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the preprocessing pipeline function, we can execute the pipeline locally or on the cloud. To run our pipeline locally, we specify the `RUNNER` variable as `DirectRunner`. To run our pipeline in the cloud, we set `RUNNER` to be `DataflowRunner`. In either case, this variable is passed to the options dictionary that we use to instantiate the pipeline. \n",
    "\n",
    "As with training a model, it is good practice to test your preprocessing pipeline locally with a subset of your data before running it against your entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Beam pipeline locally\n",
    "\n",
    "We'll start by testing our pipeline locally. This takes upto 5 minutes. You will see a message \"Done\" when it has finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Dataflow job preprocess-taxifeatures-191126-071833 ... hang on\n",
      "gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/ch4/taxi_preproc/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Dataset qwiklabs-gcp-ml-49b827b781ab:temp_dataset_4f3107af575844cba006d36ebfaeb3aa does not exist so we will create it as temporary with location=US\n",
      "WARNING:root:Dataset qwiklabs-gcp-ml-49b827b781ab:temp_dataset_12d7766748c24cf895f432764bbc84ad does not exist so we will create it as temporary with location=US\n",
      "WARNING:root:Dataset qwiklabs-gcp-ml-49b827b781ab:temp_dataset_d2da67cce4024d82869aefe17ed5d469 does not exist so we will create it as temporary with location=US\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "preprocess(\"50*10000\", \"DirectRunner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Beam pipeline on Cloud Dataflow¶\n",
    "\n",
    "Again, we'll clear out our bucket to GCS to ensure a fresh run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/ch4/taxi_preproc/#1574752342780388...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/ch4/taxi_preproc/TEST.csv-00000-of-00001#1574752749962206...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/ch4/taxi_preproc/TRAIN.csv-00000-of-00002#1574752750919600...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/ch4/taxi_preproc/TRAIN.csv-00001-of-00002#1574752750913354...\n",
      "Removing gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/ch4/taxi_preproc/VALID.csv-00000-of-00001#1574752749140729...\n",
      "/ [5/5 objects] 100% Done                                                       \n",
      "Operation completed over 5 objects.                                              \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if gsutil ls -r gs://${BUCKET} | grep -q gs://${BUCKET}/taxifare/ch4/taxi_preproc/; then\n",
    "    gsutil -m rm -rf gs://${BUCKET}/taxifare/ch4/taxi_preproc/\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step will take **15-20 minutes**. Monitor job progress on the Dataflow section of [Cloud Console](https://pantheon.corp.google.com/dataflow?project=munn-sandbox&folder&organizationId=433637338589). Note, you can change the first arugment to \"None\" to process the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:--region not set; will default to us-central1. Future releases of Beam will require the user to set the region explicitly. https://cloud.google.com/compute/docs/regions-zones/regions-zones\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Dataflow job preprocess-taxifeatures-191126-072008 ... hang on\n",
      "gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/ch4/taxi_preproc/\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "preprocess(\"50*100\", \"DataflowRunner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the job finishes, we can look at the files that have been created and have a look at what they contain. You will notice that the files have been sharded into many csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1799400  2019-11-26T07:25:39Z  gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/ch4/taxi_preproc/TEST.csv-00000-of-00001\n",
      "   7986279  2019-11-26T07:25:30Z  gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/ch4/taxi_preproc/TRAIN.csv-00000-of-00001\n",
      "   1673668  2019-11-26T07:25:32Z  gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/ch4/taxi_preproc/VALID.csv-00000-of-00001\n",
      "                                 gs://qwiklabs-gcp-ml-49b827b781ab/taxifare/ch4/taxi_preproc/tmp/\n",
      "TOTAL: 3 objects, 11459347 bytes (10.93 MiB)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls -l gs://$BUCKET/taxifare/ch4/taxi_preproc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5,7,0,-73.999495,40.738719,-73.999661,40.727274\n",
      "2.5,5,0,-74.001277,40.741765,-73.99112,40.751328\n",
      "2.5,1,0,-73.983477,40.741278,-73.984838,40.741248\n",
      "2.5,1,0,-74.000582,40.72078,-74.001115,40.720127\n",
      "2.5,7,0,-73.989008,40.721487,-73.988672,40.722463\n",
      "2.5,2,0,-73.78817,40.641467,-73.794338,40.664787\n",
      "2.5,3,0,-73.912556,40.847338,-73.912556,40.847337\n",
      "2.5,6,0,-73.991488,40.728046,-73.99002,40.729696\n",
      "2.5,6,0,-74.002639,40.733425,-74.001557,40.732889\n",
      "2.5,6,0,-74.004138,40.743293,-74.003546,40.743132\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil cat \"gs://$BUCKET/taxifare/ch4/taxi_preproc/TRAIN.csv-00000-of-*\" | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop a model with new inputs\n",
    "\n",
    "We can now develop a model with these inputs. Download the first shard of the preprocessed data to a subfolder called `sample` so we can develop locally first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ -d sample ]; then\n",
    "    rm -rf sample\n",
    "fi\n",
    "mkdir sample\n",
    "gsutil cat \"gs://$BUCKET/taxifare/ch4/taxi_preproc/TRAIN.csv-00000-of-*\" > sample/train.csv\n",
    "gsutil cat \"gs://$BUCKET/taxifare/ch4/taxi_preproc/VALID.csv-00000-of-*\" > sample/valid.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin let's copy the `model.py` and `task.py` we developed in the previous notebooks here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "MODELDIR=./taxifaremodel\n",
    "\n",
    "test -d $MODELDIR || mkdir $MODELDIR\n",
    "cp -r ../../03_model_performance/taxifaremodel/* $MODELDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the files contained within the `taxifaremodel` folder. Within `model.py` we see that `feature_cols` has  three engineered features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_cols = [\n",
      "  #1. Engineered using tf.feature_column module\n",
      "  tf.feature_column.indicator_column(categorical_column = fc_day_hr),\n",
      "  fc_bucketized_plat,\n",
      "  fc_bucketized_plon,\n",
      "  fc_bucketized_dlat,\n",
      "  fc_bucketized_dlon,\n",
      "  #2. Engineered in input functions\n",
      "  tf.feature_column.numeric_column(key = \"latdiff\"),\n",
      "  tf.feature_column.numeric_column(key = \"londiff\"),\n",
      "  tf.feature_column.numeric_column(key = \"euclidean_dist\") \n",
      "]\n",
      "\n",
      "#3. Serving Input Receiver Function\n",
      "def serving_input_receiver_fn():\n",
      "    receiver_tensors = {\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "grep -A 15 \"feature_cols =\" taxifaremodel/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the engineered features that are created by the `add_engineered_features` function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        features = add_engineered_features(features)\n",
      "        \n",
      "        # Separate the label from the features\n",
      "        label = features.pop(\"fare_amount\") # remove label from features and store\n",
      "\n",
      "        return features, label\n",
      "--\n",
      "def add_engineered_features(features):\n",
      "    features[\"dayofweek\"] = features[\"dayofweek\"] - 1 # subtract one since our days of week are 1-7 instead of 0-6\n",
      "    \n",
      "    features[\"latdiff\"] = features[\"pickuplat\"] - features[\"dropofflat\"] # East/West\n",
      "    features[\"londiff\"] = features[\"pickuplon\"] - features[\"dropofflon\"] # North/South\n",
      "    features[\"euclidean_dist\"] = tf.sqrt(features[\"latdiff\"]**2 + features[\"londiff\"]**2)\n",
      "--\n",
      "    features = add_engineered_features(receiver_tensors) # 'features' is what is passed on to the model\n",
      "    \n",
      "    return tf.estimator.export.ServingInputReceiver(features = features, receiver_tensors = receiver_tensors)\n",
      "  \n",
      "#4. Train and Evaluate\n",
      "def train_and_evaluate(params):\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "grep -A 5 \"add_engineered_features(\" taxifaremodel/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try out this model on the local sample we've created to make sure everything works as expected. Note, this takes about **5 minutes** to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-26 07:36:46.998345: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From taxifaremodel/model.py:143: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From taxifaremodel/model.py:143: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 100 or save_checkpoints_secs None.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/training_util.py:236: initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:12: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.decode_csv is deprecated. Please use tf.io.decode_csv instead.\n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4271: _variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4326: _num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling __init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/canned/head.py:437: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/adagrad.py:76: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2019-11-26 07:36:50.357968: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-11-26 07:36:50.364167: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2019-11-26 07:36:50.364457: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556514e05270 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2019-11-26 07:36:50.364481: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2019-11-26 07:36:50.367074: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2019-11-26 07:36:52.354573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-26 07:36:52.355173: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556514e8d290 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2019-11-26 07:36:52.355206: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2019-11-26 07:36:52.355479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-26 07:36:52.355990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "2019-11-26 07:36:52.356054: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-11-26 07:36:52.357710: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "2019-11-26 07:36:52.358934: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
      "2019-11-26 07:36:52.359344: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
      "2019-11-26 07:36:52.361356: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2019-11-26 07:36:52.362816: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2019-11-26 07:36:52.367131: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2019-11-26 07:36:52.367257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-26 07:36:52.367748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-26 07:36:52.368128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
      "2019-11-26 07:36:52.368287: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-11-26 07:36:52.843003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-11-26 07:36:52.843059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
      "2019-11-26 07:36:52.843074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
      "2019-11-26 07:36:52.843476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-26 07:36:52.844063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-26 07:36:52.844427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10747 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /home/jupyter/training-data-analyst/courses/machine_learning/deepdive/04_advanced_preprocessing/labs/taxi_trained/model.ckpt.\n",
      "2019-11-26 07:36:53.757596: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "INFO:tensorflow:loss = 4141.368, step = 0\n",
      "INFO:tensorflow:Saving checkpoints for 10 into /home/jupyter/training-data-analyst/courses/machine_learning/deepdive/04_advanced_preprocessing/labs/taxi_trained/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From taxifaremodel/model.py:124: The name tf.metrics.root_mean_squared_error is deprecated. Please use tf.compat.v1.metrics.root_mean_squared_error instead.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-11-26T07:36:54Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2019-11-26 07:36:54.986156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-26 07:36:54.986422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "2019-11-26 07:36:54.986471: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-11-26 07:36:54.986620: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "2019-11-26 07:36:54.986644: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
      "2019-11-26 07:36:54.986680: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
      "2019-11-26 07:36:54.986701: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2019-11-26 07:36:54.986721: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2019-11-26 07:36:54.986742: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2019-11-26 07:36:54.986808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-26 07:36:54.987126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-26 07:36:54.987387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
      "2019-11-26 07:36:54.987422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-11-26 07:36:54.987456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
      "2019-11-26 07:36:54.987463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
      "2019-11-26 07:36:54.987605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-26 07:36:54.987961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-26 07:36:54.988291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10747 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/training-data-analyst/courses/machine_learning/deepdive/04_advanced_preprocessing/labs/taxi_trained/model.ckpt-10\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-11-26-07:37:00\n",
      "INFO:tensorflow:Saving dict for global step 10: average_loss = 124.11052, global_step = 10, label/mean = 11.229975, loss = 15859.729, prediction/mean = 5.1510754, rmse = 11.140491\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10: /home/jupyter/training-data-analyst/courses/machine_learning/deepdive/04_advanced_preprocessing/labs/taxi_trained/model.ckpt-10\n",
      "INFO:tensorflow:Performing the final export in the end of training.\n",
      "WARNING:tensorflow:From taxifaremodel/model.py:95: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Regression input must be a single string Tensor; got {'dayofweek': <tf.Tensor 'sub:0' shape=(?,) dtype=int32>, 'latdiff': <tf.Tensor 'sub_1:0' shape=(?,) dtype=float32>, 'pickuplat': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'dropofflat': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=float32>, 'londiff': <tf.Tensor 'sub_2:0' shape=(?,) dtype=float32>, 'hourofday': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=int32>, 'pickuplon': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'dropofflon': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=float32>, 'euclidean_dist': <tf.Tensor 'Sqrt:0' shape=(?,) dtype=float32>}\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'dayofweek': <tf.Tensor 'sub:0' shape=(?,) dtype=int32>, 'latdiff': <tf.Tensor 'sub_1:0' shape=(?,) dtype=float32>, 'pickuplat': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'dropofflat': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=float32>, 'londiff': <tf.Tensor 'sub_2:0' shape=(?,) dtype=float32>, 'hourofday': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=int32>, 'pickuplon': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float32>, 'dropofflon': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=float32>, 'euclidean_dist': <tf.Tensor 'Sqrt:0' shape=(?,) dtype=float32>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "2019-11-26 07:37:01.270698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-26 07:37:01.270986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "2019-11-26 07:37:01.271023: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-11-26 07:37:01.271061: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "2019-11-26 07:37:01.271086: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
      "2019-11-26 07:37:01.271108: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
      "2019-11-26 07:37:01.271130: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2019-11-26 07:37:01.271151: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2019-11-26 07:37:01.271174: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2019-11-26 07:37:01.271238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-26 07:37:01.271502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-26 07:37:01.271716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
      "2019-11-26 07:37:01.271752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-11-26 07:37:01.271759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
      "2019-11-26 07:37:01.271764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
      "2019-11-26 07:37:01.271901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-26 07:37:01.272168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-26 07:37:01.272405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10747 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/training-data-analyst/courses/machine_learning/deepdive/04_advanced_preprocessing/labs/taxi_trained/model.ckpt-10\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/jupyter/training-data-analyst/courses/machine_learning/deepdive/04_advanced_preprocessing/labs/taxi_trained/export/exporter/temp-1574753820/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 2064.2122.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/taxifare\n",
    "python -m taxifaremodel.task \\\n",
    "    --train_data_path=${PWD}/sample/train.csv \\\n",
    "    --eval_data_path=${PWD}/sample/valid.csv  \\\n",
    "    --output_dir=${PWD}/taxi_trained \\\n",
    "    --train_steps=10 \\\n",
    "    --job-dir=/tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've only done 10 training steps, so we don't expect the model to have good performance. Let's have a look at the exported files from our training job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxi_trained/export:\n",
      "exporter\n",
      "\n",
      "taxi_trained/export/exporter:\n",
      "1574753820\n",
      "\n",
      "taxi_trained/export/exporter/1574753820:\n",
      "saved_model.pb\n",
      "variables\n",
      "\n",
      "taxi_trained/export/exporter/1574753820/variables:\n",
      "variables.data-00000-of-00002\n",
      "variables.data-00001-of-00002\n",
      "variables.index\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -R taxi_trained/export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `saved_model_cli` to look at the exported signature. Note that the model doesn't need any of the engineered features as inputs. It will compute latdiff, londiff, euclidean from the provided inputs, thanks to the add_engineered call in the serving_input_fn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['predict']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['dayofweek'] tensor_info:\n",
      "        dtype: DT_INT32\n",
      "        shape: (-1)\n",
      "        name: sub:0\n",
      "    inputs['dropofflat'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: Placeholder_4:0\n",
      "    inputs['dropofflon'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: Placeholder_5:0\n",
      "    inputs['euclidean_dist'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: Sqrt:0\n",
      "    inputs['hourofday'] tensor_info:\n",
      "        dtype: DT_INT32\n",
      "        shape: (-1)\n",
      "        name: Placeholder_1:0\n",
      "    inputs['latdiff'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: sub_1:0\n",
      "    inputs['londiff'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: sub_2:0\n",
      "    inputs['pickuplat'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: Placeholder_3:0\n",
      "    inputs['pickuplon'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1)\n",
      "        name: Placeholder_2:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['predictions'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: dnn/logits/BiasAdd:0\n",
      "  Method name is: tensorflow/serving/predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-26 07:37:52.772317: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "model_dir=$(ls ${PWD}/taxi_trained/export/exporter | tail -1)\n",
    "saved_model_cli show --dir ${PWD}/taxi_trained/export/exporter/${model_dir} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test out prediciton with out model, we create a temporary json file containing the expected feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/test.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/test.json\n",
    "{\"dayofweek\": 0, \"hourofday\": 17, \"pickuplon\": -73.885262, \"pickuplat\": 40.773008, \"dropofflon\": -73.987232, \"dropofflat\": 40.732403}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTIONS\n",
      "[5.246157646179199]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The `gcloud ml-engine` commands have been renamed and will soon be removed. Please use `gcloud ai-platform` instead.\n",
      "If the signature defined in the model is not serving_default then you must specify it via --signature-name flag, otherwise the command may fail.\n",
      "WARNING: 2019-11-26 07:38:03.242031: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:48: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:50: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:601: The name tf.gfile.IsDirectory is deprecated. Please use tf.io.gfile.isdir instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:221: The name tf.saved_model.loader.maybe_saved_model_directory is deprecated. Please use tf.compat.v1.saved_model.loader.maybe_saved_model_directory instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:228: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:228: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2019-11-26 07:38:04.783562: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2019-11-26 07:38:06.730645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-26 07:38:06.731114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "2019-11-26 07:38:06.731146: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-11-26 07:38:06.732501: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "2019-11-26 07:38:06.733798: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
      "2019-11-26 07:38:06.734075: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
      "2019-11-26 07:38:06.735415: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2019-11-26 07:38:06.736450: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2019-11-26 07:38:06.739905: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2019-11-26 07:38:06.740041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-26 07:38:06.740511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-26 07:38:06.740955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
      "2019-11-26 07:38:06.744169: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-11-26 07:38:06.751084: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2019-11-26 07:38:06.751379: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55956c6fde20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2019-11-26 07:38:06.751413: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2019-11-26 07:38:06.813935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-26 07:38:06.814502: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55956c716e90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2019-11-26 07:38:06.814537: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2019-11-26 07:38:06.814760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-26 07:38:06.815200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "2019-11-26 07:38:06.815241: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-11-26 07:38:06.815285: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "2019-11-26 07:38:06.815308: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
      "2019-11-26 07:38:06.815321: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
      "2019-11-26 07:38:06.815334: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2019-11-26 07:38:06.815346: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2019-11-26 07:38:06.815358: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2019-11-26 07:38:06.815425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-26 07:38:06.815857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-26 07:38:06.816227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
      "2019-11-26 07:38:06.816269: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-11-26 07:38:07.283792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-11-26 07:38:07.283841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
      "2019-11-26 07:38:07.283852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
      "2019-11-26 07:38:07.284274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-26 07:38:07.284655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-11-26 07:38:07.285022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10747 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:230: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:230: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "2019-11-26 07:38:07.671251: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "model_dir=$(ls ${PWD}/taxi_trained/export/exporter)\n",
    "gcloud ml-engine local predict \\\n",
    "    --model-dir=${PWD}/taxi_trained/export/exporter/${model_dir} \\\n",
    "    --json-instances=/tmp/test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on the Cloud\n",
    "\n",
    "This will take 10-15 minutes even though the prompt immediately returns after the job is submitted. Monitor job progress on the [ML Engine section of Cloud Console](https://pantheon.corp.google.com/mlengine/jobs) and wait for the training job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/taxifare/ch4/taxi_trained\n",
    "JOBNAME=lab4a_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "    --region=$REGION \\\n",
    "    --module-name=taxifaremodel.task \\\n",
    "    --package-path=${PWD}/taxifaremodel \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --scale-tier=BASIC \\\n",
    "    --runtime-version=$TFVERSION \\\n",
    "    -- \\\n",
    "    --train_data_path=\"gs://${BUCKET}/taxifare/ch4/taxi_preproc/TRAIN*\" \\\n",
    "    --eval_data_path=\"gs://${BUCKET}/taxifare/ch4/taxi_preproc/VALID*\"  \\\n",
    "    --train_steps=5000 \\\n",
    "    --output_dir=$OUTDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has finished training on the cloud, we can check the export folder to see that a model has been correctly saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/taxifare/ch4/taxi_trained/export/exporter | tail -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can use the `saved_model_cli` to examine the exported signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "model_dir=$(gsutil ls gs://${BUCKET}/taxifare/ch4/taxi_trained/export/exporter | tail -1)\n",
    "saved_model_cli show --dir ${model_dir} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check out model's prediction with a local predict job on our test file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "model_dir=$(gsutil ls gs://${BUCKET}/taxifare/ch4/taxi_trained/export/exporter | tail -1)\n",
    "gcloud ml-engine local predict \\\n",
    "    --model-dir=${model_dir} \\\n",
    "    --json-instances=/tmp/test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Google Inc.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
